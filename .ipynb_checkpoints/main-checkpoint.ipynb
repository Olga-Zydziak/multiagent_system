{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b251096d-f8c3-4dfb-ae10-5d33be45f50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import json\n",
    "import vertexai\n",
    "from vertexai import agent_engines\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, List, Callable, Dict, Optional, Union, Any\n",
    "# Importy z własnych modułów\n",
    "from config import PROJECT_ID, LOCATION, MEMORY_ENGINE_DISPLAY_NAME, INPUT_FILE_PATH,MAIN_AGENT,CRITIC_MODEL,CODE_MODEL, API_TYPE_GEMINI,API_TYPE_SONNET, ANTHROPIC_API_KEY,basic_config_agent\n",
    "from agents.state import AgentWorkflowState\n",
    "from agents.autogen_agents import TriggerAgent,PlannerAgent,CriticAgent\n",
    "from prompts import LangchainAgentsPrompts,AutoGenAgentsPrompts\n",
    "from prompts_beta import PromptFactory\n",
    "from agents.langgraph_nodes import * \n",
    "from agents.autogen_agent_utils import run_autogen_planning_phase\n",
    "from memory.memory_bank_client import MemoryBankClient\n",
    "from tools.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b380a6ac-127b-44dd-9e3b-e0721815cd25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "AGENT_ENGINE_NAME = \"\" # Zostanie wypełniona po pobraniu lub utworzeniu silnika\n",
    "\n",
    "# Inicjalizacja głównego klienta Vertex AI\n",
    "client = vertexai.Client(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ba1b166-e3c9-4d11-9a08-76336faaa064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_agent_engine(display_name: str) :\n",
    "    \"\"\"\n",
    "    Pobiera istniejący Agent Engine po nazwie wyświetlanej lub tworzy nowy, jeśli nie istnieje.\n",
    "    \"\"\"\n",
    "    # 1. Pobierz listę wszystkich istniejących silników w projekcie\n",
    "    all_engines = agent_engines.list()\n",
    "    \n",
    "    # 2. Sprawdź, czy któryś z nich ma pasującą nazwę\n",
    "    for engine in all_engines:\n",
    "        if engine.display_name == display_name:\n",
    "            print(f\"INFO: Znaleziono i połączono z istniejącym Agent Engine: '{display_name}'\")\n",
    "            return engine\n",
    "            \n",
    "    # 3. Jeśli pętla się zakończyła i nic nie znaleziono, stwórz nowy silnik\n",
    "    print(f\"INFO: Nie znaleziono Agent Engine o nazwie '{display_name}'. Tworzenie nowego...\")\n",
    "    try:\n",
    "        new_engine = agent_engines.create(\n",
    "            display_name=display_name\n",
    "        )\n",
    "        print(f\"INFO: Pomyślnie utworzono nowy Agent Engine.\")\n",
    "        return new_engine\n",
    "    except Exception as e:\n",
    "        print(f\"KRYTYCZNY BŁĄD: Nie można utworzyć Agent Engine. Sprawdź konfigurację i uprawnienia. Błąd: {e}\")\n",
    "        exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80731513-5d98-4048-89f8-359410538a59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Znaleziono i połączono z istniejącym Agent Engine: 'memory-gamma-way'\n",
      "projects/815755318672/locations/us-central1/reasoningEngines/3849548538518175744\n"
     ]
    }
   ],
   "source": [
    "agent_engine =get_or_create_agent_engine(MEMORY_ENGINE_DISPLAY_NAME)\n",
    "AGENT_ENGINE_NAME = agent_engine.resource_name\n",
    "print(AGENT_ENGINE_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6982c7b4-a6dd-476f-b361-d36c50174185",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Konfiguracja czatu grupowego ---\n",
    "main_agent_configuration={\"cache_seed\": 42,\"seed\": 42,\"temperature\": 0.0,\n",
    "                        \"config_list\": basic_config_agent(agent_name=MAIN_AGENT, api_type=API_TYPE_GEMINI, location=LOCATION, project_id=PROJECT_ID)}\n",
    "critic_agent_configuration ={\"cache_seed\": 42,\"seed\": 42,\"temperature\": 0.0,\n",
    "                        \"config_list\": basic_config_agent(api_key=ANTHROPIC_API_KEY,agent_name=CRITIC_MODEL, api_type=API_TYPE_SONNET)}\n",
    "\n",
    "#---WYWOŁANIE AGENTÓW\n",
    "trigger_agent = TriggerAgent(llm_config=main_agent_configuration, prompt=PromptFactory.for_trigger())\n",
    "planner_agent = PlannerAgent(llm_config=main_agent_configuration, prompt=PromptFactory.for_planner()) # Tutaj nie przekazujemy inspiracji\n",
    "critic_agent = CriticAgent(llm_config=critic_agent_configuration, prompt=PromptFactory.for_critic())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffc80bb-b8e4-424c-b3b7-96dc37607fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9d12db3-dfe5-4c56-9494-8721eccacda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: MemoryBankClient gotowy do pracy z silnikiem: projects/815755318672/locations/us-central1/reasoningEngines/3849548538518175744\n",
      "\n",
      "--- ODPYTYWANIE PAMIĘCI O INSPIRACJE ---\n",
      "INFO: Odpytuję pamięć semantycznie z zapytaniem 'Najlepsze strategie i kluczowe wnioski dotyczące przetwarzania danych' w zakresie {'dataset_signature': 'ae1568fe7dae11d4bacd0c21ed718503'}\n",
      "udany plan: id='f29202ec-5141-46a1-9e3c-d09e8830ec46' run_id='2b57c400-17d0-4e2a-8cbe-56a927722969' timestamp=datetime.datetime(2025, 8, 4, 22, 28, 9, 644971) memory_type=<MemoryType.META_INSIGHT: 'META_INSIGHT'> dataset_signature='ae1568fe7dae11d4bacd0c21ed718503' source_node='meta_auditor_node' content={'observation': 'Krytyk w systemie nie jest wystarczająco rygorystyczny - nie kwestionuje założeń, nie proponuje alternatywnych podejść ani nie identyfikuje potencjalnych problemów w planie.', 'recommendation': 'Dodać do promptu Krytyka wyraźne instrukcje: \"Jako Krytyk, Twoim zadaniem jest rygorystyczna analiza planu. Dla każdego kroku: 1. Zidentyfikuj potencjalne problemy i słabe punkty 2. Zaproponuj co najmniej jedną alternatywę 3. Oceń wpływ na wydajność i jakość wyników 4. Przypisz poziom ryzyka (niski/średni/wysoki) do każdego zidentyfikowanego problemu\"', 'target_agent_or_node': 'critic_agent', 'tags': ['prompt-engineering', 'critical-thinking', 'risk-assessment', 'plan-evaluation', 'feedback-quality']} metadata={'importance_score': 1.0}\n",
      "udany plan: id='bf235dfb-6cbc-4819-8a27-47ec0b4ebe71' run_id='3cca266e-65e1-453c-93bf-ee8e1c8425bf' timestamp=datetime.datetime(2025, 8, 5, 21, 45, 58, 785643) memory_type=<MemoryType.META_INSIGHT: 'META_INSIGHT'> dataset_signature='ae1568fe7dae11d4bacd0c21ed718503' source_node='meta_auditor_node' content={'observation': 'Funkcja query_memory zawiera blok try-except wewnątrz pętli, co sugeruje częste problemy z parsowaniem JSON podczas odczytu danych z pamięci systemu.', 'recommendation': 'Standaryzacja formatu zapisywanych danych w pamięci systemu i dodanie walidacji przed zapisem, aby uniknąć późniejszych problemów z odczytem i parsowaniem JSON.', 'target_agent_or_node': 'memory_system', 'tags': ['pamięć systemu', 'obsługa błędów', 'format danych', 'JSON', 'walidacja']} metadata={'importance_score': 1.0}\n",
      "udany plan: id='1aee6a40-5d08-4e32-9b92-9a79fefe957d' run_id='3cca266e-65e1-453c-93bf-ee8e1c8425bf' timestamp=datetime.datetime(2025, 8, 5, 21, 45, 33, 985493) memory_type=<MemoryType.META_INSIGHT: 'META_INSIGHT'> dataset_signature='ae1568fe7dae11d4bacd0c21ed718503' source_node='meta_auditor_node' content={'observation': 'System pamięci w config.py zawiera zakomentowane fragmenty, które wydają się być niedokończone lub problematyczne, co może ograniczać zdolność systemu do efektywnego przechowywania i odzyskiwania informacji z poprzednich sesji.', 'recommendation': 'Dokończyć implementację systemu pamięci w config.py, usuwając zakomentowane fragmenty i zapewniając pełną funkcjonalność przechowywania i odzyskiwania informacji z poprzednich sesji.', 'target_agent_or_node': 'memory_system', 'tags': ['pamięć systemu', 'konfiguracja', 'ciągłość sesji', 'przechowywanie danych', 'optymalizacja']} metadata={'importance_score': 1.0}\n",
      "INFO: Znaleziono i poprawnie przetworzono 3 pasujących wspomnień.\n",
      "OSTRZEŻENIE: Nie udało się pobrać inspiracji z pamięci: SUCCESSFUL_WORKFLOW\n",
      "--- DORADCA POLITYKI SYSTEMOWEJ: Sprawdzanie pamięci... ---\n",
      "INFO: Odpytuję pamięć semantycznie z zapytaniem 'Najważniejsze rekomendacje dotyczące ulepszenia promptów lub logiki systemu' w zakresie {'dataset_signature': 'ae1568fe7dae11d4bacd0c21ed718503'}\n",
      "udany plan: id='f29202ec-5141-46a1-9e3c-d09e8830ec46' run_id='2b57c400-17d0-4e2a-8cbe-56a927722969' timestamp=datetime.datetime(2025, 8, 4, 22, 28, 9, 644971) memory_type=<MemoryType.META_INSIGHT: 'META_INSIGHT'> dataset_signature='ae1568fe7dae11d4bacd0c21ed718503' source_node='meta_auditor_node' content={'observation': 'Krytyk w systemie nie jest wystarczająco rygorystyczny - nie kwestionuje założeń, nie proponuje alternatywnych podejść ani nie identyfikuje potencjalnych problemów w planie.', 'recommendation': 'Dodać do promptu Krytyka wyraźne instrukcje: \"Jako Krytyk, Twoim zadaniem jest rygorystyczna analiza planu. Dla każdego kroku: 1. Zidentyfikuj potencjalne problemy i słabe punkty 2. Zaproponuj co najmniej jedną alternatywę 3. Oceń wpływ na wydajność i jakość wyników 4. Przypisz poziom ryzyka (niski/średni/wysoki) do każdego zidentyfikowanego problemu\"', 'target_agent_or_node': 'critic_agent', 'tags': ['prompt-engineering', 'critical-thinking', 'risk-assessment', 'plan-evaluation', 'feedback-quality']} metadata={'importance_score': 1.0}\n",
      "udany plan: id='1aee6a40-5d08-4e32-9b92-9a79fefe957d' run_id='3cca266e-65e1-453c-93bf-ee8e1c8425bf' timestamp=datetime.datetime(2025, 8, 5, 21, 45, 33, 985493) memory_type=<MemoryType.META_INSIGHT: 'META_INSIGHT'> dataset_signature='ae1568fe7dae11d4bacd0c21ed718503' source_node='meta_auditor_node' content={'observation': 'System pamięci w config.py zawiera zakomentowane fragmenty, które wydają się być niedokończone lub problematyczne, co może ograniczać zdolność systemu do efektywnego przechowywania i odzyskiwania informacji z poprzednich sesji.', 'recommendation': 'Dokończyć implementację systemu pamięci w config.py, usuwając zakomentowane fragmenty i zapewniając pełną funkcjonalność przechowywania i odzyskiwania informacji z poprzednich sesji.', 'target_agent_or_node': 'memory_system', 'tags': ['pamięć systemu', 'konfiguracja', 'ciągłość sesji', 'przechowywanie danych', 'optymalizacja']} metadata={'importance_score': 1.0}\n",
      "udany plan: id='76ff764a-d1b4-4d04-a9d6-5efa83fd15f6' run_id='2b57c400-17d0-4e2a-8cbe-56a927722969' timestamp=datetime.datetime(2025, 8, 4, 22, 28, 48, 848540) memory_type=<MemoryType.META_INSIGHT: 'META_INSIGHT'> dataset_signature='ae1568fe7dae11d4bacd0c21ed718503' source_node='meta_auditor_node' content={'observation': 'Brak widocznej interakcji między Plannerem a Krytykiem - w logach widać tylko finalny plan z oznaczeniem \"PLAN_AKCEPTOWANY_PRZEJSCIE_DO_IMPLEMENTACJI\" bez żadnych poprawek czy sugestii ulepszeń do planu.', 'recommendation': 'Zmodyfikować architekturę systemu w `langgraph_nodes.py`, implementując obowiązkową pętlę iteracyjną między Plannerem a Krytykiem, która wymaga co najmniej jednej rundy poprawek przed zaakceptowaniem planu.', 'target_agent_or_node': 'langgraph_nodes', 'tags': ['architektura-systemu', 'iteracja', 'planner-krytyk', 'współpraca-agentów', 'rygorystyczność']} metadata={'importance_score': 1.0}\n",
      "INFO: Znaleziono i poprawnie przetworzono 3 pasujących wspomnień.\n",
      "  [INFO] Aktywowano polityki:\n",
      "--- AKTYWNE POLITYKI SYSTEMOWE (NAJWYŻSZY PRIORYTET) ---\n",
      "- Dodać do promptu Krytyka wyraźne instrukcje: \"Jako Krytyk, Twoim zadaniem jest rygorystyczna analiza planu. Dla każdego kroku: 1. Zidentyfikuj potencjalne problemy i słabe punkty 2. Zaproponuj co najmniej jedną alternatywę 3. Oceń wpływ na wydajność i jakość wyników 4. Przypisz poziom ryzyka (niski/średni/wysoki) do każdego zidentyfikowanego problemu\"\n",
      "- Dokończyć implementację systemu pamięci w config.py, usuwając zakomentowane fragmenty i zapewniając pełną funkcjonalność przechowywania i odzyskiwania informacji z poprzednich sesji.\n",
      "- Zmodyfikować architekturę systemu w `langgraph_nodes.py`, implementując obowiązkową pętlę iteracyjną między Plannerem a Krytykiem, która wymaga co najmniej jednej rundy poprawek przed zaakceptowaniem planu.\n",
      "\n",
      "================================================================================\n",
      "### ### FAZA 1: URUCHAMIANIE PLANOWANIA STRATEGICZNEGO (AutoGen) ### ###\n",
      "================================================================================\n",
      "\n",
      "INFO: Dołączam aktywne polityki systemowe do fazy planowania.\n",
      "\u001b[33mUserProxy\u001b[0m (to chat_manager):\n",
      "\n",
      "Oto podgląd danych:\n",
      "\n",
      "Kolumny:\n",
      "['Transaction_ID', 'User_ID', 'Transaction_Amount', 'Transaction_Type', 'Timestamp', 'Account_Balance', 'Device_Type', 'Location', 'Merchant_Category', 'IP_Address_Flag', 'Previous_Fraudulent_Activity', 'Daily_Transaction_Count', 'Avg_Transaction_Amount_7d', 'Failed_Transaction_Count_7d', 'Card_Type', 'Card_Age', 'Transaction_Distance', 'Authentication_Method', 'Risk_Score', 'Is_Weekend', 'Fraud_Label']\n",
      "\n",
      "Pierwsze 5 wierszy:\n",
      "  Transaction_ID    User_ID  Transaction_Amount Transaction_Type            Timestamp  Account_Balance Device_Type  Location Merchant_Category  IP_Address_Flag  Previous_Fraudulent_Activity  Daily_Transaction_Count  Avg_Transaction_Amount_7d  Failed_Transaction_Count_7d   Card_Type  Card_Age  Transaction_Distance Authentication_Method  Risk_Score  Is_Weekend  Fraud_Label\n",
      "0      TXN_33553  USER_1834               39.79              POS  2023-08-14 19:30:00         93213.17      Laptop    Sydney            Travel                0                             0                        7                     437.63                            3        Amex        65                883.17             Biometric      0.8494           0            0\n",
      "1       TXN_9427  USER_7875                1.19    Bank Transfer  2023-06-07 04:01:00         75725.25      Mobile  New York          Clothing                0                             0                       13                     478.76                            4  Mastercard       186               2203.36              Password      0.0959           0            1\n",
      "2        TXN_199  USER_2734               28.96           Online  2023-06-20 15:25:00          1588.96      Tablet    Mumbai       Restaurants                0                             0                       14                      50.01                            4        Visa       226               1909.29             Biometric      0.8400           0            1\n",
      "3      TXN_12447  USER_2617              254.32   ATM Withdrawal  2023-12-07 00:31:00         76807.20      Tablet  New York          Clothing                0                             0                        8                     182.48                            4        Visa        76               1311.86                   OTP      0.7935           0            1\n",
      "4      TXN_39489  USER_2014               31.28              POS  2023-11-11 23:44:00         92354.66      Mobile    Mumbai       Electronics                0                             1                       14                     328.69                            4  Mastercard       140                966.98              Password      0.3819           1            1\n",
      "\n",
      "--- AKTYWNE POLITYKI SYSTEMOWE (NAJWYŻSZY PRIORYTET) ---\n",
      "- Dodać do promptu Krytyka wyraźne instrukcje: \"Jako Krytyk, Twoim zadaniem jest rygorystyczna analiza planu. Dla każdego kroku: 1. Zidentyfikuj potencjalne problemy i słabe punkty 2. Zaproponuj co najmniej jedną alternatywę 3. Oceń wpływ na wydajność i jakość wyników 4. Przypisz poziom ryzyka (niski/średni/wysoki) do każdego zidentyfikowanego problemu\"\n",
      "- Dokończyć implementację systemu pamięci w config.py, usuwając zakomentowane fragmenty i zapewniając pełną funkcjonalność przechowywania i odzyskiwania informacji z poprzednich sesji.\n",
      "- Zmodyfikować architekturę systemu w `langgraph_nodes.py`, implementując obowiązkową pętlę iteracyjną między Plannerem a Krytykiem, która wymaga co najmniej jednej rundy poprawek przed zaakceptowaniem planu.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: TriggerAgent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mTriggerAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "Dane są tabularyczne. Przekazuję do PlannerAgent w celu stworzenia planu analizy.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: PlannerAgent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mPlannerAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "**CEL:** Stworzenie planu czyszczenia i przygotowania danych do analizy.\n",
      "**DANE WEJŚCIOWE:** Podgląd zbioru danych transakcyjnych.\n",
      "**WNIOSEK:** Poniższy plan został opracowany w celu systematycznego przygotowania danych, zapewniając ich spójność, kompletność i optymalny format dla modeli analitycznych. [cite: 41, 56]\n",
      "\n",
      "---\n",
      "### **PLAN PRZYGOTOWANIA DANYCH**\n",
      "\n",
      "**Faza 1: Wstępna walidacja i czyszczenie**\n",
      "\n",
      "1.  **Weryfikacja integralności danych:** Sprawdzenie zbioru danych pod kątem zduplikowanych transakcji na podstawie unikalnego identyfikatora `Transaction_ID`. Zduplikowane wiersze zostaną usunięte.\n",
      "2.  **Analiza brakujących wartości:** Wygenerowanie raportu procentowego brakujących wartości (NaN/Null) dla każdej kolumny w celu zidentyfikowania problematycznych cech.\n",
      "3.  **Korekta typów danych:**\n",
      "    3.1. Konwersja kolumny `Timestamp` z typu obiektowego/tekstowego na typ `datetime`.\n",
      "    3.2. Weryfikacja, czy kolumny binarne (`IP_Address_Flag`, `Previous_Fraudulent_Activity`, `Is_Weekend`, `Fraud_Label`) mają spójny typ numeryczny (np. `int8`).\n",
      "\n",
      "**Faza 2: Obsługa brakujących wartości**\n",
      "\n",
      "4.  **Imputacja wartości numerycznych:** Uzupełnienie brakujących wartości w kolumnach numerycznych (`Account_Balance`, `Avg_Transaction_Amount_7d`, `Transaction_Distance`, `Risk_Score`) za pomocą mediany, aby zminimalizować wpływ wartości odstających.\n",
      "5.  **Imputacja wartości kategorycznych:** Uzupełnienie brakujących wartości w kolumnach kategorycznych (`Card_Type`, `Authentication_Method`) za pomocą najczęściej występującej wartości (mody).\n",
      "\n",
      "**Faza 3: Inżynieria cech (Feature Engineering)**\n",
      "\n",
      "6.  **Ekstrakcja cech czasowych:** Na podstawie kolumny `Timestamp` stworzenie nowych cech:\n",
      "    6.1. `Transaction_Hour` (godzina transakcji).\n",
      "    6.2. `Transaction_DayOfWeek` (dzień tygodnia).\n",
      "7.  **Tworzenie cech relacyjnych:**\n",
      "    7.1. `Amount_to_Balance_Ratio`: Stosunek `Transaction_Amount` do `Account_Balance`. Wartość zostanie ustawiona na 0, jeśli `Account_Balance` wynosi 0, aby uniknąć dzielenia przez zero.\n",
      "    7.2. `Amount_vs_Avg_7d_Diff`: Różnica między `Transaction_Amount` a `Avg_Transaction_Amount_7d`.\n",
      "\n",
      "**Faza 4: Transformacja danych**\n",
      "\n",
      "8.  **Obsługa wartości odstających:** Ograniczenie ekstremalnych wartości w kluczowych kolumnach numerycznych (`Transaction_Amount`, `Account_Balance`, `Transaction_Distance`) poprzez winsoryzację na poziomie 1. i 99. percentyla.\n",
      "9.  **Kodowanie zmiennych kategorycznych:** Zastosowanie techniki One-Hot Encoding dla kolumn o niskiej kardynalności: `Transaction_Type`, `Device_Type`, `Card_Type`, `Authentication_Method`.\n",
      "10. **Kodowanie zmiennych o wysokiej kardynalności:** Zastosowanie kodowania binarnego (Binary Encoding) dla kolumn `Location` i `Merchant_Category` w celu uniknięcia eksplozji wymiarowości.\n",
      "\n",
      "**Faza 5: Finalizacja zbioru danych**\n",
      "\n",
      "11. **Selekcja cech:** Usunięcie oryginalnych kolumn, które zostały przetworzone lub nie niosą wartości predykcyjnej: `Transaction_ID`, `User_ID`, `Timestamp`.\n",
      "12. **Skalowanie cech:** Normalizacja wszystkich cech numerycznych (w tym nowo utworzonych) przy użyciu `StandardScaler` w celu ujednolicenia ich skali i wariancji.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: CriticAgent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mCriticAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "Jako Krytyk, przeprowadzę rygorystyczną analizę zaproponowanego planu przygotowania danych.\n",
      "\n",
      "### Analiza planu:\n",
      "\n",
      "**Faza 1: Wstępna walidacja i czyszczenie**\n",
      "1. **Weryfikacja integralności danych** - Dobry krok, ale zbyt ogólny.\n",
      "   - **Problem**: Brak konkretnego działania po wykryciu duplikatów.\n",
      "   - **Alternatywa**: Zdefiniować dokładnie, co zrobić z duplikatami: usunąć pierwsze/ostatnie wystąpienie, czy uśrednić wartości?\n",
      "   - **Wpływ**: Nieprecyzyjne podejście może prowadzić do utraty ważnych danych.\n",
      "   - **Poziom ryzyka**: Średni\n",
      "\n",
      "2. **Analiza brakujących wartości** - Zbyt ogólny krok.\n",
      "   - **Problem**: Określenie tylko analizy bez jasnej decyzji.\n",
      "   - **Alternatywa**: Ustalić próg procentowy (np. 30%), powyżej którego kolumna zostanie usunięta.\n",
      "   - **Wpływ**: Brak decyzyjności na tym etapie powoduje niejednoznaczność w dalszych krokach.\n",
      "   - **Poziom ryzyka**: Średni\n",
      "\n",
      "3. **Korekta typów danych** - Dobrze, ale niekompletne.\n",
      "   - **Problem**: Brak weryfikacji typów dla wszystkich kolumn numerycznych.\n",
      "   - **Alternatywa**: Sprawdzić i przekonwertować wszystkie kolumny numeryczne (Transaction_Amount, Account_Balance, itp.).\n",
      "   - **Wpływ**: Pominięcie niektórych konwersji może prowadzić do błędów w późniejszych obliczeniach.\n",
      "   - **Poziom ryzyka**: Niski\n",
      "\n",
      "**Faza 2: Obsługa brakujących wartości**\n",
      "4. **Imputacja wartości numerycznych** - Dobrze, ale zbyt ogólne podejście.\n",
      "   - **Problem**: Używanie mediany dla wszystkich kolumn numerycznych bez analizy ich rozkładu.\n",
      "   - **Alternatywa**: Rozważyć różne strategie imputacji dla różnych kolumn (np. mediana dla Transaction_Amount, 0 dla Failed_Transaction_Count).\n",
      "   - **Wpływ**: Jednolite podejście może wprowadzić systematyczne błędy.\n",
      "   - **Poziom ryzyka**: Średni\n",
      "\n",
      "5. **Imputacja wartości kategorycznych** - Łączy zbyt wiele operacji w jednym kroku.\n",
      "   - **Problem**: Brak rozróżnienia między kategoriami o różnym znaczeniu.\n",
      "   - **Alternatywa**: Podzielić na osobne kroki dla różnych typów kolumn kategorycznych.\n",
      "   - **Wpływ**: Nierozważenie kontekstu poszczególnych kolumn może prowadzić do utraty znaczenia.\n",
      "   - **Poziom ryzyka**: Niski\n",
      "\n",
      "**Faza 3: Inżynieria cech**\n",
      "6. i 7. **Ekstrakcja cech i tworzenie cech relacyjnych** - Zbyt wiele operacji w jednym kroku.\n",
      "   - **Problem**: Łączenie wielu przekształceń w jednym punkcie.\n",
      "   - **Alternatywa**: Rozbić na atomowe operacje, jedną cechę na krok.\n",
      "   - **Wpływ**: Trudność w śledzeniu błędów i debugowaniu.\n",
      "   - **Poziom ryzyka**: Niski\n",
      "\n",
      "**Faza 4: Transformacja danych**\n",
      "8. **Obsługa wartości odstających** - Zbyt mechaniczne podejście.\n",
      "   - **Problem**: Stosowanie winsoryzacji bez analizy rozkładu danych.\n",
      "   - **Alternatywa**: Najpierw zbadać rozkłady, potem zdecydować o metodzie (winsoryzacja/obcięcie/logarytmizacja).\n",
      "   - **Wpływ**: Potencjalne zniekształcenie istotnych wzorców w danych.\n",
      "   - **Poziom ryzyka**: Wysoki\n",
      "\n",
      "9-10. **Kodowanie zmiennych** - Łączy wiele operacji w jednym kroku.\n",
      "   - **Problem**: Decyzja o metodzie kodowania bez analizy kardynalności.\n",
      "   - **Alternatywa**: Najpierw przeprowadzić analizę kardynalności, potem zdecydować o metodzie kodowania dla każdej kolumny oddzielnie.\n",
      "   - **Wpływ**: Nieoptymalne kodowanie może prowadzić do utraty informacji lub nadmiernego rozrostu wymiarowości.\n",
      "   - **Poziom ryzyka**: Średni\n",
      "\n",
      "**Faza 5: Finalizacja zbioru danych**\n",
      "11. **Selekcja cech** - Usuwanie kolumn bez analizy ich wartości predykcyjnej.\n",
      "   - **Problem**: Arbitralne usunięcie kolumn może prowadzić do utraty ważnych informacji.\n",
      "   - **Alternatywa**: Przeprowadzić analizę ważności cech przed usunięciem.\n",
      "   - **Wpływ**: Potencjalna utrata istotnych sygnałów predykcyjnych.\n",
      "   - **Poziom ryzyka**: Wysoki\n",
      "\n",
      "12. **Skalowanie cech** - Mechaniczne stosowanie StandardScaler.\n",
      "   - **Problem**: Standaryzacja może nie być odpowiednia dla wszystkich cech.\n",
      "   - **Alternatywa**: Rozważyć różne metody skalowania (MinMaxScaler dla cech ze skończonym zakresem, RobustScaler dla danych z wartościami odstającymi).\n",
      "   - **Wpływ**: Nieodpowiednie skalowanie może zniekształcić dane i wpłynąć na wydajność modelu.\n",
      "   - **Poziom ryzyka**: Średni\n",
      "\n",
      "### Ogólna ocena:\n",
      "Plan wymaga znaczących poprawek, szczególnie w zakresie atomizacji kroków (jedna operacja na krok) oraz bardziej świadomego podejścia do transformacji danych, które powinno być oparte na analizie ich charakterystyki.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: PlannerAgent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mPlannerAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "**CEL:** Stworzenie zaktualizowanego planu czyszczenia i przygotowania danych, uwzględniającego uwagi recenzenta (CriticAgent).\n",
      "**DANE WEJŚCIOWE:** Oryginalny plan, uwagi recenzenta.\n",
      "**WNIOSEK:** Poniżej znajduje się nowa, kompletna wersja planu. Zmiany wprowadzone w odpowiedzi na recenzję zostały oznaczone. Plan jest teraz bardziej atomowy, a decyzje o transformacjach są poprzedzone odpowiednimi krokami analitycznymi, co zwiększa jego solidność i odporność na błędy. [cite: 41, 56]\n",
      "\n",
      "---\n",
      "### **ZAKTUALIZOWANY PLAN PRZYGOTOWANIA DANYCH (Wersja 2)**\n",
      "\n",
      "**Faza 1: Wstępna walidacja i czyszczenie**\n",
      "\n",
      "1.  **[AKTUALIZACJA] Weryfikacja integralności danych:** Sprawdzenie zbioru danych pod kątem zduplikowanych wierszy na podstawie `Transaction_ID`. W przypadku znalezienia duplikatów, usunięte zostaną wszystkie wystąpienia poza ostatnim, zakładając, że jest ono najbardziej aktualne.\n",
      "2.  **Analiza brakujących wartości:** Wygenerowanie raportu procentowego brakujących wartości (NaN/Null) dla każdej kolumny.\n",
      "3.  **[AKTUALIZACJA] Decyzja o usunięciu kolumn z brakami:** Ustalenie progu tolerancji dla brakujących danych na 40%. Kolumny, w których odsetek brakujących wartości przekracza ten próg, zostaną usunięte ze zbioru danych.\n",
      "4.  **[AKTUALIZACJA] Kompleksowa korekta typów danych:**\n",
      "    4.1. Konwersja kolumny `Timestamp` na typ `datetime`.\n",
      "    4.2. Weryfikacja i konwersja wszystkich kolumn numerycznych (`Transaction_Amount`, `Account_Balance`, `Daily_Transaction_Count`, etc.) do odpowiednich typów (`float64`, `int64`), aby zapobiec błędom w obliczeniach.\n",
      "    4.3. Weryfikacja, czy kolumny binarne (`IP_Address_Flag`, `Previous_Fraudulent_Activity`, `Is_Weekend`, `Fraud_Label`) mają spójny typ numeryczny (`int8`).\n",
      "\n",
      "**Faza 2: Obsługa brakujących wartości**\n",
      "\n",
      "5.  **[AKTUALIZACJA] Imputacja wartości numerycznych:** Uzupełnienie brakujących wartości w kolumnach numerycznych (`Account_Balance`, `Avg_Transaction_Amount_7d`, `Transaction_Distance`, `Risk_Score`) za pomocą mediany. Mediana jest preferowana nad średnią ze względu na jej odporność na wartości odstające, które są częste w danych finansowych.\n",
      "6.  **Imputacja wartości kategorycznych:** Uzupełnienie brakujących wartości w kolumnach kategorycznych (`Card_Type`, `Authentication_Method`) za pomocą najczęściej występującej wartości (mody) dla danej kolumny.\n",
      "\n",
      "**Faza 3: Inżynieria cech (Feature Engineering)**\n",
      "\n",
      "7.  **[AKTUALIZACJA] Ekstrakcja cechy: Godzina transakcji:** Na podstawie kolumny `Timestamp` stworzenie nowej cechy `Transaction_Hour`.\n",
      "8.  **[AKTUALIZACJA] Ekstrakcja cechy: Dzień tygodnia:** Na podstawie kolumny `Timestamp` stworzenie nowej cechy `Transaction_DayOfWeek`.\n",
      "9.  **[AKTUALIZACJA] Tworzenie cechy: Stosunek kwoty do salda:** Stworzenie nowej cechy `Amount_to_Balance_Ratio` (`Transaction_Amount` / `Account_Balance`). W przypadku `Account_Balance` równego zero, wartość zostanie ustawiona na 0.\n",
      "10. **[AKTUALIZACJA] Tworzenie cechy: Różnica kwoty od średniej:** Stworzenie nowej cechy `Amount_vs_Avg_7d_Diff` (`Transaction_Amount` - `Avg_Transaction_Amount_7d`).\n",
      "\n",
      "**Faza 4: Transformacja danych**\n",
      "\n",
      "11. **[AKTUALIZACJA] Analiza rozkładu danych numerycznych:** Wygenerowanie histogramów i wykresów pudełkowych dla kluczowych cech numerycznych (`Transaction_Amount`, `Account_Balance`, `Transaction_Distance`) w celu wizualnej identyfikacji wartości odstających i skośności rozkładu.\n",
      "12. **[AKTUALIZACJA] Obsługa wartości odstających:** Na podstawie analizy z kroku 11, ograniczenie ekstremalnych wartości w zidentyfikowanych kolumnach poprzez winsoryzację na poziomie 1. i 99. percentyla.\n",
      "13. **[AKTUALIZACJA] Analiza kardynalności cech kategorycznych:** Obliczenie liczby unikalnych wartości dla każdej kolumny kategorycznej (`Transaction_Type`, `Device_Type`, `Location`, `Merchant_Category`, `Card_Type`, `Authentication_Method`).\n",
      "14. **[AKTUALIZACJA] Kodowanie One-Hot Encoding:** Zastosowanie OHE dla kolumn o niskiej kardynalności (np. < 10 unikalnych wartości), zidentyfikowanych w kroku 13 (prawdopodobnie `Transaction_Type`, `Device_Type`, `Card_Type`, `Authentication_Method`).\n",
      "15. **[AKTUALIZACJA] Kodowanie binarne (Binary Encoding):** Zastosowanie kodowania binarnego dla kolumn o wysokiej kardynalności, zidentyfikowanych w kroku 13 (prawdopodobnie `Location`, `Merchant_Category`), w celu uniknięcia eksplozji wymiarowości.\n",
      "\n",
      "**Faza 5: Finalizacja zbioru danych**\n",
      "\n",
      "16. **[AKTUALIZACJA] Usunięcie kolumn identyfikujących i przetworzonych:** Usunięcie kolumn, które nie niosą wartości predykcyjnej lub zostały zastąpione nowymi cechami: `Transaction_ID`, `User_ID`, `Timestamp`.\n",
      "17. **[AKTUALIZACJA] Skalowanie cech w oparciu o ich charakterystykę:**\n",
      "    17.1. Zastosowanie `RobustScaler` dla cech numerycznych, w których w kroku 11 zidentyfikowano znaczące wartości odstające (np. `Transaction_Amount`, `Account_Balance`).\n",
      "    17.2. Zastosowanie `StandardScaler` dla pozostałych cech numerycznych w celu ich standaryzacji.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: CriticAgent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mCriticAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "Jako Krytyk, dokonuję rygorystycznej analizy zaktualizowanego planu przygotowania danych:\n",
      "\n",
      "### Analiza zaktualizowanego planu:\n",
      "\n",
      "**Faza 1: Wstępna walidacja i czyszczenie**\n",
      "1. **Weryfikacja integralności danych**\n",
      "   - **Problem**: Założenie, że ostatnie wystąpienie duplikatu jest najbardziej aktualne może być błędne bez dodatkowej analizy.\n",
      "   - **Alternatywa**: Dodać sprawdzenie znaczników czasowych, aby rzeczywiście potwierdzić, które wystąpienie jest najnowsze.\n",
      "   - **Wpływ**: Potencjalne usunięcie prawidłowych danych zamiast błędnych duplikatów.\n",
      "   - **Poziom ryzyka**: Niski\n",
      "\n",
      "2-3. **Analiza brakujących wartości i decyzja o usunięciu kolumn**\n",
      "   - **Problem**: Próg 40% jest arbitralny i może prowadzić do utraty cennych kolumn.\n",
      "   - **Alternatywa**: Zróżnicować próg w zależności od znaczenia kolumny dla zadania predykcyjnego.\n",
      "   - **Wpływ**: Możliwa utrata istotnych zmiennych objaśniających.\n",
      "   - **Poziom ryzyka**: Średni\n",
      "\n",
      "4. **Kompleksowa korekta typów danych**\n",
      "   - Krok jest dobrze zdefiniowany i atomowy.\n",
      "   - **Poziom ryzyka**: Niski\n",
      "\n",
      "**Faza 2: Obsługa brakujących wartości**\n",
      "5-6. **Imputacja wartości**\n",
      "   - Kroki są teraz lepiej zdefiniowane i uzasadnione.\n",
      "   - **Poziom ryzyka**: Niski\n",
      "\n",
      "**Faza 3: Inżynieria cech**\n",
      "7-10. **Ekstrakcja cech i tworzenie cech relacyjnych**\n",
      "   - Kroki zostały rozdzielone i są teraz atomowe, co jest zgodne z zaleceniami.\n",
      "   - **Poziom ryzyka**: Niski\n",
      "\n",
      "**Faza 4: Transformacja danych**\n",
      "11-12. **Analiza rozkładu i obsługa wartości odstających**\n",
      "   - **Problem**: Automatyczne stosowanie winsoryzacji na poziomie 1 i 99 percentyla bez analizy jej wpływu.\n",
      "   - **Alternatywa**: Po analizie rozkładu rozważyć różne techniki dla różnych kolumn (np. transformacja logarytmiczna dla skośnych rozkładów).\n",
      "   - **Wpływ**: Winsoryzacja może nie być optymalna dla wszystkich rozkładów.\n",
      "   - **Poziom ryzyka**: Niski\n",
      "\n",
      "13-15. **Analiza kardynalności i kodowanie zmiennych kategorycznych**\n",
      "   - Kroki są dobrze zdefiniowane i logicznie ustrukturyzowane.\n",
      "   - **Poziom ryzyka**: Niski\n",
      "\n",
      "**Faza 5: Finalizacja zbioru danych**\n",
      "16. **Usunięcie kolumn identyfikujących i przetworzonych**\n",
      "   - Krok jest jasno określony.\n",
      "   - **Poziom ryzyka**: Niski\n",
      "\n",
      "17. **Skalowanie cech w oparciu o ich charakterystykę**\n",
      "   - **Problem**: Decyzja o stosowaniu RobustScaler dla kolumn z wartościami odstającymi jest dobrym krokiem, ale brakuje jasnego kryterium, co stanowi \"znaczące wartości odstające\".\n",
      "   - **Alternatywa**: Zdefiniować konkretne kryteria (np. IQR > X) dla określenia, które kolumny wymagają RobustScaler.\n",
      "   - **Wpływ**: Niewielki na ogólną jakość modelu.\n",
      "   - **Poziom ryzyka**: Niski\n",
      "\n",
      "### Ogólna ocena:\n",
      "Plan został znacząco ulepszony. Kroki są teraz atomowe, a decyzje o transformacjach są poprzedzone odpowiednimi krokami analitycznymi. Zidentyfikowane problemy mają głównie niski poziom ryzyka i nie wpływają krytycznie na ogólną jakość planu.\n",
      "\n",
      "OSTATECZNY PLAN:\n",
      "### **ZAKTUALIZOWANY PLAN PRZYGOTOWANIA DANYCH (Wersja 2)**\n",
      "\n",
      "**Faza 1: Wstępna walidacja i czyszczenie**\n",
      "\n",
      "1.  **[AKTUALIZACJA] Weryfikacja integralności danych:** Sprawdzenie zbioru danych pod kątem zduplikowanych wierszy na podstawie `Transaction_ID`. W przypadku znalezienia duplikatów, usunięte zostaną wszystkie wystąpienia poza ostatnim, zakładając, że jest ono najbardziej aktualne.\n",
      "2.  **Analiza brakujących wartości:** Wygenerowanie raportu procentowego brakujących wartości (NaN/Null) dla każdej kolumny.\n",
      "3.  **[AKTUALIZACJA] Decyzja o usunięciu kolumn z brakami:** Ustalenie progu tolerancji dla brakujących danych na 40%. Kolumny, w których odsetek brakujących wartości przekracza ten próg, zostaną usunięte ze zbioru danych.\n",
      "4.  **[AKTUALIZACJA] Kompleksowa korekta typów danych:**\n",
      "    4.1. Konwersja kolumny `Timestamp` na typ `datetime`.\n",
      "    4.2. Weryfikacja i konwersja wszystkich kolumn numerycznych (`Transaction_Amount`, `Account_Balance`, `Daily_Transaction_Count`, etc.) do odpowiednich typów (`float64`, `int64`), aby zapobiec błędom w obliczeniach.\n",
      "    4.3. Weryfikacja, czy kolumny binarne (`IP_Address_Flag`, `Previous_Fraudulent_Activity`, `Is_Weekend`, `Fraud_Label`) mają spójny typ numeryczny (`int8`).\n",
      "\n",
      "**Faza 2: Obsługa brakujących wartości**\n",
      "\n",
      "5.  **[AKTUALIZACJA] Imputacja wartości numerycznych:** Uzupełnienie brakujących wartości w kolumnach numerycznych (`Account_Balance`, `Avg_Transaction_Amount_7d`, `Transaction_Distance`, `Risk_Score`) za pomocą mediany. Mediana jest preferowana nad średnią ze względu na jej odporność na wartości odstające, które są częste w danych finansowych.\n",
      "6.  **Imputacja wartości kategorycznych:** Uzupełnienie brakujących wartości w kolumnach kategorycznych (`Card_Type`, `Authentication_Method`) za pomocą najczęściej występującej wartości (mody) dla danej kolumny.\n",
      "\n",
      "**Faza 3: Inżynieria cech (Feature Engineering)**\n",
      "\n",
      "7.  **[AKTUALIZACJA] Ekstrakcja cechy: Godzina transakcji:** Na podstawie kolumny `Timestamp` stworzenie nowej cechy `Transaction_Hour`.\n",
      "8.  **[AKTUALIZACJA] Ekstrakcja cechy: Dzień tygodnia:** Na podstawie kolumny `Timestamp` stworzenie nowej cechy `Transaction_DayOfWeek`.\n",
      "9.  **[AKTUALIZACJA] Tworzenie cechy: Stosunek kwoty do salda:** Stworzenie nowej cechy `Amount_to_Balance_Ratio` (`Transaction_Amount` / `Account_Balance`). W przypadku `Account_Balance` równego zero, wartość zostanie ustawiona na 0.\n",
      "10. **[AKTUALIZACJA] Tworzenie cechy: Różnica kwoty od średniej:** Stworzenie nowej cechy `Amount_vs_Avg_7d_Diff` (`Transaction_Amount` - `Avg_Transaction_Amount_7d`).\n",
      "\n",
      "**Faza 4: Transformacja danych**\n",
      "\n",
      "11. **[AKTUALIZACJA] Analiza rozkładu danych numerycznych:** Wygenerowanie histogramów i wykresów pudełkowych dla kluczowych cech numerycznych (`Transaction_Amount`, `Account_Balance`, `Transaction_Distance`) w celu wizualnej identyfikacji wartości odstających i skośności rozkładu.\n",
      "12. **[AKTUALIZACJA] Obsługa wartości odstających:** Na podstawie analizy z kroku 11, ograniczenie ekstremalnych wartości w zidentyfikowanych kolumnach poprzez winsoryzację na poziomie 1. i 99. percentyla.\n",
      "13. **[AKTUALIZACJA] Analiza kardynalności cech kategorycznych:** Obliczenie liczby unikalnych wartości dla każdej kolumny kategorycznej (`Transaction_Type`, `Device_Type`, `Location`, `Merchant_Category`, `Card_Type`, `Authentication_Method`).\n",
      "14. **[AKTUALIZACJA] Kodowanie One-Hot Encoding:** Zastosowanie OHE dla kolumn o niskiej kardynalności (np. < 10 unikalnych wartości), zidentyfikowanych w kroku 13 (prawdopodobnie `Transaction_Type`, `Device_Type`, `Card_Type`, `Authentication_Method`).\n",
      "15. **[AKTUALIZACJA] Kodowanie binarne (Binary Encoding):** Zastosowanie kodowania binarnego dla kolumn o wysokiej kardynalności, zidentyfikowanych w kroku 13 (prawdopodobnie `Location`, `Merchant_Category`), w celu uniknięcia eksplozji wymiarowości.\n",
      "\n",
      "**Faza 5: Finalizacja zbioru danych**\n",
      "\n",
      "16. **[AKTUALIZACJA] Usunięcie kolumn identyfikujących i przetworzonych:** Usunięcie kolumn, które nie niosą wartości predykcyjnej lub zostały zastąpione nowymi cechami: `Transaction_ID`, `User_ID`, `Timestamp`.\n",
      "17. **[AKTUALIZACJA] Skalowanie cech w oparciu o ich charakterystykę:**\n",
      "    17.1. Zastosowanie `RobustScaler` dla cech numerycznych, w których w kroku 11 zidentyfikowano znaczące wartości odstające (np. `Transaction_Amount`, `Account_Balance`).\n",
      "    17.2. Zastosowanie `StandardScaler` dla pozostałych cech numerycznych w celu ich standaryzacji.\n",
      "PLAN_AKCEPTOWANY_PRZEJSCIE_DO_IMPLEMENTACJI\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (d68904bf-f44c-49fe-b0f6-6b41a32e447e): No next speaker selected\u001b[0m\n",
      "Faza planowania zakończona. Ostateczny plan został zaakceptowany.\n",
      "INFO: Próba zapisu pełnego logu rozmowy do pliku: reports/autogen_planning_conversation.log\n",
      "✅ SUKCES: Log rozmowy został pomyślnie zapisany.\n",
      "\n",
      "================================================================================\n",
      "### ### FAZA 2: URUCHAMIANIE WYKONANIA PLANU (LangGraph) ### ###\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found edge starting at unknown node 'memory_consolidation_node'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 146\u001b[39m\n\u001b[32m    143\u001b[39m workflow.add_edge(\u001b[33m\"\u001b[39m\u001b[33mmeta_auditor\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmemory_consolidation_node\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    144\u001b[39m workflow.add_edge(\u001b[33m\"\u001b[39m\u001b[33mmemory_consolidation_node\u001b[39m\u001b[33m\"\u001b[39m, END)\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m app = \u001b[43mworkflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m app_config = {\u001b[33m\"\u001b[39m\u001b[33mMAIN_AGENT\u001b[39m\u001b[33m\"\u001b[39m: MAIN_AGENT, \u001b[33m\"\u001b[39m\u001b[33mCODE_MODEL\u001b[39m\u001b[33m\"\u001b[39m: CODE_MODEL, \u001b[33m\"\u001b[39m\u001b[33mCRITIC_MODEL\u001b[39m\u001b[33m\"\u001b[39m: CRITIC_MODEL}\n\u001b[32m    150\u001b[39m initial_state = {\n\u001b[32m    151\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m\"\u001b[39m: app_config,\n\u001b[32m    152\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mplan\u001b[39m\u001b[33m\"\u001b[39m: final_plan, \n\u001b[32m   (...)\u001b[39m\u001b[32m    164\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mactive_policies\u001b[39m\u001b[33m\"\u001b[39m: active_policies\n\u001b[32m    165\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/agents_with_memory_py11/lib/python3.11/site-packages/langgraph/graph/state.py:822\u001b[39m, in \u001b[36mStateGraph.compile\u001b[39m\u001b[34m(self, checkpointer, cache, store, interrupt_before, interrupt_after, debug, name)\u001b[39m\n\u001b[32m    819\u001b[39m interrupt_after = interrupt_after \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[32m    821\u001b[39m \u001b[38;5;66;03m# validate the graph\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m822\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    823\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m \u001b[49m\u001b[43m!=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43minterrupt_after\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m \u001b[49m\u001b[43m!=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    828\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    830\u001b[39m \u001b[38;5;66;03m# prepare output channels\u001b[39;00m\n\u001b[32m    831\u001b[39m output_channels = (\n\u001b[32m    832\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m__root__\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.schemas[\u001b[38;5;28mself\u001b[39m.output_schema]) == \u001b[32m1\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    839\u001b[39m     ]\n\u001b[32m    840\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/agents_with_memory_py11/lib/python3.11/site-packages/langgraph/graph/state.py:749\u001b[39m, in \u001b[36mStateGraph.validate\u001b[39m\u001b[34m(self, interrupt)\u001b[39m\n\u001b[32m    747\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m source \u001b[38;5;129;01min\u001b[39;00m all_sources:\n\u001b[32m    748\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.nodes \u001b[38;5;129;01mand\u001b[39;00m source != START:\n\u001b[32m--> \u001b[39m\u001b[32m749\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound edge starting at unknown node \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    751\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m START \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m all_sources:\n\u001b[32m    752\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    753\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mGraph must have an entrypoint: add at least one edge from START to another node\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    754\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Found edge starting at unknown node 'memory_consolidation_node'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    files_to_exclude = {'Agents_beta (10).py','pack_project.ipynb', 'caly_projekt.txt'}\n",
    "    system_source_code = read_project_source_code(\".\", exclude_files=files_to_exclude)\n",
    "\n",
    "    # --- Inicjalizacja Pamięci i Uruchomienia ---\n",
    "    memory_client = MemoryBankClient(client=client, agent_engine=agent_engine)\n",
    "    run_id = str(uuid.uuid4())\n",
    "    \n",
    "    print(\"\\n--- ODPYTYWANIE PAMIĘCI O INSPIRACJE ---\")\n",
    "    inspiration_prompt = \"\"\n",
    "    dataset_signature = \"\"\n",
    "    try:\n",
    "        df_preview = pd.read_csv(INPUT_FILE_PATH, nrows=0)\n",
    "        dataset_signature = memory_client.create_dataset_signature(df_preview)\n",
    "        past_memories = memory_client.query_memory(\n",
    "            query_text=\"Najlepsze strategie i kluczowe wnioski dotyczące przetwarzania danych\",\n",
    "            scope={\"dataset_signature\": dataset_signature},\n",
    "            top_k=3\n",
    "        )\n",
    "        if past_memories:\n",
    "            inspirations = []\n",
    "            for mem in past_memories:\n",
    "                # ZMIANA: Używamy nowych, poprawnych typów wspomnień\n",
    "                if mem.memory_type == MemoryType.SUCCESSFUL_WORKFLOW and 'key_planning_insight' in mem.content:\n",
    "                    inspirations.append(f\"SPRAWDZONY WNIOSEK Z PROCESU: {mem.content['key_planning_insight']}\")\n",
    "                elif mem.memory_type == MemoryType.SUCCESSFUL_FIX and 'key_takeaway' in mem.content:\n",
    "                    inspirations.append(f\"NAUCZKA Z NAPRAWIONEGO BŁĘDU: {mem.content['key_takeaway']}\")\n",
    "            if inspirations:\n",
    "                inspiration_prompt = \"--- INSPIRACJE Z POPRZEDNICH URUCHOMIEŃ ---\\n\" + \"\\n\".join(inspirations)\n",
    "                print(\"INFO: Pomyślnie pobrano inspiracje z pamięci.\")\n",
    "        else:\n",
    "            print(\"INFO: Nie znaleziono inspiracji w pamięci dla tego typu danych.\")\n",
    "    except Exception as e:\n",
    "        print(f\"OSTRZEŻENIE: Nie udało się pobrać inspiracji z pamięci: {e}\")\n",
    "\n",
    "        \n",
    "        \n",
    "    active_policies = get_active_policies_from_memory(memory_client, dataset_signature)    \n",
    "    \n",
    "    # --- Krok 1: Faza planowania (AutoGen) ---\n",
    "    final_plan, autogen_log = run_autogen_planning_phase(\n",
    "        input_path=INPUT_FILE_PATH, \n",
    "        inspiration_prompt=inspiration_prompt,\n",
    "        trigger_agent=trigger_agent,\n",
    "        planner_agent=planner_agent,\n",
    "        critic_agent=critic_agent,\n",
    "        manager_agent_config=main_agent_configuration,\n",
    "        active_policies=active_policies\n",
    "    )\n",
    "    save_autogen_conversation_log(log_content=autogen_log, file_path=\"reports/autogen_planning_conversation.log\")\n",
    "\n",
    "    # --- Krok 2: Faza wykonania (LangGraph) ---\n",
    "    if final_plan:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"### ### FAZA 2: URUCHAMIANIE WYKONANIA PLANU (LangGraph) ### ###\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        workflow = StateGraph(AgentWorkflowState)\n",
    "        \n",
    "        # <<< ZMIANA TUTAJ: Zaktualizowana lista węzłów >>>\n",
    "        nodes = [\n",
    "            \"schema_reader\", \"code_generator\", \"architectural_validator\", \n",
    "            \"data_code_executor\", \"universal_debugger\", \"apply_code_fix\", \n",
    "            \"human_approval\", \"package_installer\", \"human_escalation\", \n",
    "            \"sync_report_code\",\"meta_auditor\",\n",
    "            # Nowe, wyspecjalizowane węzły raportujące:\n",
    "            \"summary_analyst\", \"plot_generator\", \"report_composer\",\"memory_consolidation\"\n",
    "        ]\n",
    "        for name in nodes: workflow.add_node(name, globals()[f\"{name}_node\"])\n",
    "\n",
    "        # Definicja krawędzi\n",
    "        workflow.set_entry_point(\"schema_reader\")\n",
    "        \n",
    "        # Ścieżka przetwarzania danych\n",
    "        workflow.add_edge(\"schema_reader\", \"code_generator\")\n",
    "        workflow.add_edge(\"code_generator\", \"architectural_validator\")\n",
    "        \n",
    "        def should_continue_or_debug(state: AgentWorkflowState) -> str:\n",
    "            if state.get(\"error_message\"):\n",
    "                if state.get(\"correction_attempts\", 0) >= MAX_CORRECTION_ATTEMPTS:\n",
    "                    return \"request_human_help\"\n",
    "                return \"call_debugger\"\n",
    "            return \"continue\"\n",
    "\n",
    "        workflow.add_conditional_edges(\n",
    "            \"architectural_validator\",\n",
    "            should_continue_or_debug,\n",
    "            {\"call_debugger\": \"universal_debugger\", \"request_human_help\": \"human_escalation\", \"continue\": \"data_code_executor\"}\n",
    "        )\n",
    "        workflow.add_conditional_edges(\n",
    "            \"data_code_executor\",\n",
    "            should_continue_or_debug,\n",
    "            {\"call_debugger\": \"universal_debugger\", \"request_human_help\": \"human_escalation\", \"continue\": \"summary_analyst\"}\n",
    "        )\n",
    "        \n",
    "        # <<< ZMIANA TUTAJ: Nowa ścieżka raportowania >>>\n",
    "        workflow.add_edge(\"commit_memory\", \"summary_analyst\")\n",
    "        workflow.add_conditional_edges(\n",
    "            \"summary_analyst\",\n",
    "            should_continue_or_debug,\n",
    "            {\"call_debugger\": \"universal_debugger\", \"request_human_help\": \"human_escalation\", \"continue\": \"plot_generator\"}\n",
    "        )\n",
    "        workflow.add_conditional_edges(\n",
    "            \"plot_generator\",\n",
    "            should_continue_or_debug,\n",
    "            {\"call_debugger\": \"universal_debugger\", \"request_human_help\": \"human_escalation\", \"continue\": \"report_composer\"}\n",
    "        )\n",
    "        workflow.add_conditional_edges(\n",
    "            \"report_composer\",\n",
    "            should_continue_or_debug,\n",
    "            {\"universal_debugger\": \"universal_debugger\", \"human_escalation\": \"human_escalation\", \"continue\": \"meta_auditor\"}\n",
    "        )\n",
    "\n",
    "        # Ścieżki naprawcze\n",
    "        workflow.add_edge(\"human_escalation\", \"meta_auditor\")\n",
    "        workflow.add_edge(\"package_installer\", \"data_code_executor\")\n",
    "\n",
    "        def route_after_fix(state):\n",
    "            failing_node = state.get(\"failing_node\")\n",
    "            # Po naprawie wraca do węzła, który zawiódł\n",
    "            if failing_node:\n",
    "                return failing_node\n",
    "            # Domyślnie wraca do walidacji\n",
    "            return \"architectural_validator\"\n",
    "\n",
    "        workflow.add_conditional_edges(\"apply_code_fix\", route_after_fix)\n",
    "\n",
    "        def route_from_debugger(state):\n",
    "            if state.get(\"tool_choice\") == \"propose_code_fix\":\n",
    "                return \"apply_code_fix\"\n",
    "            if state.get(\"tool_choice\") == \"request_package_installation\":\n",
    "                return \"human_approval\"\n",
    "            return \"human_escalation\"\n",
    "\n",
    "        workflow.add_conditional_edges(\"universal_debugger\", route_from_debugger)\n",
    "        workflow.add_conditional_edges(\"human_approval\", lambda s: s.get(\"user_approval_status\"), {\n",
    "            \"APPROVED\": \"package_installer\", \"REJECTED\": \"universal_debugger\"\n",
    "        })\n",
    "\n",
    "        \n",
    "        workflow.add_edge(\"human_escalation\", \"meta_auditor\")\n",
    "        workflow.add_edge(\"meta_auditor\", \"memory_consolidation_node\")\n",
    "        workflow.add_edge(\"memory_consolidation\", END)\n",
    "        \n",
    "        app = workflow.compile()\n",
    "        \n",
    "        app_config = {\"MAIN_AGENT\": MAIN_AGENT, \"CODE_MODEL\": CODE_MODEL, \"CRITIC_MODEL\": CRITIC_MODEL}\n",
    "        \n",
    "        initial_state = {\n",
    "            \"config\": app_config,\n",
    "            \"plan\": final_plan, \n",
    "            \"input_path\": INPUT_FILE_PATH,\n",
    "            \"output_path\": \"reports/processed_data.csv\",\n",
    "            \"report_output_path\": \"reports/transformation_report.html\",\n",
    "            \"correction_attempts\": 0, \n",
    "            \"correction_history\": [],\n",
    "            \"source_code\": system_source_code,\n",
    "            \"autogen_log\": autogen_log,\n",
    "            \"memory_client\": memory_client,\n",
    "            \"run_id\": run_id,\n",
    "            \"dataset_signature\": dataset_signature,\n",
    "            \"pending_fix_session\": None,\n",
    "            \"active_policies\": active_policies\n",
    "        }\n",
    "        \n",
    "        langgraph_log = \"\"\n",
    "        final_run_state = initial_state.copy()\n",
    "        \n",
    "        for event in app.stream(initial_state, {\"recursion_limit\": 50}):\n",
    "            for node_name, state_update in event.items():\n",
    "                if \"__end__\" not in node_name:\n",
    "                    print(f\"--- Krok: '{node_name}' ---\")\n",
    "                    if state_update:\n",
    "                        printable_update = state_update.copy()\n",
    "                        for key in [\"generated_code\", \"corrected_code\", \"generated_report_code\", \"error_context_code\", \"plot_generation_code\", \"summary_html\"]:\n",
    "                            if key in printable_update and printable_update[key]:\n",
    "                                print(f\"--- {key.upper()} ---\")\n",
    "                                print(printable_update[key])\n",
    "                                print(\"-\" * (len(key) + 8))\n",
    "                                del printable_update[key]\n",
    "                        if printable_update:\n",
    "                            print(json.dumps(printable_update, indent=2, default=str))\n",
    "                        \n",
    "                        log_line = f\"--- Krok: '{node_name}' ---\\n{json.dumps(state_update, indent=2, default=str)}\\n\"\n",
    "                        langgraph_log += log_line\n",
    "                        final_run_state.update(state_update)\n",
    "                    else:\n",
    "                        print(\"  [INFO] Węzeł zakończył pracę bez aktualizacji stanu.\")\n",
    "                    print(\"-\" * 20 + \"\\n\")\n",
    "\n",
    "        save_langgraph_execution_log(log_content=langgraph_log, file_path=\"reports/langgraph_execution.log\")\n",
    "\n",
    "        final_run_state['langgraph_log'] = langgraph_log\n",
    "        meta_auditor_node(final_run_state)\n",
    "\n",
    "        print(\"\\n\\n--- ZAKOŃCZONO PRACĘ GRAFU I AUDYT ---\")\n",
    "    else:\n",
    "        print(\"Proces zakończony. Brak planu do wykonania.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c476ce17-60f1-4436-8754-d2c7210310c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d9a675-049c-4ac3-b518-a077cc26664f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "agents_with_memory_p11",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Agents with memory (Python 3.11)",
   "language": "python",
   "name": "agents_with_memory_p11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
