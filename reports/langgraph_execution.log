========================================
### PE≈ÅNY ZAPIS WYKONANIA GRAFU LANGGRAPH (FAZA WYKONANIA) ###
========================================

--- Krok: 'schema_reader' ---
{
  "available_columns": [
    "Transaction_ID",
    "User_ID",
    "Transaction_Amount",
    "Transaction_Type",
    "Timestamp",
    "Account_Balance",
    "Device_Type",
    "Location",
    "Merchant_Category",
    "IP_Address_Flag",
    "Previous_Fraudulent_Activity",
    "Daily_Transaction_Count",
    "Avg_Transaction_Amount_7d",
    "Failed_Transaction_Count_7d",
    "Card_Type",
    "Card_Age",
    "Transaction_Distance",
    "Authentication_Method",
    "Risk_Score",
    "Is_Weekend",
    "Fraud_Label"
  ],
  "dataset_signature": "ae1568fe7dae11d4bacd0c21ed718503"
}
--- Krok: 'code_generator' ---
{
  "generated_code": "# Krok 1: Import niezb\u0119dnych bibliotek\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef process_data(input_path: str, output_path: str):\n    \"\"\"\n    Kompletny pipeline przetwarzania danych zgodny z planem biznesowym.\n    \"\"\"\n    \n    # Krok 2: Wczytanie danych\n    try:\n        df = pd.read_csv(input_path)\n        print(f\"Dane wczytane pomy\u015blnie. Kszta\u0142t: {df.shape}\")\n    except Exception as e:\n        print(f\"B\u0142\u0105d podczas wczytywania danych: {e}\")\n        return\n    \n    # Krok 3: Weryfikacja integralno\u015bci\n    print(\"\\n=== ANALIZA WST\u0118PNA ===\")\n    print(\"Info o danych:\")\n    print(df.info())\n    print(\"\\nOpis statystyczny:\")\n    print(df.describe(include='all'))\n    print(\"\\nBrakuj\u0105ce warto\u015bci:\")\n    print(df.isnull().sum())\n    \n    # Krok 4: Analiza zmiennej docelowej\n    print(\"\\n=== ANALIZA ZMIENNEJ DOCELOWEJ ===\")\n    fraud_distribution = df['Fraud_Label'].value_counts(normalize=True)\n    print(\"Rozk\u0142ad Fraud_Label:\")\n    print(fraud_distribution)\n    \n    # Krok 5: Planowanie strategii dla niezbalansowanych klas\n    fraud_ratio = fraud_distribution.min()\n    if fraud_ratio < 0.1:\n        print(f\"Wykryto znacz\u0105c\u0105 nier\u00f3wnowag\u0119 klas (mniejszo\u015b\u0107: {fraud_ratio:.3f})\")\n        print(\"Zalecana strategia: SMOTE lub class_weight w modelu\")\n    \n    # Krok 6: Konwersja Timestamp\n    try:\n        df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n        print(\"Timestamp przekonwertowany na datetime\")\n    except Exception as e:\n        print(f\"B\u0142\u0105d konwersji Timestamp: {e}\")\n    \n    # Krok 7: Zarz\u0105dzanie identyfikatorami - usu\u0144 Transaction_ID\n    if 'Transaction_ID' in df.columns:\n        df = df.drop('Transaction_ID', axis=1)\n        print(\"Transaction_ID usuni\u0119te\")\n    \n    # Krok 8: Ekstrakcja cech czasowych\n    df['Transaction_Hour'] = df['Timestamp'].dt.hour\n    df['Transaction_DayOfWeek'] = df['Timestamp'].dt.dayofweek\n    df['Transaction_Month'] = df['Timestamp'].dt.month\n    print(\"Cechy czasowe utworzone\")\n    \n    # Krok 9: Utworzenie cechy binarnej Is_Night_Transaction\n    df['Is_Night_Transaction'] = ((df['Transaction_Hour'] >= 23) | (df['Transaction_Hour'] <= 5)).astype(int)\n    print(\"Cecha Is_Night_Transaction utworzona\")\n    \n    # Krok 10: Agregacja cech na poziomie u\u017cytkownika\n    user_stats = df.groupby('User_ID').agg({\n        'Transaction_Amount': ['mean', 'count'],\n        'Merchant_Category': 'nunique'\n    }).round(2)\n    \n    user_stats.columns = ['User_Avg_Transaction_Amount', 'User_Transaction_Count', 'User_Distinct_Merchant_Count']\n    user_stats = user_stats.reset_index()\n    \n    # Po\u0142\u0105cz z g\u0142\u00f3wnym DataFrame\n    df = df.merge(user_stats, on='User_ID', how='left')\n    print(\"Cechy agregowane na poziomie u\u017cytkownika utworzone\")\n    \n    # Krok 11: Usuni\u0119cie oryginalnych kolumn\n    df = df.drop(['Timestamp', 'User_ID'], axis=1)\n    print(\"Timestamp i User_ID usuni\u0119te\")\n    \n    # Krok 12: Analiza kardynalno\u015bci zmiennych kategorycznych\n    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n    print(\"\\n=== ANALIZA KARDYNALNO\u015aCI ===\")\n    cardinality_info = {}\n    for col in categorical_cols:\n        unique_count = df[col].nunique()\n        cardinality_info[col] = unique_count\n        print(f\"{col}: {unique_count} unikalnych warto\u015bci\")\n    \n    # Krok 13: Kodowanie zmiennych kategorycznych\n    low_cardinality_threshold = 20\n    \n    # One-Hot Encoding dla niskiej kardynalno\u015bci\n    low_card_cols = [col for col, count in cardinality_info.items() if count <= low_cardinality_threshold]\n    if low_card_cols:\n        df_encoded = pd.get_dummies(df, columns=low_card_cols, prefix=low_card_cols)\n        print(f\"One-Hot Encoding zastosowane dla: {low_card_cols}\")\n    else:\n        df_encoded = df.copy()\n    \n    # Target Encoding dla wysokiej kardynalno\u015bci\n    high_card_cols = [col for col, count in cardinality_info.items() if count > low_cardinality_threshold]\n    for col in high_card_cols:\n        if col in df_encoded.columns:\n            target_mean = df_encoded.groupby(col)['Fraud_Label'].mean()\n            df_encoded[f'{col}_target_encoded'] = df_encoded[col].map(target_mean)\n            df_encoded = df_encoded.drop(col, axis=1)\n            print(f\"Target Encoding zastosowane dla: {col}\")\n    \n    df = df_encoded.copy()\n    \n    # Krok 14: Podzia\u0142 na zbi\u00f3r treningowy i testowy\n    X = df.drop('Fraud_Label', axis=1)\n    y = df['Fraud_Label']\n    \n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42, stratify=y\n    )\n    print(f\"\\nPodzia\u0142 danych - Train: {X_train.shape}, Test: {X_test.shape}\")\n    \n    # Krok 15: Kontekstowa imputacja warto\u015bci\n    numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n    \n    # Imputacja dla Transaction_Amount i Account_Balance median\u0105\n    amount_balance_cols = [col for col in ['Transaction_Amount', 'Account_Balance'] if col in numeric_cols]\n    if amount_balance_cols:\n        imputer_median = SimpleImputer(strategy='median')\n        imputer_median.fit(X_train[amount_balance_cols])\n        X_train[amount_balance_cols] = imputer_median.transform(X_train[amount_balance_cols])\n        X_test[amount_balance_cols] = imputer_median.transform(X_test[amount_balance_cols])\n    \n    # Imputacja dla pozosta\u0142ych kolumn numerycznych\n    other_numeric_cols = [col for col in numeric_cols if col not in amount_balance_cols]\n    if other_numeric_cols:\n        imputer_mean = SimpleImputer(strategy='mean')\n        imputer_mean.fit(X_train[other_numeric_cols])\n        X_train[other_numeric_cols] = imputer_mean.transform(X_train[other_numeric_cols])\n        X_test[other_numeric_cols] = imputer_mean.transform(X_test[other_numeric_cols])\n    \n    print(\"Imputacja warto\u015bci zako\u0144czona\")\n    \n    # Krok 16: Analiza korelacji\n    correlation_matrix = X_train.corr()\n    high_corr_threshold = 0.9\n    \n    # Znajd\u017a pary wysoce skorelowanych cech\n    high_corr_pairs = []\n    for i in range(len(correlation_matrix.columns)):\n        for j in range(i+1, len(correlation_matrix.columns)):\n            if abs(correlation_matrix.iloc[i, j]) > high_corr_threshold:\n                col1, col2 = correlation_matrix.columns[i], correlation_matrix.columns[j]\n                high_corr_pairs.append((col1, col2, correlation_matrix.iloc[i, j]))\n    \n    print(f\"\\nZnaleziono {len(high_corr_pairs)} par wysoce skorelowanych cech\")\n    \n    # Krok 17: Redukcja cech skorelowanych\n    cols_to_remove = set()\n    for col1, col2, corr_val in high_corr_pairs:\n        # Oblicz korelacj\u0119 ka\u017cdej cechy ze zmienn\u0105 docelow\u0105\n        corr_col1_target = abs(X_train[col1].corr(y_train))\n        corr_col2_target = abs(X_train[col2].corr(y_train))\n        \n        # Usu\u0144 cech\u0119 z ni\u017csz\u0105 korelacj\u0105 ze zmienn\u0105 docelow\u0105\n        if corr_col1_target < corr_col2_target:\n            cols_to_remove.add(col1)\n        else:\n            cols_to_remove.add(col2)\n    \n    if cols_to_remove:\n        X_train = X_train.drop(columns=list(cols_to_remove))\n        X_test = X_test.drop(columns=list(cols_to_remove))\n        print(f\"Usuni\u0119to {len(cols_to_remove)} wysoce skorelowanych cech\")\n    \n    # Krok 18: Obs\u0142uga warto\u015bci odstaj\u0105cych - Winsoryzacja\n    numeric_cols_updated = X_train.select_dtypes(include=[np.number]).columns.tolist()\n    \n    winsorize_limits = {}\n    for col in numeric_cols_updated:\n        # Oblicz percentyle na zbiorze treningowym\n        lower_percentile = np.percentile(X_train[col], 5)\n        upper_percentile = np.percentile(X_train[col], 95)\n        winsorize_limits[col] = (lower_percentile, upper_percentile)\n        \n        # Zastosuj winsoryzacj\u0119\n        X_train[col] = np.clip(X_train[col], lower_percentile, upper_percentile)\n        X_test[col] = np.clip(X_test[col], lower_percentile, upper_percentile)\n    \n    print(\"Winsoryzacja warto\u015bci odstaj\u0105cych zako\u0144czona\")\n    \n    # Krok 19: Skalowanie cech\n    # Identyfikuj cechy z outlierami (wysokie IQR)\n    robust_cols = []\n    standard_cols = []\n    \n    for col in numeric_cols_updated:\n        q75, q25 = np.percentile(X_train[col], [75, 25])\n        iqr = q75 - q25\n        if iqr > X_train[col].std():  # Je\u015bli IQR > std, u\u017cyj RobustScaler\n            robust_cols.append(col)\n        else:\n            standard_cols.append(col)\n    \n    # RobustScaler dla cech z outlierami\n    if robust_cols:\n        robust_scaler = RobustScaler()\n        robust_scaler.fit(X_train[robust_cols])\n        X_train[robust_cols] = robust_scaler.transform(X_train[robust_cols])\n        X_test[robust_cols] = robust_scaler.transform(X_test[robust_cols])\n    \n    # StandardScaler dla pozosta\u0142ych\n    if standard_cols:\n        standard_scaler = StandardScaler()\n        standard_scaler.fit(X_train[standard_cols])\n        X_train[standard_cols] = standard_scaler.transform(X_train[standard_cols])\n        X_test[standard_cols] = standard_scaler.transform(X_test[standard_cols])\n    \n    print(\"Skalowanie cech zako\u0144czone\")\n    \n    # Krok 20: Ostateczna weryfikacja\n    print(\"\\n=== OSTATECZNA WERYFIKACJA ===\")\n    print(f\"X_train shape: {X_train.shape}\")\n    print(f\"X_test shape: {X_test.shape}\")\n    print(f\"Brakuj\u0105ce warto\u015bci w X_train: {X_train.isnull().sum().sum()}\")\n    print(f\"Brakuj\u0105ce warto\u015bci w X_test: {X_test.isnull().sum().sum()}\")\n    print(f\"Wszystkie kolumny numeryczne w X_train: {X_train.select_dtypes(include=[np.number]).shape[1] == X_train.shape[1]}\")\n    print(f\"Wszystkie kolumny numeryczne w X_test: {X_test.select_dtypes(include=[np.number]).shape[1] == X_test.shape[1]}\")\n    \n    # Zapisz przetworzone dane\n    try:\n        # Po\u0142\u0105cz dane treningowe i testowe z odpowiednimi etykietami\n        train_data = X_train.copy()\n        train_data['Fraud_Label'] = y_train.values\n        train_data['dataset_split'] = 'train'\n        \n        test_data = X_test.copy()\n        test_data['Fraud_Label'] = y_test.values\n        test_data['dataset_split'] = 'test'\n        \n        final_data = pd.concat([train_data, test_data], ignore_index=True)\n        final_data.to_csv(output_path, index=False)\n        print(f\"\\nDane zapisane pomy\u015blnie do: {output_path}\")\n        print(f\"Finalne wymiary: {final_data.shape}\")\n        \n    except Exception as e:\n        print(f\"B\u0142\u0105d podczas zapisywania: {e}\")\n\nprocess_data(input_path, output_path)  # noqa: F821"
}
--- Krok: 'architectural_validator' ---
{
  "error_message": null,
  "pending_fix_session": null
}
--- Krok: 'data_code_executor' ---
{
  "error_message": null,
  "correction_attempts": 0
}
--- Krok: 'summary_analyst' ---
{
  "summary_html": "<h2>Podsumowanie Transformacji Danych - Kluczowe Korzy\u015bci</h2>\n\n<h4>Eliminacja Brakuj\u0105cych Danych</h4>\n<ul>\n<li><strong>100% kompletno\u015b\u0107 danych:</strong> Wszystkie 50,000 rekord\u00f3w zachowa\u0142o pe\u0142n\u0105 integralno\u015b\u0107 - brak warto\u015bci NaN w finalnym zbiorze</li>\n<li><strong>Zaawansowana imputacja:</strong> Zastosowano kontekstow\u0105 imputacj\u0119 warto\u015bci z wykorzystaniem median dla kluczowych zmiennych finansowych</li>\n</ul>\n\n<h4>Optymalizacja Struktury Danych</h4>\n<ul>\n<li><strong>Rozszerzenie wymiarowo\u015bci:</strong> Zwi\u0119kszenie liczby kolumn z 21 do 45 poprzez inteligentn\u0105 in\u017cynieri\u0119 cech</li>\n<li><strong>Standaryzacja typ\u00f3w:</strong> Konwersja wszystkich zmiennych predykcyjnych do formatu numerycznego (float64/bool) zapewniaj\u0105ca kompatybilno\u015b\u0107 z algorytmami ML</li>\n<li><strong>Eliminacja redundancji:</strong> Usuni\u0119cie identyfikator\u00f3w (Transaction_ID, User_ID) i przekszta\u0142cenie zmiennych czasowych</li>\n</ul>\n\n<h4>Wzbogacenie Informacyjne</h4>\n<ul>\n<li><strong>Nowe cechy czasowe:</strong> Ekstrakcja 4 nowych zmiennych z Timestamp (godzina, dzie\u0144 tygodnia, miesi\u0105c, transakcje nocne)</li>\n<li><strong>Agregacje u\u017cytkownik\u00f3w:</strong> Utworzenie 3 cech behawioralnych na poziomie u\u017cytkownika zwi\u0119kszaj\u0105cych si\u0142\u0119 predykcyjn\u0105</li>\n<li><strong>One-Hot Encoding:</strong> Przekszta\u0142cenie 7 zmiennych kategorycznych na 25 zmiennych binarnych dla lepszej interpretacji przez modele</li>\n</ul>\n\n<h4>Standaryzacja i Skalowanie</h4>\n<ul>\n<li><strong>Normalizacja rozk\u0142ad\u00f3w:</strong> Zastosowanie RobustScaler i StandardScaler zapewniaj\u0105ce stabilno\u015b\u0107 numeryczn\u0105</li>\n<li><strong>Kontrola warto\u015bci odstaj\u0105cych:</strong> Implementacja Winsoryzacji chroni\u0105cej przed ekstremalnymi warto\u015bciami</li>\n<li><strong>Redukcja korelacji:</strong> Eliminacja wysoce skorelowanych cech (|r| > 0.9) optymalizuj\u0105ca wydajno\u015b\u0107 modeli</li>\n</ul>\n\n<h4>Gotowo\u015b\u0107 do Modelowania</h4>\n<ul>\n<li><strong>Podzia\u0142 train/test:</strong> Prawid\u0142owe rozdzielenie danych zapobiegaj\u0105ce wyciekowi informacji</li>\n<li><strong>Balansowanie klas:</strong> Zidentyfikowana strategia dla niezbalansowanych klas (32.1% fraud vs 67.9% legitimate)</li>\n<li><strong>Pe\u0142na kompatybilno\u015b\u0107 ML:</strong> Finalne dane w 100% gotowe do zastosowania w algorytmach uczenia maszynowego</li>\n</ul>"
}
--- Krok: 'plot_generator' ---
{
  "plot_generation_code": "# Krok 0: Inicjalizacja listy do przechowywania figur\nfigures_to_embed = []\n\n# --- Wizualizacja 1: Ilustracja obs\u0142ugi warto\u015bci odstaj\u0105cych (Krok 18) ---\n# Por\u00f3wnanie rozk\u0142adu 'Transaction_Amount' przed i po Winsoryzacji.\nfig1, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n# Wykres przed transformacj\u0105\nax1.boxplot(df_original['Transaction_Amount'], vert=False)\nax1.set_title('Przed obs\u0142ug\u0105 outlier\u00f3w')\nax1.set_xlabel('Kwota Transakcji')\nax1.set_yticklabels([''])\n\n# Wykres po transformacji\nax2.boxplot(df_processed['Transaction_Amount'], vert=False)\nax2.set_title('Po obs\u0142udze outlier\u00f3w (Winsoryzacja)')\nax2.set_xlabel('Kwota Transakcji')\nax2.set_yticklabels([''])\n\nfig1.suptitle(\"Ilustracja obs\u0142ugi warto\u015bci odstaj\u0105cych dla 'Transaction_Amount'\", fontsize=16)\nfig1.tight_layout(rect=[0, 0.03, 1, 0.95])\nfigures_to_embed.append(fig1)\n\n# --- Wizualizacja 2: Ilustracja skalowania cech (Krok 19) ---\n# Por\u00f3wnanie rozk\u0142adu 'Account_Balance' przed i po skalowaniu.\nfig2, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n# Wykres przed transformacj\u0105\nax1.hist(df_original['Account_Balance'], bins=50, color='skyblue', edgecolor='black')\nax1.set_title('Przed skalowaniem')\nax1.set_xlabel('Saldo Konta')\nax1.set_ylabel('Cz\u0119stotliwo\u015b\u0107')\n\n# Wykres po transformacji\nax2.hist(df_processed['Account_Balance'], bins=50, color='lightgreen', edgecolor='black')\nax2.set_title('Po skalowaniu (RobustScaler)')\nax2.set_xlabel('Przeskalowane Saldo Konta')\nax2.set_ylabel('Cz\u0119stotliwo\u015b\u0107')\n\nfig2.suptitle(\"Ilustracja skalowania cechy 'Account_Balance'\", fontsize=16)\nfig2.tight_layout(rect=[0, 0.03, 1, 0.95])\nfigures_to_embed.append(fig2)\n\n# --- Wizualizacja 3: Ilustracja in\u017cynierii cech czasowych (Kroki 7 i 8) ---\n# Pokazanie rozk\u0142ad\u00f3w nowo utworzonych cech: 'Transaction_Hour' i 'Is_Night_Transaction'.\nfig3, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n\n# Wykres dla 'Transaction_Hour'\nax1.hist(df_processed['Transaction_Hour'], bins=24, color='#FFC300', edgecolor='black')\nax1.set_title('Rozk\u0142ad nowej cechy: Godzina transakcji')\nax1.set_xlabel('Godzina Dnia')\nax1.set_ylabel('Liczba Transakcji')\nax1.set_xticks(range(0, 25, 2))\n\n# Wykres dla 'Is_Night_Transaction'\nnight_counts = df_processed['Is_Night_Transaction'].value_counts()\nax2.bar(night_counts.index, night_counts.values, color=['#581845', '#C70039'])\nax2.set_title('Rozk\u0142ad nowej cechy: Transakcja nocna')\nax2.set_xlabel('Czy transakcja nocna?')\nax2.set_ylabel('Liczba Transakcji')\nax2.set_xticks([0, 1])\nax2.set_xticklabels(['Nie (0)', 'Tak (1)'])\n\nfig3.suptitle(\"Ilustracja in\u017cynierii cech czasowych\", fontsize=16)\nfig3.tight_layout(rect=[0, 0.03, 1, 0.95])\nfigures_to_embed.append(fig3)\n\n# --- Wizualizacja 4: Ilustracja in\u017cynierii cech zagregowanych (Krok 9) ---\n# Pokazanie rozk\u0142ad\u00f3w nowo utworzonych cech na poziomie u\u017cytkownika.\nfig4, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n\n# Wykres dla 'User_Avg_Transaction_Amount'\nax1.hist(df_processed['User_Avg_Transaction_Amount'], bins=50, color='#00A896', edgecolor='black')\nax1.set_title('\u015arednia kwota transakcji u\u017cytkownika')\nax1.set_xlabel('\u015arednia kwota')\nax1.set_ylabel('Liczba u\u017cytkownik\u00f3w')\n\n# Wykres dla 'User_Transaction_Count'\nax2.hist(df_processed['User_Transaction_Count'], bins=50, color='#02C39A', edgecolor='black')\nax2.set_title('Liczba transakcji u\u017cytkownika')\nax2.set_xlabel('Ca\u0142kowita liczba transakcji')\nax2.set_ylabel('Liczba u\u017cytkownik\u00f3w')\n\nfig4.suptitle(\"Ilustracja in\u017cynierii cech zagregowanych na poziomie u\u017cytkownika\", fontsize=16)\nfig4.tight_layout(rect=[0, 0.03, 1, 0.95])\nfigures_to_embed.append(fig4)"
}
