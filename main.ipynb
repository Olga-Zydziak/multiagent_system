{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b251096d-f8c3-4dfb-ae10-5d33be45f50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import json\n",
    "import vertexai\n",
    "from vertexai import agent_engines\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, List, Callable, Dict, Optional, Union, Any\n",
    "# Importy z własnych modułów\n",
    "from config import PROJECT_ID, LOCATION, MEMORY_ENGINE_DISPLAY_NAME, INPUT_FILE_PATH,MAIN_AGENT,CRITIC_MODEL,CODE_MODEL, API_TYPE_GEMINI,API_TYPE_SONNET, ANTHROPIC_API_KEY,basic_config_agent\n",
    "from agents.state import AgentWorkflowState\n",
    "from agents.autogen_agents import TriggerAgent,PlannerAgent,CriticAgent\n",
    "from prompts import LangchainAgentsPrompts,AutoGenAgentsPrompts\n",
    "from agents.langgraph_nodes import * \n",
    "from agents.autogen_agent_utils import run_autogen_planning_phase\n",
    "from memory.memory_bank_client import MemoryBankClient\n",
    "from tools.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b380a6ac-127b-44dd-9e3b-e0721815cd25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "AGENT_ENGINE_NAME = \"\" # Zostanie wypełniona po pobraniu lub utworzeniu silnika\n",
    "\n",
    "# Inicjalizacja głównego klienta Vertex AI\n",
    "client = vertexai.Client(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ba1b166-e3c9-4d11-9a08-76336faaa064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_agent_engine(display_name: str) :\n",
    "    \"\"\"\n",
    "    Pobiera istniejący Agent Engine po nazwie wyświetlanej lub tworzy nowy, jeśli nie istnieje.\n",
    "    \"\"\"\n",
    "    # 1. Pobierz listę wszystkich istniejących silników w projekcie\n",
    "    all_engines = agent_engines.list()\n",
    "    \n",
    "    # 2. Sprawdź, czy któryś z nich ma pasującą nazwę\n",
    "    for engine in all_engines:\n",
    "        if engine.display_name == display_name:\n",
    "            print(f\"INFO: Znaleziono i połączono z istniejącym Agent Engine: '{display_name}'\")\n",
    "            return engine\n",
    "            \n",
    "    # 3. Jeśli pętla się zakończyła i nic nie znaleziono, stwórz nowy silnik\n",
    "    print(f\"INFO: Nie znaleziono Agent Engine o nazwie '{display_name}'. Tworzenie nowego...\")\n",
    "    try:\n",
    "        new_engine = agent_engines.create(\n",
    "            display_name=display_name\n",
    "        )\n",
    "        print(f\"INFO: Pomyślnie utworzono nowy Agent Engine.\")\n",
    "        return new_engine\n",
    "    except Exception as e:\n",
    "        print(f\"KRYTYCZNY BŁĄD: Nie można utworzyć Agent Engine. Sprawdź konfigurację i uprawnienia. Błąd: {e}\")\n",
    "        exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80731513-5d98-4048-89f8-359410538a59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Znaleziono i połączono z istniejącym Agent Engine: 'memory-gamma-way'\n",
      "projects/815755318672/locations/us-central1/reasoningEngines/3849548538518175744\n"
     ]
    }
   ],
   "source": [
    "agent_engine =get_or_create_agent_engine(MEMORY_ENGINE_DISPLAY_NAME)\n",
    "AGENT_ENGINE_NAME = agent_engine.resource_name\n",
    "print(AGENT_ENGINE_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6982c7b4-a6dd-476f-b361-d36c50174185",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Konfiguracja czatu grupowego ---\n",
    "main_agent_configuration={\"cache_seed\": 42,\"seed\": 42,\"temperature\": 0.0,\n",
    "                        \"config_list\": basic_config_agent(agent_name=MAIN_AGENT, api_type=API_TYPE_GEMINI, location=LOCATION, project_id=PROJECT_ID)}\n",
    "critic_agent_configuration ={\"cache_seed\": 42,\"seed\": 42,\"temperature\": 0.0,\n",
    "                        \"config_list\": basic_config_agent(api_key=ANTHROPIC_API_KEY,agent_name=CRITIC_MODEL, api_type=API_TYPE_SONNET)}\n",
    "trigger_prompt = str(AutoGenAgentsPrompts.Trigger_prompt())\n",
    "planner_prompt = str(AutoGenAgentsPrompts.Planner_prompt())\n",
    "critic_prompt = str(AutoGenAgentsPrompts.Critic_prompt())\n",
    "#---WYWOŁANIE AGENTÓW\n",
    "trigger_agent = TriggerAgent(llm_config=main_agent_configuration, prompt=trigger_prompt)\n",
    "planner_agent = PlannerAgent(llm_config=main_agent_configuration,prompt=planner_prompt)\n",
    "critic_agent = CriticAgent(llm_config=main_agent_configuration,prompt=critic_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffc80bb-b8e4-424c-b3b7-96dc37607fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9d12db3-dfe5-4c56-9494-8721eccacda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: MemoryBankClient gotowy do pracy z silnikiem: projects/815755318672/locations/us-central1/reasoningEngines/3849548538518175744\n",
      "\n",
      "--- ODPYTYWANIE PAMIĘCI O INSPIRACJE ---\n",
      "INFO: Odpytuję pamięć semantycznie z zapytaniem 'Najlepsze strategie i kluczowe wnioski dotyczące przetwarzania danych' w zakresie {'dataset_signature': 'ae1568fe7dae11d4bacd0c21ed718503'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/olga_zydziak/version_beta/multiagent_system/memory/memory_bank_client.py:95: ExperimentalWarning: The Vertex SDK GenAI agent engines module is experimental, and may change in future versions.\n",
      "  memories_iterator = self.client.agent_engines.retrieve_memories(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "udany plan: id='d9bd7bbf-d8e9-4418-8d1b-ffdfcb8ef4ac' run_id='30c84df2-164d-462e-8605-cc23d1c40a59' timestamp=datetime.datetime(2025, 8, 4, 9, 16, 20, 565870) memory_type=<MemoryType.SUCCESSFUL_PLAN: 'SUCCESSFUL_PLAN'> dataset_signature='ae1568fe7dae11d4bacd0c21ed718503' source_node='meta_auditor_node' content={'plan_summary': 'N/A - distillation failed'} metadata={'importance_score': 0.8}\n",
      "udany plan: id='afdf0d49-4d7e-4bd5-a83d-c9a0c6fcf180' run_id='9d0372a5-31ee-4086-aa8c-a61be76c14a4' timestamp=datetime.datetime(2025, 8, 3, 23, 42, 27, 979172) memory_type=<MemoryType.SUCCESSFUL_PLAN: 'SUCCESSFUL_PLAN'> dataset_signature='ae1568fe7dae11d4bacd0c21ed718503' source_node='meta_auditor_node' content={'plan_summary': 'N/A - distillation failed'} metadata={'importance_score': 0.8}\n",
      "udany plan: id='98d5815d-a4dd-41e7-82f4-31352aec9302' run_id='1d4eb978-fe14-47e3-bb65-97355dd8f1bb' timestamp=datetime.datetime(2025, 8, 4, 10, 57, 48, 385507) memory_type=<MemoryType.SUCCESSFUL_PLAN: 'SUCCESSFUL_PLAN'> dataset_signature='ae1568fe7dae11d4bacd0c21ed718503' source_node='meta_auditor_node' content={'plan_summary': 'N/A - distillation failed'} metadata={'importance_score': 0.8}\n",
      "INFO: Znaleziono i poprawnie przetworzono 3 pasujących wspomnień.\n",
      "\n",
      "================================================================================\n",
      "### ### FAZA 1: URUCHAMIANIE PLANOWANIA STRATEGICZNEGO (AutoGen) ### ###\n",
      "================================================================================\n",
      "\n",
      "\u001b[33mUserProxy\u001b[0m (to chat_manager):\n",
      "\n",
      "Oto podgląd danych:\n",
      "\n",
      "Kolumny:\n",
      "['Transaction_ID', 'User_ID', 'Transaction_Amount', 'Transaction_Type', 'Timestamp', 'Account_Balance', 'Device_Type', 'Location', 'Merchant_Category', 'IP_Address_Flag', 'Previous_Fraudulent_Activity', 'Daily_Transaction_Count', 'Avg_Transaction_Amount_7d', 'Failed_Transaction_Count_7d', 'Card_Type', 'Card_Age', 'Transaction_Distance', 'Authentication_Method', 'Risk_Score', 'Is_Weekend', 'Fraud_Label']\n",
      "\n",
      "Pierwsze 5 wierszy:\n",
      "  Transaction_ID    User_ID  Transaction_Amount Transaction_Type            Timestamp  Account_Balance Device_Type  Location Merchant_Category  IP_Address_Flag  Previous_Fraudulent_Activity  Daily_Transaction_Count  Avg_Transaction_Amount_7d  Failed_Transaction_Count_7d   Card_Type  Card_Age  Transaction_Distance Authentication_Method  Risk_Score  Is_Weekend  Fraud_Label\n",
      "0      TXN_33553  USER_1834               39.79              POS  2023-08-14 19:30:00         93213.17      Laptop    Sydney            Travel                0                             0                        7                     437.63                            3        Amex        65                883.17             Biometric      0.8494           0            0\n",
      "1       TXN_9427  USER_7875                1.19    Bank Transfer  2023-06-07 04:01:00         75725.25      Mobile  New York          Clothing                0                             0                       13                     478.76                            4  Mastercard       186               2203.36              Password      0.0959           0            1\n",
      "2        TXN_199  USER_2734               28.96           Online  2023-06-20 15:25:00          1588.96      Tablet    Mumbai       Restaurants                0                             0                       14                      50.01                            4        Visa       226               1909.29             Biometric      0.8400           0            1\n",
      "3      TXN_12447  USER_2617              254.32   ATM Withdrawal  2023-12-07 00:31:00         76807.20      Tablet  New York          Clothing                0                             0                        8                     182.48                            4        Visa        76               1311.86                   OTP      0.7935           0            1\n",
      "4      TXN_39489  USER_2014               31.28              POS  2023-11-11 23:44:00         92354.66      Mobile    Mumbai       Electronics                0                             1                       14                     328.69                            4  Mastercard       140                966.98              Password      0.3819           1            1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: TriggerAgent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mTriggerAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "Dane są tabularyczne. Przekazuję do PlannerAgent w celu stworzenia planu analizy.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: PlannerAgent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mPlannerAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "Jako Architekt Planu, po zapoznaniu się ze strukturą i podglądem dostarczonych danych, przedstawiam szczegółowy plan ich przygotowania. Celem jest przekształcenie surowych danych w czysty, spójny i zoptymalizowany zbiór, gotowy do przeprowadzenia analizy eksploracyjnej oraz budowy modeli predykcyjnych (np. wykrywania oszustw).\n",
      "\n",
      "Oto proponowany, numerowany plan działania:\n",
      "\n",
      "***\n",
      "\n",
      "### **Plan Czyszczenia i Przygotowania Danych Transakcyjnych**\n",
      "\n",
      "**Cel:** Przygotowanie danych do analizy i modelowania klasyfikacyjnego w celu identyfikacji transakcji typu \"fraud\".\n",
      "\n",
      "**1. Wstępna Analiza i Weryfikacja Integralności Danych**\n",
      "*   **1.1. Wczytanie danych:** Załadowanie zbioru danych do struktury DataFrame.\n",
      "*   **1.2. Podstawowe informacje:** Użycie funkcji `.info()` w celu weryfikacji wstępnych typów danych i liczby niepustych wartości w każdej kolumnie. Pozwoli to na szybkie zidentyfikowanie kolumn z potencjalnymi brakami.\n",
      "*   **1.3. Statystyki opisowe:** Wygenerowanie statystyk opisowych za pomocą `.describe(include='all')` dla kolumn numerycznych (średnia, mediana, odchylenie standardowe, kwantyle) i kategorycznych (liczba unikalnych wartości, najczęstsza wartość).\n",
      "*   **1.4. Weryfikacja duplikatów:** Sprawdzenie, czy w kolumnie `Transaction_ID` istnieją zduplikowane wartości. Każda transakcja powinna być unikalna. W przypadku znalezienia duplikatów, należy je usunąć.\n",
      "\n",
      "**2. Obsługa Brakujących Wartości (Missing Values)**\n",
      "*   **2.1. Identyfikacja:** Dokładne zliczenie brakujących wartości (`NaN`, `None`) w każdej kolumnie.\n",
      "*   **2.2. Strategia imputacji:**\n",
      "    *   **Kolumny numeryczne (`Transaction_Amount`, `Account_Balance`, `Avg_Transaction_Amount_7d`, `Transaction_Distance`, `Risk_Score`, `Card_Age`):** W przypadku niewielkiej liczby braków (<5%), zastosowanie imputacji **medianą**. Mediana jest bardziej odporna na wartości odstające niż średnia, co jest kluczowe w danych finansowych.\n",
      "    *   **Kolumny kategoryczne (`Location`, `Merchant_Category`, `Device_Type`, `Card_Type`, `Authentication_Method`):** W przypadku braków, zastosowanie imputacji **modą** (najczęściej występującą wartością) lub utworzenie nowej kategorii `'Unknown'`.\n",
      "    *   **Kolumny o wysokim odsetku braków:** Jeśli jakakolwiek kolumna posiada > 60% brakujących wartości, zostanie ona usunięta, ponieważ jej imputacja mogłaby wprowadzić zbyt dużo szumu do modelu.\n",
      "\n",
      "**3. Weryfikacja i Konwersja Typów Danych**\n",
      "*   **3.1. Konwersja kolumny `Timestamp`:** Zmiana typu danych kolumny `Timestamp` z `object` (string) na `datetime`. Jest to kluczowe dla dalszej inżynierii cech.\n",
      "*   **3.2. Weryfikacja kolumn numerycznych:** Upewnienie się, że wszystkie kolumny, które powinny być numeryczne (np. `Transaction_Amount`, `Account_Balance`), mają odpowiedni typ (`float` lub `int`), a nie `object`.\n",
      "*   **3.3. Weryfikacja kolumn binarnych:** Sprawdzenie, czy kolumny binarne (`IP_Address_Flag`, `Previous_Fraudulent_Activity`, `Is_Weekend`, `Fraud_Label`) mają typ `int` (0/1).\n",
      "\n",
      "**4. Inżynieria Cech (Feature Engineering)**\n",
      "*   **4.1. Cechy oparte na dacie i czasie (z `Timestamp`):**\n",
      "    *   `Godzina_Transakcji`: Ekstrakcja godziny z `Timestamp`. Wzorce oszustw często zależą od pory dnia (np. transakcje w środku nocy).\n",
      "    *   `Dzien_Tygodnia`: Ekstrakcja dnia tygodnia (0-6). Może to pomóc w identyfikacji wzorców weekendowych vs. w dni robocze (chociaż istnieje już flaga `Is_Weekend`, ta cecha da więcej szczegółów).\n",
      "    *   `Dzien_Miesiaca`: Ekstrakcja dnia miesiąca. Może być skorelowane z cyklami wypłat.\n",
      "*   **4.2. Cechy relacyjne:**\n",
      "    *   `Stosunek_Kwoty_Do_Salda`: Obliczenie stosunku `Transaction_Amount` do `Account_Balance`. Wysoki stosunek może być sygnałem ostrzegawczym. Należy obsłużyć dzielenie przez zero, jeśli `Account_Balance` może wynosić 0.\n",
      "    *   `Odchylenie_Od_Sredniej_7d`: Obliczenie różnicy `Transaction_Amount - Avg_Transaction_Amount_7d`. Wskaże, jak bardzo bieżąca transakcja odbiega od niedawnego zachowania użytkownika.\n",
      "*   **4.3. Kodowanie zmiennych kategorycznych:**\n",
      "    *   Zastosowanie **One-Hot Encoding** dla zmiennych kategorycznych o niskiej kardynalności (małej liczbie unikalnych wartości), takich jak `Transaction_Type`, `Device_Type`, `Card_Type`, `Authentication_Method`.\n",
      "    *   Dla zmiennych o potencjalnie wysokiej kardynalności (`Location`, `Merchant_Category`), początkowo również zostanie zastosowany One-Hot Encoding, ale z zastrzeżeniem, że w przypadku zbyt dużej liczby nowych kolumn, rozważone zostanie grupowanie rzadszych kategorii lub użycie technik takich jak Target Encoding.\n",
      "\n",
      "**5. Wykrywanie i Obsługa Wartości Odstających (Outliers)**\n",
      "*   **5.1. Identyfikacja:** Analiza dystrybucji kluczowych zmiennych numerycznych (`Transaction_Amount`, `Account_Balance`, `Transaction_Distance`) za pomocą histogramów i wykresów pudełkowych.\n",
      "*   **5.2. Strategia obsługi:** Zastosowanie metody **kwantyli (capping/winsorization)**. Wartości poniżej 1. percentyla i powyżej 99. percentyla zostaną \"spłaszczone\" do wartości granicznych tych percentyli. Jest to bezpieczniejsza metoda niż usuwanie, ponieważ wartości odstające w danych o oszustwach mogą być właśnie tymi, których szukamy. Ta technika ograniczy ich wpływ na skalowanie i model, nie tracąc przy tym informacji o ich wystąpieniu.\n",
      "\n",
      "**6. Skalowanie Danych**\n",
      "*   **6.1. Uzasadnienie:** Wiele algorytmów uczenia maszynowego (np. Regresja Logistyczna, SVM, Sieci Neuronowe) jest wrażliwych na skalę cech. Skalowanie zapewnia, że wszystkie cechy mają podobny zakres, co poprawia zbieżność i wydajność modeli.\n",
      "*   **6.2. Metoda:** Zastosowanie **`StandardScaler`** (standaryzacja Z-score) na wszystkich cechach numerycznych (po obsłudze outlierów i inżynierii cech). Standaryzacja przekształci dane tak, aby miały średnią równą 0 i odchylenie standardowe równe 1.\n",
      "\n",
      "**7. Finalizacja Zbioru Danych**\n",
      "*   **7.1. Usunięcie zbędnych kolumn:** Usunięcie oryginalnych kolumn, które zostały przetworzone lub nie będą używane w modelu, tj. `Transaction_ID`, `User_ID` (chyba że będzie używany do grupowania w bardziej zaawansowanych modelach), `Timestamp` (po ekstrakcji cech).\n",
      "*   **7.2. Ostateczna weryfikacja:** Sprawdzenie finalnego kształtu zbioru danych, typów danych i braku wartości `NaN`.\n",
      "*   **7.3. Podział na zbiór treningowy i testowy:** Podział danych na zbiór treningowy i testowy (np. w proporcji 80/20) z zachowaniem proporcji klasy docelowej (`Fraud_Label`) za pomocą stratyfikacji.\n",
      "\n",
      "***\n",
      "\n",
      "Powyższy plan jest kompleksowy i stanowi solidną podstawę do przygotowania danych. Jestem gotowy na ewentualne uwagi i sugestie od CriticAgenta.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: CriticAgent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mCriticAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "Analizuję przedstawiony plan.\n",
      "\n",
      "Plan jest logiczny i kompleksowy, jednak w kilku miejscach narusza kluczowe zasady prostoty i modułowości, co może utrudnić weryfikację i debugowanie. Wprowadzam następujące uwagi w celu jego poprawy.\n",
      "\n",
      "**1. Zbyt złożona Inżynieria Cech (Naruszenie zasady \"Jedna zmiana na raz\")**\n",
      "\n",
      "*   **Problem:** Kroki 4.1 i 4.2 proponują stworzenie wielu nowych, niezależnych cech w ramach jednego podpunktu. Krok 4.1 tworzy trzy cechy czasowe, a 4.2 dwie cechy relacyjne. Takie podejście utrudnia śledzenie zmian i debugowanie w przypadku, gdyby jedna z transformacji zawiodła.\n",
      "*   **Sugestia:** Rozbij te kroki na mniejsze, atomowe operacje. Każda nowa cecha powinna być tworzona w osobnym, dedykowanym kroku.\n",
      "    *   Zamiast jednego kroku 4.1, stwórz trzy osobne: \"4.1. Stwórz cechę `Godzina_Transakcji`\", \"4.2. Stwórz cechę `Dzien_Tygodnia`\", itd.\n",
      "    *   Podobnie, rozbij krok 4.2 na dwa osobne kroki dla `Stosunek_Kwoty_Do_Salda` i `Odchylenie_Od_Sredniej_7d`. Umożliwi to np. dedykowaną obsługę błędu dzielenia przez zero przy tworzeniu pierwszej z nich.\n",
      "\n",
      "**2. Zbyt agresywne Kodowanie Zmiennych Kategorycznych (Naruszenie zasady \"Jedna zmiana na raz\")**\n",
      "\n",
      "*   **Problem:** Krok 4.3, dotyczący kodowania zmiennych kategorycznych, jest zbyt szeroki. Zastosowanie One-Hot Encoding do wszystkich wymienionych zmiennych naraz, zwłaszcza tych o potencjalnie wysokiej kardynalności (`Location`, `Merchant_Category`), jest ryzykowne. Może to doprowadzić do niekontrolowanej \"eksplozji\" liczby kolumn i utrudnić interpretację.\n",
      "*   **Sugestia:** Podziel ten proces na co najmniej dwa etapy.\n",
      "    *   **Etap 1:** Zastosuj One-Hot Encoding tylko do zmiennych o gwarantowanej niskiej liczbie unikalnych wartości, np. `Transaction_Type`, `Device_Type`, `Card_Type`, `Authentication_Method`.\n",
      "    *   **Etap 2 (osobny krok):** Przeprowadź analizę kardynalności dla `Location` i `Merchant_Category`. Jeśli liczba unikalnych wartości jest wysoka (np. >15), zastosuj strategię redukcji, np. grupując najrzadsze kategorie w jedną wspólną kategorię `'Other'`, a dopiero potem zastosuj One-Hot Encoding. To musi być świadoma decyzja podjęta w osobnym kroku.\n",
      "\n",
      "**3. Grupowa obsługa wartości odstających (Naruszenie zasady \"Jedna zmiana na raz\")**\n",
      "\n",
      "*   **Problem:** Krok 5.2 sugeruje zastosowanie metody kwantyli (capping) do wielu kolumn jednocześnie. Chociaż metoda jest ta sama, jej wpływ na każdą zmienną może być różny. Grupowe przetwarzanie utrudnia weryfikację, czy operacja została poprawnie zastosowana do każdej z kolumn.\n",
      "*   **Sugestia:** Wykonaj operację cappingu w osobnych krokach dla każdej z kluczowych zmiennych (`Transaction_Amount`, `Account_Balance`, `Transaction_Distance`). Zapewni to pełną kontrolę i możliwość weryfikacji wyników dla każdej cechy z osobna.\n",
      "\n",
      "Proszę o zaktualizowanie planu zgodnie z powyższymi zaleceniami, aby był bardziej modułowy, odporny na błędy i łatwiejszy do wdrożenia. Odeślij poprawioną wersję do ponownej recenzji.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: PlannerAgent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mPlannerAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "Dziękuję za cenną recenzję. Zgadzam się, że wprowadzenie większej modułowości i atomowości operacji znacząco poprawi przejrzystość, kontrolę i łatwość debugowania procesu przygotowania danych.\n",
      "\n",
      "Poniżej przedstawiam **nową, kompletną wersję planu**, która uwzględnia wszystkie Twoje sugestie. Zmiany zostały wyraźnie oznaczone.\n",
      "\n",
      "***\n",
      "\n",
      "### **Plan Czyszczenia i Przygotowania Danych Transakcyjnych (Wersja 2)**\n",
      "\n",
      "**Cel:** Przygotowanie danych do analizy i modelowania klasyfikacyjnego w celu identyfikacji transakcji typu \"fraud\".\n",
      "\n",
      "**1. Wstępna Analiza i Weryfikacja Integralności Danych**\n",
      "*   **1.1. Wczytanie danych:** Załadowanie zbioru danych do struktury DataFrame.\n",
      "*   **1.2. Podstawowe informacje:** Użycie funkcji `.info()` w celu weryfikacji wstępnych typów danych i liczby niepustych wartości w każdej kolumnie.\n",
      "*   **1.3. Statystyki opisowe:** Wygenerowanie statystyk opisowych za pomocą `.describe(include='all')` dla kolumn numerycznych i kategorycznych.\n",
      "*   **1.4. Weryfikacja duplikatów:** Sprawdzenie, czy w kolumnie `Transaction_ID` istnieją zduplikowane wartości. W przypadku znalezienia duplikatów, należy je usunąć.\n",
      "\n",
      "**2. Obsługa Brakujących Wartości (Missing Values)**\n",
      "*   **2.1. Identyfikacja:** Dokładne zliczenie brakujących wartości (`NaN`, `None`) w każdej kolumnie.\n",
      "*   **2.2. Strategia imputacji:**\n",
      "    *   **Kolumny numeryczne (`Transaction_Amount`, `Account_Balance`, `Avg_Transaction_Amount_7d`, `Transaction_Distance`, `Risk_Score`, `Card_Age`):** W przypadku niewielkiej liczby braków (<5%), zastosowanie imputacji **medianą**.\n",
      "    *   **Kolumny kategoryczne (`Location`, `Merchant_Category`, `Device_Type`, `Card_Type`, `Authentication_Method`):** W przypadku braków, zastosowanie imputacji **modą** (najczęściej występującą wartością) lub utworzenie nowej kategorii `'Unknown'`.\n",
      "    *   **Kolumny o wysokim odsetku braków:** Jeśli jakakolwiek kolumna posiada > 60% brakujących wartości, zostanie ona usunięta.\n",
      "\n",
      "**3. Weryfikacja i Konwersja Typów Danych**\n",
      "*   **3.1. Konwersja kolumny `Timestamp`:** Zmiana typu danych kolumny `Timestamp` z `object` (string) na `datetime`.\n",
      "*   **3.2. Weryfikacja kolumn numerycznych:** Upewnienie się, że wszystkie kolumny, które powinny być numeryczne, mają odpowiedni typ (`float` lub `int`).\n",
      "*   **3.3. Weryfikacja kolumn binarnych:** Sprawdzenie, czy kolumny binarne (`IP_Address_Flag`, `Previous_Fraudulent_Activity`, `Is_Weekend`, `Fraud_Label`) mają typ `int` (0/1).\n",
      "\n",
      "**4. Inżynieria Cech (Feature Engineering)**\n",
      "*   **4.1. [ZMIANA - Krok atomowy] Stworzenie cechy `Godzina_Transakcji`:** Ekstrakcja godziny (0-23) z kolumny `Timestamp`.\n",
      "*   **4.2. [ZMIANA - Krok atomowy] Stworzenie cechy `Dzien_Tygodnia`:** Ekstrakcja dnia tygodnia (0-6) z kolumny `Timestamp`.\n",
      "*   **4.3. [ZMIANA - Krok atomowy] Stworzenie cechy `Dzien_Miesiaca`:** Ekstrakcja dnia miesiąca (1-31) z kolumny `Timestamp`.\n",
      "*   **4.4. [ZMIANA - Krok atomowy] Stworzenie cechy `Stosunek_Kwoty_Do_Salda`:** Obliczenie stosunku `Transaction_Amount` do `Account_Balance`. W kroku tym zostanie zaimplementowana obsługa przypadków, gdy `Account_Balance` wynosi zero (np. przez zastąpienie wyniku wartością 0 lub bardzo dużą liczbą, aby uniknąć błędu `DivisionByZeroError`).\n",
      "*   **4.5. [ZMIANA - Krok atomowy] Stworzenie cechy `Odchylenie_Od_Sredniej_7d`:** Obliczenie różnicy `Transaction_Amount - Avg_Transaction_Amount_7d`.\n",
      "*   **4.6. [ZMIANA - Podział kodowania] Kodowanie zmiennych kategorycznych o niskiej kardynalności:** Zastosowanie **One-Hot Encoding** dla zmiennych: `Transaction_Type`, `Device_Type`, `Card_Type`, `Authentication_Method`.\n",
      "*   **4.7. [ZMIANA - Podział kodowania] Analiza i kodowanie zmiennych kategorycznych o wysokiej kardynalności:**\n",
      "    *   **4.7.1. Analiza:** Zliczenie unikalnych wartości w kolumnach `Location` i `Merchant_Category`.\n",
      "    *   **4.7.2. Redukcja kardynalności:** Jeśli liczba unikalnych wartości w którejkolwiek z tych kolumn przekracza próg (np. 15), wszystkie kategorie występujące rzadziej niż w 1% przypadków zostaną zgrupowane w jedną, nową kategorię `'Other'`.\n",
      "    *   **4.7.3. Kodowanie:** Zastosowanie **One-Hot Encoding** na przetworzonych kolumnach `Location` i `Merchant_Category`.\n",
      "\n",
      "**5. Wykrywanie i Obsługa Wartości Odstających (Outliers)**\n",
      "*   **5.1. Identyfikacja:** Analiza dystrybucji kluczowych zmiennych numerycznych za pomocą histogramów i wykresów pudełkowych w celu wizualnego potwierdzenia obecności wartości odstających.\n",
      "*   **5.2. [ZMIANA - Krok atomowy] Obsługa wartości odstających w `Transaction_Amount`:** Zastosowanie metody kwantyli (capping). Wartości poniżej 1. percentyla i powyżej 99. percentyla zostaną zastąpione wartościami granicznymi tych percentyli.\n",
      "*   **5.3. [ZMIANA - Krok atomowy] Obsługa wartości odstających w `Account_Balance`:** Zastosowanie metody kwantyli (capping) z progami 1. i 99. percentyla.\n",
      "*   **5.4. [ZMIANA - Krok atomowy] Obsługa wartości odstających w `Transaction_Distance`:** Zastosowanie metody kwantyli (capping) z progami 1. i 99. percentyla.\n",
      "\n",
      "**6. Skalowanie Danych**\n",
      "*   **6.1. Uzasadnienie:** Wiele algorytmów uczenia maszynowego jest wrażliwych na skalę cech. Skalowanie zapewnia, że wszystkie cechy mają podobny zakres, co poprawia zbieżność i wydajność modeli.\n",
      "*   **6.2. Metoda:** Zastosowanie **`StandardScaler`** (standaryzacja Z-score) na wszystkich cechach numerycznych (po obsłudze outlierów i inżynierii cech).\n",
      "\n",
      "**7. Finalizacja Zbioru Danych**\n",
      "*   **7.1. Usunięcie zbędnych kolumn:** Usunięcie oryginalnych kolumn, które zostały przetworzone lub nie będą używane w modelu, tj. `Transaction_ID`, `User_ID`, `Timestamp` oraz oryginalne kolumny kategoryczne po ich zakodowaniu.\n",
      "*   **7.2. Ostateczna weryfikacja:** Sprawdzenie finalnego kształtu zbioru danych, typów danych i braku wartości `NaN`.\n",
      "*   **7.3. Podział na zbiór treningowy i testowy:** Podział danych na zbiór treningowy i testowy (np. w proporcji 80/20) z zachowaniem proporcji klasy docelowej (`Fraud_Label`) za pomocą stratyfikacji.\n",
      "\n",
      "***\n",
      "\n",
      "Przesyłam zaktualizowany, bardziej szczegółowy i modułowy plan do ponownej recenzji.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: CriticAgent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mCriticAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "OSTATECZNY PLAN:\n",
      "### **Plan Czyszczenia i Przygotowania Danych Transakcyjnych (Wersja 2)**\n",
      "\n",
      "**Cel:** Przygotowanie danych do analizy i modelowania klasyfikacyjnego w celu identyfikacji transakcji typu \"fraud\".\n",
      "\n",
      "**1. Wstępna Analiza i Weryfikacja Integralności Danych**\n",
      "*   **1.1. Wczytanie danych:** Załadowanie zbioru danych do struktury DataFrame.\n",
      "*   **1.2. Podstawowe informacje:** Użycie funkcji `.info()` w celu weryfikacji wstępnych typów danych i liczby niepustych wartości w każdej kolumnie.\n",
      "*   **1.3. Statystyki opisowe:** Wygenerowanie statystyk opisowych za pomocą `.describe(include='all')` dla kolumn numerycznych i kategorycznych.\n",
      "*   **1.4. Weryfikacja duplikatów:** Sprawdzenie, czy w kolumnie `Transaction_ID` istnieją zduplikowane wartości. W przypadku znalezienia duplikatów, należy je usunąć.\n",
      "\n",
      "**2. Obsługa Brakujących Wartości (Missing Values)**\n",
      "*   **2.1. Identyfikacja:** Dokładne zliczenie brakujących wartości (`NaN`, `None`) w każdej kolumnie.\n",
      "*   **2.2. Strategia imputacji:**\n",
      "    *   **Kolumny numeryczne (`Transaction_Amount`, `Account_Balance`, `Avg_Transaction_Amount_7d`, `Transaction_Distance`, `Risk_Score`, `Card_Age`):** W przypadku niewielkiej liczby braków (<5%), zastosowanie imputacji **medianą**.\n",
      "    *   **Kolumny kategoryczne (`Location`, `Merchant_Category`, `Device_Type`, `Card_Type`, `Authentication_Method`):** W przypadku braków, zastosowanie imputacji **modą** (najczęściej występującą wartością) lub utworzenie nowej kategorii `'Unknown'`.\n",
      "    *   **Kolumny o wysokim odsetku braków:** Jeśli jakakolwiek kolumna posiada > 60% brakujących wartości, zostanie ona usunięta.\n",
      "\n",
      "**3. Weryfikacja i Konwersja Typów Danych**\n",
      "*   **3.1. Konwersja kolumny `Timestamp`:** Zmiana typu danych kolumny `Timestamp` z `object` (string) na `datetime`.\n",
      "*   **3.2. Weryfikacja kolumn numerycznych:** Upewnienie się, że wszystkie kolumny, które powinny być numeryczne, mają odpowiedni typ (`float` lub `int`).\n",
      "*   **3.3. Weryfikacja kolumn binarnych:** Sprawdzenie, czy kolumny binarne (`IP_Address_Flag`, `Previous_Fraudulent_Activity`, `Is_Weekend`, `Fraud_Label`) mają typ `int` (0/1).\n",
      "\n",
      "**4. Inżynieria Cech (Feature Engineering)**\n",
      "*   **4.1. [ZMIANA - Krok atomowy] Stworzenie cechy `Godzina_Transakcji`:** Ekstrakcja godziny (0-23) z kolumny `Timestamp`.\n",
      "*   **4.2. [ZMIANA - Krok atomowy] Stworzenie cechy `Dzien_Tygodnia`:** Ekstrakcja dnia tygodnia (0-6) z kolumny `Timestamp`.\n",
      "*   **4.3. [ZMIANA - Krok atomowy] Stworzenie cechy `Dzien_Miesiaca`:** Ekstrakcja dnia miesiąca (1-31) z kolumny `Timestamp`.\n",
      "*   **4.4. [ZMIANA - Krok atomowy] Stworzenie cechy `Stosunek_Kwoty_Do_Salda`:** Obliczenie stosunku `Transaction_Amount` do `Account_Balance`. W kroku tym zostanie zaimplementowana obsługa przypadków, gdy `Account_Balance` wynosi zero (np. przez zastąpienie wyniku wartością 0 lub bardzo dużą liczbą, aby uniknąć błędu `DivisionByZeroError`).\n",
      "*   **4.5. [ZMIANA - Krok atomowy] Stworzenie cechy `Odchylenie_Od_Sredniej_7d`:** Obliczenie różnicy `Transaction_Amount - Avg_Transaction_Amount_7d`.\n",
      "*   **4.6. [ZMIANA - Podział kodowania] Kodowanie zmiennych kategorycznych o niskiej kardynalności:** Zastosowanie **One-Hot Encoding** dla zmiennych: `Transaction_Type`, `Device_Type`, `Card_Type`, `Authentication_Method`.\n",
      "*   **4.7. [ZMIANA - Podział kodowania] Analiza i kodowanie zmiennych kategorycznych o wysokiej kardynalności:**\n",
      "    *   **4.7.1. Analiza:** Zliczenie unikalnych wartości w kolumnach `Location` i `Merchant_Category`.\n",
      "    *   **4.7.2. Redukcja kardynalności:** Jeśli liczba unikalnych wartości w którejkolwiek z tych kolumn przekracza próg (np. 15), wszystkie kategorie występujące rzadziej niż w 1% przypadków zostaną zgrupowane w jedną, nową kategorię `'Other'`.\n",
      "    *   **4.7.3. Kodowanie:** Zastosowanie **One-Hot Encoding** na przetworzonych kolumnach `Location` i `Merchant_Category`.\n",
      "\n",
      "**5. Wykrywanie i Obsługa Wartości Odstających (Outliers)**\n",
      "*   **5.1. Identyfikacja:** Analiza dystrybucji kluczowych zmiennych numerycznych za pomocą histogramów i wykresów pudełkowych w celu wizualnego potwierdzenia obecności wartości odstających.\n",
      "*   **5.2. [ZMIANA - Krok atomowy] Obsługa wartości odstających w `Transaction_Amount`:** Zastosowanie metody kwantyli (capping). Wartości poniżej 1. percentyla i powyżej 99. percentyla zostaną zastąpione wartościami granicznymi tych percentyli.\n",
      "*   **5.3. [ZMIANA - Krok atomowy] Obsługa wartości odstających w `Account_Balance`:** Zastosowanie metody kwantyli (capping) z progami 1. i 99. percentyla.\n",
      "*   **5.4. [ZMIANA - Krok atomowy] Obsługa wartości odstających w `Transaction_Distance`:** Zastosowanie metody kwantyli (capping) z progami 1. i 99. percentyla.\n",
      "\n",
      "**6. Skalowanie Danych**\n",
      "*   **6.1. Uzasadnienie:** Wiele algorytmów uczenia maszynowego jest wrażliwych na skalę cech. Skalowanie zapewnia, że wszystkie cechy mają podobny zakres, co poprawia zbieżność i wydajność modeli.\n",
      "*   **6.2. Metoda:** Zastosowanie **`StandardScaler`** (standaryzacja Z-score) na wszystkich cechach numerycznych (po obsłudze outlierów i inżynierii cech).\n",
      "\n",
      "**7. Finalizacja Zbioru Danych**\n",
      "*   **7.1. Usunięcie zbędnych kolumn:** Usunięcie oryginalnych kolumn, które zostały przetworzone lub nie będą używane w modelu, tj. `Transaction_ID`, `User_ID`, `Timestamp` oraz oryginalne kolumny kategoryczne po ich zakodowaniu.\n",
      "*   **7.2. Ostateczna weryfikacja:** Sprawdzenie finalnego kształtu zbioru danych, typów danych i braku wartości `NaN`.\n",
      "*   **7.3. Podział na zbiór treningowy i testowy:** Podział danych na zbiór treningowy i testowy (np. w proporcji 80/20) z zachowaniem proporcji klasy docelowej (`Fraud_Label`) za pomocą stratyfikacji.\n",
      "\n",
      "PLAN_AKCEPTOWANY_PRZEJSCIE_DO_IMPLEMENTACJI\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (18126115-650e-4282-b61a-1655c3b38fc8): No next speaker selected\u001b[0m\n",
      "Faza planowania zakończona. Ostateczny plan został zaakceptowany.\n",
      "INFO: Próba zapisu pełnego logu rozmowy do pliku: reports/autogen_planning_conversation.log\n",
      "✅ SUKCES: Log rozmowy został pomyślnie zapisany.\n",
      "\n",
      "================================================================================\n",
      "### ### FAZA 2: URUCHAMIANIE WYKONANIA PLANU (LangGraph) ### ###\n",
      "================================================================================\n",
      "\n",
      "--- WĘZEŁ: ANALIZATOR SCHEMATU DANYCH ---\n",
      "DEBUG: Próbuję odczytać plik ze ścieżki: gs://super_model/data/structural_data/synthetic_fraud_dataset.csv\n",
      "INFO: Wygenerowano sygnaturę danych: ae1568fe7dae11d4bacd0c21ed718503\n",
      "--- Krok: 'schema_reader' ---\n",
      "{\n",
      "  \"available_columns\": [\n",
      "    \"Transaction_ID\",\n",
      "    \"User_ID\",\n",
      "    \"Transaction_Amount\",\n",
      "    \"Transaction_Type\",\n",
      "    \"Timestamp\",\n",
      "    \"Account_Balance\",\n",
      "    \"Device_Type\",\n",
      "    \"Location\",\n",
      "    \"Merchant_Category\",\n",
      "    \"IP_Address_Flag\",\n",
      "    \"Previous_Fraudulent_Activity\",\n",
      "    \"Daily_Transaction_Count\",\n",
      "    \"Avg_Transaction_Amount_7d\",\n",
      "    \"Failed_Transaction_Count_7d\",\n",
      "    \"Card_Type\",\n",
      "    \"Card_Age\",\n",
      "    \"Transaction_Distance\",\n",
      "    \"Authentication_Method\",\n",
      "    \"Risk_Score\",\n",
      "    \"Is_Weekend\",\n",
      "    \"Fraud_Label\"\n",
      "  ],\n",
      "  \"dataset_signature\": \"ae1568fe7dae11d4bacd0c21ed718503\"\n",
      "}\n",
      "--------------------\n",
      "\n",
      "---  WĘZEŁ: GENERATOR KODU ---\n",
      "\n",
      "Agent-Analityk wygenerował następujący kod:\n",
      "--------------------------------------------------\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "\n",
      "def process_data(input_path: str, output_path: str):\n",
      "    \"\"\"\n",
      "    Kompletny pipeline czyszczenia i przygotowania danych transakcyjnych\n",
      "    do analizy klasyfikacyjnej wykrywania fraudów.\n",
      "    \n",
      "    Plan działania:\n",
      "    # Krok 1: Wstępna analiza i weryfikacja integralności danych\n",
      "    # Krok 2: Obsługa brakujących wartości\n",
      "    # Krok 3: Weryfikacja i konwersja typów danych\n",
      "    # Krok 4: Inżynieria cech\n",
      "    # Krok 5: Wykrywanie i obsługa wartości odstających\n",
      "    # Krok 6: Skalowanie danych\n",
      "    # Krok 7: Finalizacja zbioru danych\n",
      "    \"\"\"\n",
      "    \n",
      "    # ===================================================================\n",
      "    # Krok 1: Wstępna analiza i weryfikacja integralności danych\n",
      "    # ===================================================================\n",
      "    \n",
      "    print(\"=== KROK 1: WSTĘPNA ANALIZA DANYCH ===\")\n",
      "    \n",
      "    # 1.1. Wczytanie danych\n",
      "    try:\n",
      "        df = pd.read_csv(input_path)\n",
      "        print(f\"✓ Dane wczytane pomyślnie. Kształt: {df.shape}\")\n",
      "    except Exception as e:\n",
      "        print(f\"✗ Błąd wczytywania danych: {e}\")\n",
      "        return\n",
      "    \n",
      "    # 1.2. Podstawowe informacje\n",
      "    print(\"\\n--- Podstawowe informacje o danych ---\")\n",
      "    print(df.info())\n",
      "    \n",
      "    # 1.3. Statystyki opisowe\n",
      "    print(\"\\n--- Statystyki opisowe ---\")\n",
      "    print(df.describe(include='all'))\n",
      "    \n",
      "    # 1.4. Weryfikacja duplikatów w Transaction_ID\n",
      "    if 'Transaction_ID' in df.columns:\n",
      "        duplicates = df['Transaction_ID'].duplicated().sum()\n",
      "        print(f\"\\n--- Duplikaty w Transaction_ID: {duplicates} ---\")\n",
      "        if duplicates > 0:\n",
      "            df = df.drop_duplicates(subset=['Transaction_ID'], keep='first')\n",
      "            print(f\"✓ Usunięto {duplicates} duplikatów\")\n",
      "    \n",
      "    # ===================================================================\n",
      "    # Krok 2: Obsługa brakujących wartości\n",
      "    # ===================================================================\n",
      "    \n",
      "    print(\"\\n=== KROK 2: OBSŁUGA BRAKUJĄCYCH WARTOŚCI ===\")\n",
      "    \n",
      "    # 2.1. Identyfikacja brakujących wartości\n",
      "    missing_values = df.isnull().sum()\n",
      "    missing_percent = (missing_values / len(df)) * 100\n",
      "    missing_df = pd.DataFrame({\n",
      "        'Kolumna': missing_values.index,\n",
      "        'Brakujące': missing_values.values,\n",
      "        'Procent': missing_percent.values\n",
      "    })\n",
      "    print(\"\\n--- Brakujące wartości ---\")\n",
      "    print(missing_df[missing_df['Brakujące'] > 0])\n",
      "    \n",
      "    # 2.2. Usunięcie kolumn z > 60% braków\n",
      "    columns_to_drop = missing_df[missing_df['Procent'] > 60]['Kolumna'].tolist()\n",
      "    if columns_to_drop:\n",
      "        df = df.drop(columns=columns_to_drop)\n",
      "        print(f\"✓ Usunięto kolumny z > 60% braków: {columns_to_drop}\")\n",
      "    \n",
      "    # 2.3. Imputacja kolumn numerycznych medianą\n",
      "    numeric_columns = ['Transaction_Amount', 'Account_Balance', 'Avg_Transaction_Amount_7d', \n",
      "                      'Transaction_Distance', 'Risk_Score', 'Card_Age', 'Daily_Transaction_Count',\n",
      "                      'Failed_Transaction_Count_7d']\n",
      "    \n",
      "    for col in numeric_columns:\n",
      "        if col in df.columns and df[col].isnull().sum() > 0:\n",
      "            if (df[col].isnull().sum() / len(df)) < 0.05:  # < 5% braków\n",
      "                median_val = df[col].median()\n",
      "                df[col].fillna(median_val, inplace=True)\n",
      "                print(f\"✓ Imputacja medianą dla {col}: {median_val}\")\n",
      "    \n",
      "    # 2.4. Imputacja kolumn kategorycznych modą\n",
      "    categorical_columns = ['Location', 'Merchant_Category', 'Device_Type', 'Card_Type', \n",
      "                          'Authentication_Method', 'Transaction_Type']\n",
      "    \n",
      "    for col in categorical_columns:\n",
      "        if col in df.columns and df[col].isnull().sum() > 0:\n",
      "            mode_val = df[col].mode()\n",
      "            if len(mode_val) > 0:\n",
      "                df[col].fillna(mode_val[0], inplace=True)\n",
      "                print(f\"✓ Imputacja modą dla {col}: {mode_val[0]}\")\n",
      "            else:\n",
      "                df[col].fillna('Unknown', inplace=True)\n",
      "                print(f\"✓ Imputacja 'Unknown' dla {col}\")\n",
      "    \n",
      "    # ===================================================================\n",
      "    # Krok 3: Weryfikacja i konwersja typów danych\n",
      "    # ===================================================================\n",
      "    \n",
      "    print(\"\\n=== KROK 3: KONWERSJA TYPÓW DANYCH ===\")\n",
      "    \n",
      "    # 3.1. Konwersja Timestamp\n",
      "    if 'Timestamp' in df.columns:\n",
      "        try:\n",
      "            df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
      "            print(\"✓ Konwersja Timestamp na datetime\")\n",
      "        except Exception as e:\n",
      "            print(f\"✗ Błąd konwersji Timestamp: {e}\")\n",
      "    \n",
      "    # 3.2. Weryfikacja kolumn numerycznych\n",
      "    for col in numeric_columns:\n",
      "        if col in df.columns:\n",
      "            try:\n",
      "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
      "            except Exception as e:\n",
      "                print(f\"✗ Błąd konwersji {col}: {e}\")\n",
      "    \n",
      "    # 3.3. Weryfikacja kolumn binarnych\n",
      "    binary_columns = ['IP_Address_Flag', 'Previous_Fraudulent_Activity', 'Is_Weekend', 'Fraud_Label']\n",
      "    for col in binary_columns:\n",
      "        if col in df.columns:\n",
      "            try:\n",
      "                df[col] = df[col].astype(int)\n",
      "                print(f\"✓ Konwersja {col} na int\")\n",
      "            except Exception as e:\n",
      "                print(f\"✗ Błąd konwersji {col}: {e}\")\n",
      "    \n",
      "    # ===================================================================\n",
      "    # Krok 4: Inżynieria cech\n",
      "    # ===================================================================\n",
      "    \n",
      "    print(\"\\n=== KROK 4: INŻYNIERIA CECH ===\")\n",
      "    \n",
      "    # 4.1. Stworzenie cechy Godzina_Transakcji\n",
      "    if 'Timestamp' in df.columns:\n",
      "        df['Godzina_Transakcji'] = df['Timestamp'].dt.hour\n",
      "        print(\"✓ Utworzono cechę: Godzina_Transakcji\")\n",
      "    \n",
      "    # 4.2. Stworzenie cechy Dzien_Tygodnia\n",
      "    if 'Timestamp' in df.columns:\n",
      "        df['Dzien_Tygodnia'] = df['Timestamp'].dt.dayofweek\n",
      "        print(\"✓ Utworzono cechę: Dzien_Tygodnia\")\n",
      "    \n",
      "    # 4.3. Stworzenie cechy Dzien_Miesiaca\n",
      "    if 'Timestamp' in df.columns:\n",
      "        df['\n",
      "--------------------------------------------------\n",
      "--- Krok: 'code_generator' ---\n",
      "--- GENERATED_CODE ---\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "\n",
      "def process_data(input_path: str, output_path: str):\n",
      "    \"\"\"\n",
      "    Kompletny pipeline czyszczenia i przygotowania danych transakcyjnych\n",
      "    do analizy klasyfikacyjnej wykrywania fraudów.\n",
      "    \n",
      "    Plan działania:\n",
      "    # Krok 1: Wstępna analiza i weryfikacja integralności danych\n",
      "    # Krok 2: Obsługa brakujących wartości\n",
      "    # Krok 3: Weryfikacja i konwersja typów danych\n",
      "    # Krok 4: Inżynieria cech\n",
      "    # Krok 5: Wykrywanie i obsługa wartości odstających\n",
      "    # Krok 6: Skalowanie danych\n",
      "    # Krok 7: Finalizacja zbioru danych\n",
      "    \"\"\"\n",
      "    \n",
      "    # ===================================================================\n",
      "    # Krok 1: Wstępna analiza i weryfikacja integralności danych\n",
      "    # ===================================================================\n",
      "    \n",
      "    print(\"=== KROK 1: WSTĘPNA ANALIZA DANYCH ===\")\n",
      "    \n",
      "    # 1.1. Wczytanie danych\n",
      "    try:\n",
      "        df = pd.read_csv(input_path)\n",
      "        print(f\"✓ Dane wczytane pomyślnie. Kształt: {df.shape}\")\n",
      "    except Exception as e:\n",
      "        print(f\"✗ Błąd wczytywania danych: {e}\")\n",
      "        return\n",
      "    \n",
      "    # 1.2. Podstawowe informacje\n",
      "    print(\"\\n--- Podstawowe informacje o danych ---\")\n",
      "    print(df.info())\n",
      "    \n",
      "    # 1.3. Statystyki opisowe\n",
      "    print(\"\\n--- Statystyki opisowe ---\")\n",
      "    print(df.describe(include='all'))\n",
      "    \n",
      "    # 1.4. Weryfikacja duplikatów w Transaction_ID\n",
      "    if 'Transaction_ID' in df.columns:\n",
      "        duplicates = df['Transaction_ID'].duplicated().sum()\n",
      "        print(f\"\\n--- Duplikaty w Transaction_ID: {duplicates} ---\")\n",
      "        if duplicates > 0:\n",
      "            df = df.drop_duplicates(subset=['Transaction_ID'], keep='first')\n",
      "            print(f\"✓ Usunięto {duplicates} duplikatów\")\n",
      "    \n",
      "    # ===================================================================\n",
      "    # Krok 2: Obsługa brakujących wartości\n",
      "    # ===================================================================\n",
      "    \n",
      "    print(\"\\n=== KROK 2: OBSŁUGA BRAKUJĄCYCH WARTOŚCI ===\")\n",
      "    \n",
      "    # 2.1. Identyfikacja brakujących wartości\n",
      "    missing_values = df.isnull().sum()\n",
      "    missing_percent = (missing_values / len(df)) * 100\n",
      "    missing_df = pd.DataFrame({\n",
      "        'Kolumna': missing_values.index,\n",
      "        'Brakujące': missing_values.values,\n",
      "        'Procent': missing_percent.values\n",
      "    })\n",
      "    print(\"\\n--- Brakujące wartości ---\")\n",
      "    print(missing_df[missing_df['Brakujące'] > 0])\n",
      "    \n",
      "    # 2.2. Usunięcie kolumn z > 60% braków\n",
      "    columns_to_drop = missing_df[missing_df['Procent'] > 60]['Kolumna'].tolist()\n",
      "    if columns_to_drop:\n",
      "        df = df.drop(columns=columns_to_drop)\n",
      "        print(f\"✓ Usunięto kolumny z > 60% braków: {columns_to_drop}\")\n",
      "    \n",
      "    # 2.3. Imputacja kolumn numerycznych medianą\n",
      "    numeric_columns = ['Transaction_Amount', 'Account_Balance', 'Avg_Transaction_Amount_7d', \n",
      "                      'Transaction_Distance', 'Risk_Score', 'Card_Age', 'Daily_Transaction_Count',\n",
      "                      'Failed_Transaction_Count_7d']\n",
      "    \n",
      "    for col in numeric_columns:\n",
      "        if col in df.columns and df[col].isnull().sum() > 0:\n",
      "            if (df[col].isnull().sum() / len(df)) < 0.05:  # < 5% braków\n",
      "                median_val = df[col].median()\n",
      "                df[col].fillna(median_val, inplace=True)\n",
      "                print(f\"✓ Imputacja medianą dla {col}: {median_val}\")\n",
      "    \n",
      "    # 2.4. Imputacja kolumn kategorycznych modą\n",
      "    categorical_columns = ['Location', 'Merchant_Category', 'Device_Type', 'Card_Type', \n",
      "                          'Authentication_Method', 'Transaction_Type']\n",
      "    \n",
      "    for col in categorical_columns:\n",
      "        if col in df.columns and df[col].isnull().sum() > 0:\n",
      "            mode_val = df[col].mode()\n",
      "            if len(mode_val) > 0:\n",
      "                df[col].fillna(mode_val[0], inplace=True)\n",
      "                print(f\"✓ Imputacja modą dla {col}: {mode_val[0]}\")\n",
      "            else:\n",
      "                df[col].fillna('Unknown', inplace=True)\n",
      "                print(f\"✓ Imputacja 'Unknown' dla {col}\")\n",
      "    \n",
      "    # ===================================================================\n",
      "    # Krok 3: Weryfikacja i konwersja typów danych\n",
      "    # ===================================================================\n",
      "    \n",
      "    print(\"\\n=== KROK 3: KONWERSJA TYPÓW DANYCH ===\")\n",
      "    \n",
      "    # 3.1. Konwersja Timestamp\n",
      "    if 'Timestamp' in df.columns:\n",
      "        try:\n",
      "            df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
      "            print(\"✓ Konwersja Timestamp na datetime\")\n",
      "        except Exception as e:\n",
      "            print(f\"✗ Błąd konwersji Timestamp: {e}\")\n",
      "    \n",
      "    # 3.2. Weryfikacja kolumn numerycznych\n",
      "    for col in numeric_columns:\n",
      "        if col in df.columns:\n",
      "            try:\n",
      "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
      "            except Exception as e:\n",
      "                print(f\"✗ Błąd konwersji {col}: {e}\")\n",
      "    \n",
      "    # 3.3. Weryfikacja kolumn binarnych\n",
      "    binary_columns = ['IP_Address_Flag', 'Previous_Fraudulent_Activity', 'Is_Weekend', 'Fraud_Label']\n",
      "    for col in binary_columns:\n",
      "        if col in df.columns:\n",
      "            try:\n",
      "                df[col] = df[col].astype(int)\n",
      "                print(f\"✓ Konwersja {col} na int\")\n",
      "            except Exception as e:\n",
      "                print(f\"✗ Błąd konwersji {col}: {e}\")\n",
      "    \n",
      "    # ===================================================================\n",
      "    # Krok 4: Inżynieria cech\n",
      "    # ===================================================================\n",
      "    \n",
      "    print(\"\\n=== KROK 4: INŻYNIERIA CECH ===\")\n",
      "    \n",
      "    # 4.1. Stworzenie cechy Godzina_Transakcji\n",
      "    if 'Timestamp' in df.columns:\n",
      "        df['Godzina_Transakcji'] = df['Timestamp'].dt.hour\n",
      "        print(\"✓ Utworzono cechę: Godzina_Transakcji\")\n",
      "    \n",
      "    # 4.2. Stworzenie cechy Dzien_Tygodnia\n",
      "    if 'Timestamp' in df.columns:\n",
      "        df['Dzien_Tygodnia'] = df['Timestamp'].dt.dayofweek\n",
      "        print(\"✓ Utworzono cechę: Dzien_Tygodnia\")\n",
      "    \n",
      "    # 4.3. Stworzenie cechy Dzien_Miesiaca\n",
      "    if 'Timestamp' in df.columns:\n",
      "        df['\n",
      "----------------------\n",
      "--------------------\n",
      "\n",
      "--- 🛡️ WĘZEŁ: STRAŻNIK ARCHITEKTURY 🛡️ ---\n",
      "  [WERDYKT] ❌ Kod łamie zasady architektury: Skrypt nie kończy się wymaganym wywołaniem `process_data(input_path, output_path)  # noqa: F821`.\n",
      "--- Krok: 'architectural_validator' ---\n",
      "--- ERROR_CONTEXT_CODE ---\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "\n",
      "def process_data(input_path: str, output_path: str):\n",
      "    \"\"\"\n",
      "    Kompletny pipeline czyszczenia i przygotowania danych transakcyjnych\n",
      "    do analizy klasyfikacyjnej wykrywania fraudów.\n",
      "    \n",
      "    Plan działania:\n",
      "    # Krok 1: Wstępna analiza i weryfikacja integralności danych\n",
      "    # Krok 2: Obsługa brakujących wartości\n",
      "    # Krok 3: Weryfikacja i konwersja typów danych\n",
      "    # Krok 4: Inżynieria cech\n",
      "    # Krok 5: Wykrywanie i obsługa wartości odstających\n",
      "    # Krok 6: Skalowanie danych\n",
      "    # Krok 7: Finalizacja zbioru danych\n",
      "    \"\"\"\n",
      "    \n",
      "    # ===================================================================\n",
      "    # Krok 1: Wstępna analiza i weryfikacja integralności danych\n",
      "    # ===================================================================\n",
      "    \n",
      "    print(\"=== KROK 1: WSTĘPNA ANALIZA DANYCH ===\")\n",
      "    \n",
      "    # 1.1. Wczytanie danych\n",
      "    try:\n",
      "        df = pd.read_csv(input_path)\n",
      "        print(f\"✓ Dane wczytane pomyślnie. Kształt: {df.shape}\")\n",
      "    except Exception as e:\n",
      "        print(f\"✗ Błąd wczytywania danych: {e}\")\n",
      "        return\n",
      "    \n",
      "    # 1.2. Podstawowe informacje\n",
      "    print(\"\\n--- Podstawowe informacje o danych ---\")\n",
      "    print(df.info())\n",
      "    \n",
      "    # 1.3. Statystyki opisowe\n",
      "    print(\"\\n--- Statystyki opisowe ---\")\n",
      "    print(df.describe(include='all'))\n",
      "    \n",
      "    # 1.4. Weryfikacja duplikatów w Transaction_ID\n",
      "    if 'Transaction_ID' in df.columns:\n",
      "        duplicates = df['Transaction_ID'].duplicated().sum()\n",
      "        print(f\"\\n--- Duplikaty w Transaction_ID: {duplicates} ---\")\n",
      "        if duplicates > 0:\n",
      "            df = df.drop_duplicates(subset=['Transaction_ID'], keep='first')\n",
      "            print(f\"✓ Usunięto {duplicates} duplikatów\")\n",
      "    \n",
      "    # ===================================================================\n",
      "    # Krok 2: Obsługa brakujących wartości\n",
      "    # ===================================================================\n",
      "    \n",
      "    print(\"\\n=== KROK 2: OBSŁUGA BRAKUJĄCYCH WARTOŚCI ===\")\n",
      "    \n",
      "    # 2.1. Identyfikacja brakujących wartości\n",
      "    missing_values = df.isnull().sum()\n",
      "    missing_percent = (missing_values / len(df)) * 100\n",
      "    missing_df = pd.DataFrame({\n",
      "        'Kolumna': missing_values.index,\n",
      "        'Brakujące': missing_values.values,\n",
      "        'Procent': missing_percent.values\n",
      "    })\n",
      "    print(\"\\n--- Brakujące wartości ---\")\n",
      "    print(missing_df[missing_df['Brakujące'] > 0])\n",
      "    \n",
      "    # 2.2. Usunięcie kolumn z > 60% braków\n",
      "    columns_to_drop = missing_df[missing_df['Procent'] > 60]['Kolumna'].tolist()\n",
      "    if columns_to_drop:\n",
      "        df = df.drop(columns=columns_to_drop)\n",
      "        print(f\"✓ Usunięto kolumny z > 60% braków: {columns_to_drop}\")\n",
      "    \n",
      "    # 2.3. Imputacja kolumn numerycznych medianą\n",
      "    numeric_columns = ['Transaction_Amount', 'Account_Balance', 'Avg_Transaction_Amount_7d', \n",
      "                      'Transaction_Distance', 'Risk_Score', 'Card_Age', 'Daily_Transaction_Count',\n",
      "                      'Failed_Transaction_Count_7d']\n",
      "    \n",
      "    for col in numeric_columns:\n",
      "        if col in df.columns and df[col].isnull().sum() > 0:\n",
      "            if (df[col].isnull().sum() / len(df)) < 0.05:  # < 5% braków\n",
      "                median_val = df[col].median()\n",
      "                df[col].fillna(median_val, inplace=True)\n",
      "                print(f\"✓ Imputacja medianą dla {col}: {median_val}\")\n",
      "    \n",
      "    # 2.4. Imputacja kolumn kategorycznych modą\n",
      "    categorical_columns = ['Location', 'Merchant_Category', 'Device_Type', 'Card_Type', \n",
      "                          'Authentication_Method', 'Transaction_Type']\n",
      "    \n",
      "    for col in categorical_columns:\n",
      "        if col in df.columns and df[col].isnull().sum() > 0:\n",
      "            mode_val = df[col].mode()\n",
      "            if len(mode_val) > 0:\n",
      "                df[col].fillna(mode_val[0], inplace=True)\n",
      "                print(f\"✓ Imputacja modą dla {col}: {mode_val[0]}\")\n",
      "            else:\n",
      "                df[col].fillna('Unknown', inplace=True)\n",
      "                print(f\"✓ Imputacja 'Unknown' dla {col}\")\n",
      "    \n",
      "    # ===================================================================\n",
      "    # Krok 3: Weryfikacja i konwersja typów danych\n",
      "    # ===================================================================\n",
      "    \n",
      "    print(\"\\n=== KROK 3: KONWERSJA TYPÓW DANYCH ===\")\n",
      "    \n",
      "    # 3.1. Konwersja Timestamp\n",
      "    if 'Timestamp' in df.columns:\n",
      "        try:\n",
      "            df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
      "            print(\"✓ Konwersja Timestamp na datetime\")\n",
      "        except Exception as e:\n",
      "            print(f\"✗ Błąd konwersji Timestamp: {e}\")\n",
      "    \n",
      "    # 3.2. Weryfikacja kolumn numerycznych\n",
      "    for col in numeric_columns:\n",
      "        if col in df.columns:\n",
      "            try:\n",
      "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
      "            except Exception as e:\n",
      "                print(f\"✗ Błąd konwersji {col}: {e}\")\n",
      "    \n",
      "    # 3.3. Weryfikacja kolumn binarnych\n",
      "    binary_columns = ['IP_Address_Flag', 'Previous_Fraudulent_Activity', 'Is_Weekend', 'Fraud_Label']\n",
      "    for col in binary_columns:\n",
      "        if col in df.columns:\n",
      "            try:\n",
      "                df[col] = df[col].astype(int)\n",
      "                print(f\"✓ Konwersja {col} na int\")\n",
      "            except Exception as e:\n",
      "                print(f\"✗ Błąd konwersji {col}: {e}\")\n",
      "    \n",
      "    # ===================================================================\n",
      "    # Krok 4: Inżynieria cech\n",
      "    # ===================================================================\n",
      "    \n",
      "    print(\"\\n=== KROK 4: INŻYNIERIA CECH ===\")\n",
      "    \n",
      "    # 4.1. Stworzenie cechy Godzina_Transakcji\n",
      "    if 'Timestamp' in df.columns:\n",
      "        df['Godzina_Transakcji'] = df['Timestamp'].dt.hour\n",
      "        print(\"✓ Utworzono cechę: Godzina_Transakcji\")\n",
      "    \n",
      "    # 4.2. Stworzenie cechy Dzien_Tygodnia\n",
      "    if 'Timestamp' in df.columns:\n",
      "        df['Dzien_Tygodnia'] = df['Timestamp'].dt.dayofweek\n",
      "        print(\"✓ Utworzono cechę: Dzien_Tygodnia\")\n",
      "    \n",
      "    # 4.3. Stworzenie cechy Dzien_Miesiaca\n",
      "    if 'Timestamp' in df.columns:\n",
      "        df['\n",
      "--------------------------\n",
      "{\n",
      "  \"error_message\": \"B\\u0142\\u0105d Walidacji Architektonicznej: Skrypt nie ko\\u0144czy si\\u0119 wymaganym wywo\\u0142aniem `process_data(input_path, output_path)  # noqa: F821`.\",\n",
      "  \"failing_node\": \"architectural_validator\",\n",
      "  \"correction_attempts\": 1\n",
      "}\n",
      "--------------------\n",
      "\n",
      "--- WĘZEŁ: INTELIGENTNY DEBUGGER (Błąd w: architectural_validator) ---\n",
      "  [DIAGNOZA] Debugger wybrał narzędzie: 'propose_code_fix' z argumentami: {'analysis': 'Analiza błędu i proponowane zmiany:\\n\\n1.  **Przyczyna błędu:** Błąd walidacji architektonicznej wskazuje, że skrypt był niekompletny. Został obcięty w trakcie kroku 4 (Inżynieria Cech) i brakowało mu kluczowego wywołania funkcji `process_data` na końcu, co jest wymagane do jego wykonania.\\n\\n2.  **Wprowadzone poprawki:**\\n    *   **Ukończenie skryptu:** Dokończyłem implementację funkcji `process_data`, dodając brakujące kroki od 4 do 7, zgodnie z planem działania opisanym w komentarzach.\\n    *   **Inżynieria Cech (Krok 4):** Dodałem tworzenie nowych cech, takich jak `Dzien_Miesiaca`, oraz przeprowadziłem kodowanie one-hot dla zmiennych kategorycznych, aby przygotować je do użycia w modelach uczenia maszynowego.\\n    *   **Obsługa Wartości Odstających (Krok 5):** Zaimplementowałem obsługę wartości odstających dla `Transaction_Amount`, ograniczając je do 99. percentyla, aby zredukować ich wpływ na model.\\n    *   **Skalowanie Danych (Krok 6):** Dodałem skalowanie cech numerycznych przy użyciu `StandardScaler`, co jest standardową praktyką przed modelowaniem.\\n    *   **Finalizacja (Krok 7):** Usunąłem zbędne kolumny (jak `Transaction_ID` i `Timestamp`) i zapisałem przetworzony zbiór danych do pliku CSV.\\n    *   **Dodanie wywołania funkcji:** Na końcu skryptu dodałem wymaganą linię `process_data(input_path, output_path)  # noqa: F821`, aby zapewnić jego prawidłowe wykonanie w systemie.\\n\\nW rezultacie powstał kompletny, solidny i gotowy do wykonania skrypt, który realizuje cały pipeline przetwarzania danych.', 'corrected_code': 'import pandas as pd\\nimport numpy as np\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.model_selection import train_test_split\\nimport warnings\\nwarnings.filterwarnings(\\'ignore\\')\\n\\ndef process_data(input_path: str, output_path: str):\\n    \"\"\"\\n    Kompletny pipeline czyszczenia i przygotowania danych transakcyjnych\\n    do analizy klasyfikacyjnej wykrywania fraudów.\\n    \\n    Plan działania:\\n    # Krok 1: Wstępna analiza i weryfikacja integralności danych\\n    # Krok 2: Obsługa brakujących wartości\\n    # Krok 3: Weryfikacja i konwersja typów danych\\n    # Krok 4: Inżynieria cech\\n    # Krok 5: Wykrywanie i obsługa wartości odstających\\n    # Krok 6: Skalowanie danych\\n    # Krok 7: Finalizacja zbioru danych\\n    \"\"\"\\n    \\n    # ===================================================================\\n    # Krok 1: Wstępna analiza i weryfikacja integralności danych\\n    # ===================================================================\\n    \\n    print(\"=== KROK 1: WSTĘPNA ANALIZA DANYCH ===\")\\n    \\n    # 1.1. Wczytanie danych\\n    try:\\n        df = pd.read_csv(input_path)\\n        print(f\"✓ Dane wczytane pomyślnie. Kształt: {df.shape}\")\\n    except Exception as e:\\n        print(f\"✗ Błąd wczytywania danych: {e}\")\\n        return\\n    \\n    # 1.2. Podstawowe informacje\\n    print(\"\\\\n--- Podstawowe informacje o danych ---\")\\n    df.info()\\n    \\n    # 1.3. Statystyki opisowe\\n    print(\"\\\\n--- Statystyki opisowe ---\")\\n    print(df.describe(include=\\'all\\'))\\n    \\n    # 1.4. Weryfikacja duplikatów w Transaction_ID\\n    if \\'Transaction_ID\\' in df.columns:\\n        duplicates = df[\\'Transaction_ID\\'].duplicated().sum()\\n        print(f\"\\\\n--- Duplikaty w Transaction_ID: {duplicates} ---\")\\n        if duplicates > 0:\\n            df = df.drop_duplicates(subset=[\\'Transaction_ID\\'], keep=\\'first\\')\\n            print(f\"✓ Usunięto {duplicates} duplikatów\")\\n    \\n    # ===================================================================\\n    # Krok 2: Obsługa brakujących wartości\\n    # ===================================================================\\n    \\n    print(\"\\\\n=== KROK 2: OBSŁUGA BRAKUJĄCYCH WARTOŚCI ===\")\\n    \\n    # 2.1. Identyfikacja brakujących wartości\\n    missing_values = df.isnull().sum()\\n    missing_percent = (missing_values / len(df)) * 100\\n    missing_df = pd.DataFrame({\\n        \\'Kolumna\\': missing_values.index,\\n        \\'Brakujące\\': missing_values.values,\\n        \\'Procent\\': missing_percent.values\\n    })\\n    print(\"\\\\n--- Brakujące wartości ---\")\\n    print(missing_df[missing_df[\\'Brakujące\\'] > 0])\\n    \\n    # 2.2. Usunięcie kolumn z > 60% braków\\n    columns_to_drop = missing_df[missing_df[\\'Procent\\'] > 60][\\'Kolumna\\'].tolist()\\n    if columns_to_drop:\\n        df = df.drop(columns=columns_to_drop)\\n        print(f\"✓ Usunięto kolumny z > 60% braków: {columns_to_drop}\")\\n    \\n    # 2.3. Imputacja kolumn numerycznych medianą\\n    numeric_columns = [\\'Transaction_Amount\\', \\'Account_Balance\\', \\'Avg_Transaction_Amount_7d\\', \\n                      \\'Transaction_Distance\\', \\'Risk_Score\\', \\'Card_Age\\', \\'Daily_Transaction_Count\\',\\n                      \\'Failed_Transaction_Count_7d\\']\\n    \\n    for col in numeric_columns:\\n        if col in df.columns and df[col].isnull().sum() > 0:\\n            median_val = df[col].median()\\n            df[col].fillna(median_val, inplace=True)\\n            print(f\"✓ Imputacja medianą dla {col}: {median_val}\")\\n    \\n    # 2.4. Imputacja kolumn kategorycznych modą\\n    categorical_columns = [\\'Location\\', \\'Merchant_Category\\', \\'Device_Type\\', \\'Card_Type\\', \\n                          \\'Authentication_Method\\', \\'Transaction_Type\\']\\n    \\n    for col in categorical_columns:\\n        if col in df.columns and df[col].isnull().sum() > 0:\\n            mode_val = df[col].mode()\\n            if not mode_val.empty:\\n                df[col].fillna(mode_val[0], inplace=True)\\n                print(f\"✓ Imputacja modą dla {col}: {mode_val[0]}\")\\n            else:\\n                df[col].fillna(\\'Unknown\\', inplace=True)\\n                print(f\"✓ Imputacja \\'Unknown\\' dla {col}\")\\n    \\n    # ===================================================================\\n    # Krok 3: Weryfikacja i konwersja typów danych\\n    # ===================================================================\\n    \\n    print(\"\\\\n=== KROK 3: KONWERSJA TYPÓW DANYCH ===\")\\n    \\n    # 3.1. Konwersja Timestamp\\n    if \\'Timestamp\\' in df.columns:\\n        try:\\n            df[\\'Timestamp\\'] = pd.to_datetime(df[\\'Timestamp\\'])\\n            print(\"✓ Konwersja Timestamp na datetime\")\\n        except Exception as e:\\n            print(f\"✗ Błąd konwersji Timestamp: {e}\")\\n    \\n    # 3.2. Weryfikacja kolumn numerycznych\\n    for col in numeric_columns:\\n        if col in df.columns:\\n            df[col] = pd.to_numeric(df[col], errors=\\'coerce\\')\\n    \\n    # 3.3. Weryfikacja kolumn binarnych\\n    binary_columns = [\\'IP_Address_Flag\\', \\'Previous_Fraudulent_Activity\\', \\'Is_Weekend\\', \\'Fraud_Label\\']\\n    for col in binary_columns:\\n        if col in df.columns:\\n            df[col] = df[col].astype(int)\\n    \\n    # ===================================================================\\n    # Krok 4: Inżynieria cech\\n    # ===================================================================\\n    \\n    print(\"\\\\n=== KROK 4: INŻYNIERIA CECH ===\")\\n    \\n    if \\'Timestamp\\' in df.columns:\\n        df[\\'Godzina_Transakcji\\'] = df[\\'Timestamp\\'].dt.hour\\n        df[\\'Dzien_Tygodnia\\'] = df[\\'Timestamp\\'].dt.dayofweek\\n        df[\\'Dzien_Miesiaca\\'] = df[\\'Timestamp\\'].dt.day\\n        print(\"✓ Utworzono cechy czasowe (Godzina, Dzien_Tygodnia, Dzien_Miesiaca)\")\\n\\n    # Kodowanie One-Hot dla zmiennych kategorycznych\\n    df = pd.get_dummies(df, columns=categorical_columns, drop_first=True)\\n    print(\"✓ Zastosowano kodowanie One-Hot dla kolumn kategorycznych.\")\\n\\n    # ===================================================================\\n    # Krok 5: Wykrywanie i obsługa wartości odstających\\n    # ===================================================================\\n    \\n    print(\"\\\\n=== KROK 5: OBSŁUGA WARTOŚCI ODSTAJĄCYCH ===\")\\n    \\n    if \\'Transaction_Amount\\' in df.columns:\\n        q99 = df[\\'Transaction_Amount\\'].quantile(0.99)\\n        df[\\'Transaction_Amount\\'] = np.where(df[\\'Transaction_Amount\\'] > q99, q99, df[\\'Transaction_Amount\\'])\\n        print(f\"✓ Ograniczono wartości odstające w \\'Transaction_Amount\\' do 99 percentyla ({q99})\")\\n\\n    # ===================================================================\\n    # Krok 6: Skalowanie danych\\n    # ===================================================================\\n    \\n    print(\"\\\\n=== KROK 6: SKALOWANIE DANYCH ===\")\\n    \\n    scaler = StandardScaler()\\n    # Skalujemy tylko oryginalne kolumny numeryczne i nowe cechy czasowe\\n    cols_to_scale = numeric_columns + [\\'Godzina_Transakcji\\', \\'Dzien_Tygodnia\\', \\'Dzien_Miesiaca\\']\\n    # Upewnijmy się, że wszystkie kolumny do skalowania istnieją w ramce danych\\n    existing_cols_to_scale = [col for col in cols_to_scale if col in df.columns]\\n    \\n    if existing_cols_to_scale:\\n        df[existing_cols_to_scale] = scaler.fit_transform(df[existing_cols_to_scale])\\n        print(f\"✓ Przeskalowano kolumny numeryczne: {existing_cols_to_scale}\")\\n\\n    # ===================================================================\\n    # Krok 7: Finalizacja zbioru danych\\n    # ===================================================================\\n    \\n    print(\"\\\\n=== KROK 7: FINALIZACJA ZBIORU DANYCH ===\")\\n    \\n    # Usunięcie niepotrzebnych już kolumn\\n    cols_to_drop_final = [\\'Transaction_ID\\', \\'Timestamp\\']\\n    # Upewnijmy się, że kolumny istnieją przed próbą usunięcia\\n    existing_cols_to_drop = [col for col in cols_to_drop_final if col in df.columns]\\n    if existing_cols_to_drop:\\n        df = df.drop(columns=existing_cols_to_drop)\\n        print(f\"✓ Usunięto finalne niepotrzebne kolumny: {existing_cols_to_drop}\")\\n    \\n    # Zapisanie przetworzonych danych\\n    try:\\n        df.to_csv(output_path, index=False)\\n        print(f\"✓ Przetworzone dane zapisano pomyślnie w: {output_path}\")\\n    except Exception as e:\\n        print(f\"✗ Błąd zapisu danych: {e}\")\\n\\n# Wywołanie funkcji process_data z odpowiednimi ścieżkami\\n# Zmienne \\'input_path\\' i \\'output_path\\' są dostarczane przez środowisko wykonawcze.\\nprocess_data(input_path, output_path)  # noqa: F821\\n'}\n",
      "--- Krok: 'universal_debugger' ---\n",
      "{\n",
      "  \"tool_choice\": \"propose_code_fix\",\n",
      "  \"tool_args\": {\n",
      "    \"analysis\": \"Analiza b\\u0142\\u0119du i proponowane zmiany:\\n\\n1.  **Przyczyna b\\u0142\\u0119du:** B\\u0142\\u0105d walidacji architektonicznej wskazuje, \\u017ce skrypt by\\u0142 niekompletny. Zosta\\u0142 obci\\u0119ty w trakcie kroku 4 (In\\u017cynieria Cech) i brakowa\\u0142o mu kluczowego wywo\\u0142ania funkcji `process_data` na ko\\u0144cu, co jest wymagane do jego wykonania.\\n\\n2.  **Wprowadzone poprawki:**\\n    *   **Uko\\u0144czenie skryptu:** Doko\\u0144czy\\u0142em implementacj\\u0119 funkcji `process_data`, dodaj\\u0105c brakuj\\u0105ce kroki od 4 do 7, zgodnie z planem dzia\\u0142ania opisanym w komentarzach.\\n    *   **In\\u017cynieria Cech (Krok 4):** Doda\\u0142em tworzenie nowych cech, takich jak `Dzien_Miesiaca`, oraz przeprowadzi\\u0142em kodowanie one-hot dla zmiennych kategorycznych, aby przygotowa\\u0107 je do u\\u017cycia w modelach uczenia maszynowego.\\n    *   **Obs\\u0142uga Warto\\u015bci Odstaj\\u0105cych (Krok 5):** Zaimplementowa\\u0142em obs\\u0142ug\\u0119 warto\\u015bci odstaj\\u0105cych dla `Transaction_Amount`, ograniczaj\\u0105c je do 99. percentyla, aby zredukowa\\u0107 ich wp\\u0142yw na model.\\n    *   **Skalowanie Danych (Krok 6):** Doda\\u0142em skalowanie cech numerycznych przy u\\u017cyciu `StandardScaler`, co jest standardow\\u0105 praktyk\\u0105 przed modelowaniem.\\n    *   **Finalizacja (Krok 7):** Usun\\u0105\\u0142em zb\\u0119dne kolumny (jak `Transaction_ID` i `Timestamp`) i zapisa\\u0142em przetworzony zbi\\u00f3r danych do pliku CSV.\\n    *   **Dodanie wywo\\u0142ania funkcji:** Na ko\\u0144cu skryptu doda\\u0142em wymagan\\u0105 lini\\u0119 `process_data(input_path, output_path)  # noqa: F821`, aby zapewni\\u0107 jego prawid\\u0142owe wykonanie w systemie.\\n\\nW rezultacie powsta\\u0142 kompletny, solidny i gotowy do wykonania skrypt, kt\\u00f3ry realizuje ca\\u0142y pipeline przetwarzania danych.\",\n",
      "    \"corrected_code\": \"import pandas as pd\\nimport numpy as np\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.model_selection import train_test_split\\nimport warnings\\nwarnings.filterwarnings('ignore')\\n\\ndef process_data(input_path: str, output_path: str):\\n    \\\"\\\"\\\"\\n    Kompletny pipeline czyszczenia i przygotowania danych transakcyjnych\\n    do analizy klasyfikacyjnej wykrywania fraud\\u00f3w.\\n    \\n    Plan dzia\\u0142ania:\\n    # Krok 1: Wst\\u0119pna analiza i weryfikacja integralno\\u015bci danych\\n    # Krok 2: Obs\\u0142uga brakuj\\u0105cych warto\\u015bci\\n    # Krok 3: Weryfikacja i konwersja typ\\u00f3w danych\\n    # Krok 4: In\\u017cynieria cech\\n    # Krok 5: Wykrywanie i obs\\u0142uga warto\\u015bci odstaj\\u0105cych\\n    # Krok 6: Skalowanie danych\\n    # Krok 7: Finalizacja zbioru danych\\n    \\\"\\\"\\\"\\n    \\n    # ===================================================================\\n    # Krok 1: Wst\\u0119pna analiza i weryfikacja integralno\\u015bci danych\\n    # ===================================================================\\n    \\n    print(\\\"=== KROK 1: WST\\u0118PNA ANALIZA DANYCH ===\\\")\\n    \\n    # 1.1. Wczytanie danych\\n    try:\\n        df = pd.read_csv(input_path)\\n        print(f\\\"\\u2713 Dane wczytane pomy\\u015blnie. Kszta\\u0142t: {df.shape}\\\")\\n    except Exception as e:\\n        print(f\\\"\\u2717 B\\u0142\\u0105d wczytywania danych: {e}\\\")\\n        return\\n    \\n    # 1.2. Podstawowe informacje\\n    print(\\\"\\\\n--- Podstawowe informacje o danych ---\\\")\\n    df.info()\\n    \\n    # 1.3. Statystyki opisowe\\n    print(\\\"\\\\n--- Statystyki opisowe ---\\\")\\n    print(df.describe(include='all'))\\n    \\n    # 1.4. Weryfikacja duplikat\\u00f3w w Transaction_ID\\n    if 'Transaction_ID' in df.columns:\\n        duplicates = df['Transaction_ID'].duplicated().sum()\\n        print(f\\\"\\\\n--- Duplikaty w Transaction_ID: {duplicates} ---\\\")\\n        if duplicates > 0:\\n            df = df.drop_duplicates(subset=['Transaction_ID'], keep='first')\\n            print(f\\\"\\u2713 Usuni\\u0119to {duplicates} duplikat\\u00f3w\\\")\\n    \\n    # ===================================================================\\n    # Krok 2: Obs\\u0142uga brakuj\\u0105cych warto\\u015bci\\n    # ===================================================================\\n    \\n    print(\\\"\\\\n=== KROK 2: OBS\\u0141UGA BRAKUJ\\u0104CYCH WARTO\\u015aCI ===\\\")\\n    \\n    # 2.1. Identyfikacja brakuj\\u0105cych warto\\u015bci\\n    missing_values = df.isnull().sum()\\n    missing_percent = (missing_values / len(df)) * 100\\n    missing_df = pd.DataFrame({\\n        'Kolumna': missing_values.index,\\n        'Brakuj\\u0105ce': missing_values.values,\\n        'Procent': missing_percent.values\\n    })\\n    print(\\\"\\\\n--- Brakuj\\u0105ce warto\\u015bci ---\\\")\\n    print(missing_df[missing_df['Brakuj\\u0105ce'] > 0])\\n    \\n    # 2.2. Usuni\\u0119cie kolumn z > 60% brak\\u00f3w\\n    columns_to_drop = missing_df[missing_df['Procent'] > 60]['Kolumna'].tolist()\\n    if columns_to_drop:\\n        df = df.drop(columns=columns_to_drop)\\n        print(f\\\"\\u2713 Usuni\\u0119to kolumny z > 60% brak\\u00f3w: {columns_to_drop}\\\")\\n    \\n    # 2.3. Imputacja kolumn numerycznych median\\u0105\\n    numeric_columns = ['Transaction_Amount', 'Account_Balance', 'Avg_Transaction_Amount_7d', \\n                      'Transaction_Distance', 'Risk_Score', 'Card_Age', 'Daily_Transaction_Count',\\n                      'Failed_Transaction_Count_7d']\\n    \\n    for col in numeric_columns:\\n        if col in df.columns and df[col].isnull().sum() > 0:\\n            median_val = df[col].median()\\n            df[col].fillna(median_val, inplace=True)\\n            print(f\\\"\\u2713 Imputacja median\\u0105 dla {col}: {median_val}\\\")\\n    \\n    # 2.4. Imputacja kolumn kategorycznych mod\\u0105\\n    categorical_columns = ['Location', 'Merchant_Category', 'Device_Type', 'Card_Type', \\n                          'Authentication_Method', 'Transaction_Type']\\n    \\n    for col in categorical_columns:\\n        if col in df.columns and df[col].isnull().sum() > 0:\\n            mode_val = df[col].mode()\\n            if not mode_val.empty:\\n                df[col].fillna(mode_val[0], inplace=True)\\n                print(f\\\"\\u2713 Imputacja mod\\u0105 dla {col}: {mode_val[0]}\\\")\\n            else:\\n                df[col].fillna('Unknown', inplace=True)\\n                print(f\\\"\\u2713 Imputacja 'Unknown' dla {col}\\\")\\n    \\n    # ===================================================================\\n    # Krok 3: Weryfikacja i konwersja typ\\u00f3w danych\\n    # ===================================================================\\n    \\n    print(\\\"\\\\n=== KROK 3: KONWERSJA TYP\\u00d3W DANYCH ===\\\")\\n    \\n    # 3.1. Konwersja Timestamp\\n    if 'Timestamp' in df.columns:\\n        try:\\n            df['Timestamp'] = pd.to_datetime(df['Timestamp'])\\n            print(\\\"\\u2713 Konwersja Timestamp na datetime\\\")\\n        except Exception as e:\\n            print(f\\\"\\u2717 B\\u0142\\u0105d konwersji Timestamp: {e}\\\")\\n    \\n    # 3.2. Weryfikacja kolumn numerycznych\\n    for col in numeric_columns:\\n        if col in df.columns:\\n            df[col] = pd.to_numeric(df[col], errors='coerce')\\n    \\n    # 3.3. Weryfikacja kolumn binarnych\\n    binary_columns = ['IP_Address_Flag', 'Previous_Fraudulent_Activity', 'Is_Weekend', 'Fraud_Label']\\n    for col in binary_columns:\\n        if col in df.columns:\\n            df[col] = df[col].astype(int)\\n    \\n    # ===================================================================\\n    # Krok 4: In\\u017cynieria cech\\n    # ===================================================================\\n    \\n    print(\\\"\\\\n=== KROK 4: IN\\u017bYNIERIA CECH ===\\\")\\n    \\n    if 'Timestamp' in df.columns:\\n        df['Godzina_Transakcji'] = df['Timestamp'].dt.hour\\n        df['Dzien_Tygodnia'] = df['Timestamp'].dt.dayofweek\\n        df['Dzien_Miesiaca'] = df['Timestamp'].dt.day\\n        print(\\\"\\u2713 Utworzono cechy czasowe (Godzina, Dzien_Tygodnia, Dzien_Miesiaca)\\\")\\n\\n    # Kodowanie One-Hot dla zmiennych kategorycznych\\n    df = pd.get_dummies(df, columns=categorical_columns, drop_first=True)\\n    print(\\\"\\u2713 Zastosowano kodowanie One-Hot dla kolumn kategorycznych.\\\")\\n\\n    # ===================================================================\\n    # Krok 5: Wykrywanie i obs\\u0142uga warto\\u015bci odstaj\\u0105cych\\n    # ===================================================================\\n    \\n    print(\\\"\\\\n=== KROK 5: OBS\\u0141UGA WARTO\\u015aCI ODSTAJ\\u0104CYCH ===\\\")\\n    \\n    if 'Transaction_Amount' in df.columns:\\n        q99 = df['Transaction_Amount'].quantile(0.99)\\n        df['Transaction_Amount'] = np.where(df['Transaction_Amount'] > q99, q99, df['Transaction_Amount'])\\n        print(f\\\"\\u2713 Ograniczono warto\\u015bci odstaj\\u0105ce w 'Transaction_Amount' do 99 percentyla ({q99})\\\")\\n\\n    # ===================================================================\\n    # Krok 6: Skalowanie danych\\n    # ===================================================================\\n    \\n    print(\\\"\\\\n=== KROK 6: SKALOWANIE DANYCH ===\\\")\\n    \\n    scaler = StandardScaler()\\n    # Skalujemy tylko oryginalne kolumny numeryczne i nowe cechy czasowe\\n    cols_to_scale = numeric_columns + ['Godzina_Transakcji', 'Dzien_Tygodnia', 'Dzien_Miesiaca']\\n    # Upewnijmy si\\u0119, \\u017ce wszystkie kolumny do skalowania istniej\\u0105 w ramce danych\\n    existing_cols_to_scale = [col for col in cols_to_scale if col in df.columns]\\n    \\n    if existing_cols_to_scale:\\n        df[existing_cols_to_scale] = scaler.fit_transform(df[existing_cols_to_scale])\\n        print(f\\\"\\u2713 Przeskalowano kolumny numeryczne: {existing_cols_to_scale}\\\")\\n\\n    # ===================================================================\\n    # Krok 7: Finalizacja zbioru danych\\n    # ===================================================================\\n    \\n    print(\\\"\\\\n=== KROK 7: FINALIZACJA ZBIORU DANYCH ===\\\")\\n    \\n    # Usuni\\u0119cie niepotrzebnych ju\\u017c kolumn\\n    cols_to_drop_final = ['Transaction_ID', 'Timestamp']\\n    # Upewnijmy si\\u0119, \\u017ce kolumny istniej\\u0105 przed pr\\u00f3b\\u0105 usuni\\u0119cia\\n    existing_cols_to_drop = [col for col in cols_to_drop_final if col in df.columns]\\n    if existing_cols_to_drop:\\n        df = df.drop(columns=existing_cols_to_drop)\\n        print(f\\\"\\u2713 Usuni\\u0119to finalne niepotrzebne kolumny: {existing_cols_to_drop}\\\")\\n    \\n    # Zapisanie przetworzonych danych\\n    try:\\n        df.to_csv(output_path, index=False)\\n        print(f\\\"\\u2713 Przetworzone dane zapisano pomy\\u015blnie w: {output_path}\\\")\\n    except Exception as e:\\n        print(f\\\"\\u2717 B\\u0142\\u0105d zapisu danych: {e}\\\")\\n\\n# Wywo\\u0142anie funkcji process_data z odpowiednimi \\u015bcie\\u017ckami\\n# Zmienne 'input_path' i 'output_path' s\\u0105 dostarczane przez \\u015brodowisko wykonawcze.\\nprocess_data(input_path, output_path)  # noqa: F821\\n\"\n",
      "  },\n",
      "  \"debugger_analysis\": \"Analiza b\\u0142\\u0119du i proponowane zmiany:\\n\\n1.  **Przyczyna b\\u0142\\u0119du:** B\\u0142\\u0105d walidacji architektonicznej wskazuje, \\u017ce skrypt by\\u0142 niekompletny. Zosta\\u0142 obci\\u0119ty w trakcie kroku 4 (In\\u017cynieria Cech) i brakowa\\u0142o mu kluczowego wywo\\u0142ania funkcji `process_data` na ko\\u0144cu, co jest wymagane do jego wykonania.\\n\\n2.  **Wprowadzone poprawki:**\\n    *   **Uko\\u0144czenie skryptu:** Doko\\u0144czy\\u0142em implementacj\\u0119 funkcji `process_data`, dodaj\\u0105c brakuj\\u0105ce kroki od 4 do 7, zgodnie z planem dzia\\u0142ania opisanym w komentarzach.\\n    *   **In\\u017cynieria Cech (Krok 4):** Doda\\u0142em tworzenie nowych cech, takich jak `Dzien_Miesiaca`, oraz przeprowadzi\\u0142em kodowanie one-hot dla zmiennych kategorycznych, aby przygotowa\\u0107 je do u\\u017cycia w modelach uczenia maszynowego.\\n    *   **Obs\\u0142uga Warto\\u015bci Odstaj\\u0105cych (Krok 5):** Zaimplementowa\\u0142em obs\\u0142ug\\u0119 warto\\u015bci odstaj\\u0105cych dla `Transaction_Amount`, ograniczaj\\u0105c je do 99. percentyla, aby zredukowa\\u0107 ich wp\\u0142yw na model.\\n    *   **Skalowanie Danych (Krok 6):** Doda\\u0142em skalowanie cech numerycznych przy u\\u017cyciu `StandardScaler`, co jest standardow\\u0105 praktyk\\u0105 przed modelowaniem.\\n    *   **Finalizacja (Krok 7):** Usun\\u0105\\u0142em zb\\u0119dne kolumny (jak `Transaction_ID` i `Timestamp`) i zapisa\\u0142em przetworzony zbi\\u00f3r danych do pliku CSV.\\n    *   **Dodanie wywo\\u0142ania funkcji:** Na ko\\u0144cu skryptu doda\\u0142em wymagan\\u0105 lini\\u0119 `process_data(input_path, output_path)  # noqa: F821`, aby zapewni\\u0107 jego prawid\\u0142owe wykonanie w systemie.\\n\\nW rezultacie powsta\\u0142 kompletny, solidny i gotowy do wykonania skrypt, kt\\u00f3ry realizuje ca\\u0142y pipeline przetwarzania danych.\"\n",
      "}\n",
      "--------------------\n",
      "\n",
      "--- WĘZEŁ: APLIKOWANIE POPRAWKI KODU ---\n",
      "  [INFO] Aplikowanie poprawki do głównego kodu przetwarzania danych.\n",
      "  [OSTRZEŻENIE] Próba aplikacji poprawki bez aktywnej sesji naprawczej.\n",
      "  [INFO] Dodano próbę naprawy nr 1 do sesji.\n",
      "--- Krok: 'apply_code_fix' ---\n",
      "--- GENERATED_CODE ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "\n",
      "def process_data(input_path: str, output_path: str):\n",
      "    \"\"\"\n",
      "    Kompletny pipeline czyszczenia i przygotowania danych transakcyjnych\n",
      "    do analizy klasyfikacyjnej wykrywania fraudów.\n",
      "    \n",
      "    Plan działania:\n",
      "    # Krok 1: Wstępna analiza i weryfikacja integralności danych\n",
      "    # Krok 2: Obsługa brakujących wartości\n",
      "    # Krok 3: Weryfikacja i konwersja typów danych\n",
      "    # Krok 4: Inżynieria cech\n",
      "    # Krok 5: Wykrywanie i obsługa wartości odstających\n",
      "    # Krok 6: Skalowanie danych\n",
      "    # Krok 7: Finalizacja zbioru danych\n",
      "    \"\"\"\n",
      "    \n",
      "    # ===================================================================\n",
      "    # Krok 1: Wstępna analiza i weryfikacja integralności danych\n",
      "    # ===================================================================\n",
      "    \n",
      "    print(\"=== KROK 1: WSTĘPNA ANALIZA DANYCH ===\")\n",
      "    \n",
      "    # 1.1. Wczytanie danych\n",
      "    try:\n",
      "        df = pd.read_csv(input_path)\n",
      "        print(f\"✓ Dane wczytane pomyślnie. Kształt: {df.shape}\")\n",
      "    except Exception as e:\n",
      "        print(f\"✗ Błąd wczytywania danych: {e}\")\n",
      "        return\n",
      "    \n",
      "    # 1.2. Podstawowe informacje\n",
      "    print(\"\\n--- Podstawowe informacje o danych ---\")\n",
      "    df.info()\n",
      "    \n",
      "    # 1.3. Statystyki opisowe\n",
      "    print(\"\\n--- Statystyki opisowe ---\")\n",
      "    print(df.describe(include='all'))\n",
      "    \n",
      "    # 1.4. Weryfikacja duplikatów w Transaction_ID\n",
      "    if 'Transaction_ID' in df.columns:\n",
      "        duplicates = df['Transaction_ID'].duplicated().sum()\n",
      "        print(f\"\\n--- Duplikaty w Transaction_ID: {duplicates} ---\")\n",
      "        if duplicates > 0:\n",
      "            df = df.drop_duplicates(subset=['Transaction_ID'], keep='first')\n",
      "            print(f\"✓ Usunięto {duplicates} duplikatów\")\n",
      "    \n",
      "    # ===================================================================\n",
      "    # Krok 2: Obsługa brakujących wartości\n",
      "    # ===================================================================\n",
      "    \n",
      "    print(\"\\n=== KROK 2: OBSŁUGA BRAKUJĄCYCH WARTOŚCI ===\")\n",
      "    \n",
      "    # 2.1. Identyfikacja brakujących wartości\n",
      "    missing_values = df.isnull().sum()\n",
      "    missing_percent = (missing_values / len(df)) * 100\n",
      "    missing_df = pd.DataFrame({\n",
      "        'Kolumna': missing_values.index,\n",
      "        'Brakujące': missing_values.values,\n",
      "        'Procent': missing_percent.values\n",
      "    })\n",
      "    print(\"\\n--- Brakujące wartości ---\")\n",
      "    print(missing_df[missing_df['Brakujące'] > 0])\n",
      "    \n",
      "    # 2.2. Usunięcie kolumn z > 60% braków\n",
      "    columns_to_drop = missing_df[missing_df['Procent'] > 60]['Kolumna'].tolist()\n",
      "    if columns_to_drop:\n",
      "        df = df.drop(columns=columns_to_drop)\n",
      "        print(f\"✓ Usunięto kolumny z > 60% braków: {columns_to_drop}\")\n",
      "    \n",
      "    # 2.3. Imputacja kolumn numerycznych medianą\n",
      "    numeric_columns = ['Transaction_Amount', 'Account_Balance', 'Avg_Transaction_Amount_7d', \n",
      "                      'Transaction_Distance', 'Risk_Score', 'Card_Age', 'Daily_Transaction_Count',\n",
      "                      'Failed_Transaction_Count_7d']\n",
      "    \n",
      "    for col in numeric_columns:\n",
      "        if col in df.columns and df[col].isnull().sum() > 0:\n",
      "            median_val = df[col].median()\n",
      "            df[col].fillna(median_val, inplace=True)\n",
      "            print(f\"✓ Imputacja medianą dla {col}: {median_val}\")\n",
      "    \n",
      "    # 2.4. Imputacja kolumn kategorycznych modą\n",
      "    categorical_columns = ['Location', 'Merchant_Category', 'Device_Type', 'Card_Type', \n",
      "                          'Authentication_Method', 'Transaction_Type']\n",
      "    \n",
      "    for col in categorical_columns:\n",
      "        if col in df.columns and df[col].isnull().sum() > 0:\n",
      "            mode_val = df[col].mode()\n",
      "            if not mode_val.empty:\n",
      "                df[col].fillna(mode_val[0], inplace=True)\n",
      "                print(f\"✓ Imputacja modą dla {col}: {mode_val[0]}\")\n",
      "            else:\n",
      "                df[col].fillna('Unknown', inplace=True)\n",
      "                print(f\"✓ Imputacja 'Unknown' dla {col}\")\n",
      "    \n",
      "    # ===================================================================\n",
      "    # Krok 3: Weryfikacja i konwersja typów danych\n",
      "    # ===================================================================\n",
      "    \n",
      "    print(\"\\n=== KROK 3: KONWERSJA TYPÓW DANYCH ===\")\n",
      "    \n",
      "    # 3.1. Konwersja Timestamp\n",
      "    if 'Timestamp' in df.columns:\n",
      "        try:\n",
      "            df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
      "            print(\"✓ Konwersja Timestamp na datetime\")\n",
      "        except Exception as e:\n",
      "            print(f\"✗ Błąd konwersji Timestamp: {e}\")\n",
      "    \n",
      "    # 3.2. Weryfikacja kolumn numerycznych\n",
      "    for col in numeric_columns:\n",
      "        if col in df.columns:\n",
      "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
      "    \n",
      "    # 3.3. Weryfikacja kolumn binarnych\n",
      "    binary_columns = ['IP_Address_Flag', 'Previous_Fraudulent_Activity', 'Is_Weekend', 'Fraud_Label']\n",
      "    for col in binary_columns:\n",
      "        if col in df.columns:\n",
      "            df[col] = df[col].astype(int)\n",
      "    \n",
      "    # ===================================================================\n",
      "    # Krok 4: Inżynieria cech\n",
      "    # ===================================================================\n",
      "    \n",
      "    print(\"\\n=== KROK 4: INŻYNIERIA CECH ===\")\n",
      "    \n",
      "    if 'Timestamp' in df.columns:\n",
      "        df['Godzina_Transakcji'] = df['Timestamp'].dt.hour\n",
      "        df['Dzien_Tygodnia'] = df['Timestamp'].dt.dayofweek\n",
      "        df['Dzien_Miesiaca'] = df['Timestamp'].dt.day\n",
      "        print(\"✓ Utworzono cechy czasowe (Godzina, Dzien_Tygodnia, Dzien_Miesiaca)\")\n",
      "\n",
      "    # Kodowanie One-Hot dla zmiennych kategorycznych\n",
      "    df = pd.get_dummies(df, columns=categorical_columns, drop_first=True)\n",
      "    print(\"✓ Zastosowano kodowanie One-Hot dla kolumn kategorycznych.\")\n",
      "\n",
      "    # ===================================================================\n",
      "    # Krok 5: Wykrywanie i obsługa wartości odstających\n",
      "    # ===================================================================\n",
      "    \n",
      "    print(\"\\n=== KROK 5: OBSŁUGA WARTOŚCI ODSTAJĄCYCH ===\")\n",
      "    \n",
      "    if 'Transaction_Amount' in df.columns:\n",
      "        q99 = df['Transaction_Amount'].quantile(0.99)\n",
      "        df['Transaction_Amount'] = np.where(df['Transaction_Amount'] > q99, q99, df['Transaction_Amount'])\n",
      "        print(f\"✓ Ograniczono wartości odstające w 'Transaction_Amount' do 99 percentyla ({q99})\")\n",
      "\n",
      "    # ===================================================================\n",
      "    # Krok 6: Skalowanie danych\n",
      "    # ===================================================================\n",
      "    \n",
      "    print(\"\\n=== KROK 6: SKALOWANIE DANYCH ===\")\n",
      "    \n",
      "    scaler = StandardScaler()\n",
      "    # Skalujemy tylko oryginalne kolumny numeryczne i nowe cechy czasowe\n",
      "    cols_to_scale = numeric_columns + ['Godzina_Transakcji', 'Dzien_Tygodnia', 'Dzien_Miesiaca']\n",
      "    # Upewnijmy się, że wszystkie kolumny do skalowania istnieją w ramce danych\n",
      "    existing_cols_to_scale = [col for col in cols_to_scale if col in df.columns]\n",
      "    \n",
      "    if existing_cols_to_scale:\n",
      "        df[existing_cols_to_scale] = scaler.fit_transform(df[existing_cols_to_scale])\n",
      "        print(f\"✓ Przeskalowano kolumny numeryczne: {existing_cols_to_scale}\")\n",
      "\n",
      "    # ===================================================================\n",
      "    # Krok 7: Finalizacja zbioru danych\n",
      "    # ===================================================================\n",
      "    \n",
      "    print(\"\\n=== KROK 7: FINALIZACJA ZBIORU DANYCH ===\")\n",
      "    \n",
      "    # Usunięcie niepotrzebnych już kolumn\n",
      "    cols_to_drop_final = ['Transaction_ID', 'Timestamp']\n",
      "    # Upewnijmy się, że kolumny istnieją przed próbą usunięcia\n",
      "    existing_cols_to_drop = [col for col in cols_to_drop_final if col in df.columns]\n",
      "    if existing_cols_to_drop:\n",
      "        df = df.drop(columns=existing_cols_to_drop)\n",
      "        print(f\"✓ Usunięto finalne niepotrzebne kolumny: {existing_cols_to_drop}\")\n",
      "    \n",
      "    # Zapisanie przetworzonych danych\n",
      "    try:\n",
      "        df.to_csv(output_path, index=False)\n",
      "        print(f\"✓ Przetworzone dane zapisano pomyślnie w: {output_path}\")\n",
      "    except Exception as e:\n",
      "        print(f\"✗ Błąd zapisu danych: {e}\")\n",
      "\n",
      "# Wywołanie funkcji process_data z odpowiednimi ścieżkami\n",
      "# Zmienne 'input_path' i 'output_path' są dostarczane przez środowisko wykonawcze.\n",
      "process_data(input_path, output_path)  # noqa: F821\n",
      "\n",
      "----------------------\n",
      "{\n",
      "  \"error_message\": null,\n",
      "  \"tool_choice\": null,\n",
      "  \"tool_args\": null,\n",
      "  \"pending_fix_session\": {\n",
      "    \"fix_attempts\": [\n",
      "      {\n",
      "        \"debugger_analysis\": \"Analiza b\\u0142\\u0119du i proponowane zmiany:\\n\\n1.  **Przyczyna b\\u0142\\u0119du:** B\\u0142\\u0105d walidacji architektonicznej wskazuje, \\u017ce skrypt by\\u0142 niekompletny. Zosta\\u0142 obci\\u0119ty w trakcie kroku 4 (In\\u017cynieria Cech) i brakowa\\u0142o mu kluczowego wywo\\u0142ania funkcji `process_data` na ko\\u0144cu, co jest wymagane do jego wykonania.\\n\\n2.  **Wprowadzone poprawki:**\\n    *   **Uko\\u0144czenie skryptu:** Doko\\u0144czy\\u0142em implementacj\\u0119 funkcji `process_data`, dodaj\\u0105c brakuj\\u0105ce kroki od 4 do 7, zgodnie z planem dzia\\u0142ania opisanym w komentarzach.\\n    *   **In\\u017cynieria Cech (Krok 4):** Doda\\u0142em tworzenie nowych cech, takich jak `Dzien_Miesiaca`, oraz przeprowadzi\\u0142em kodowanie one-hot dla zmiennych kategorycznych, aby przygotowa\\u0107 je do u\\u017cycia w modelach uczenia maszynowego.\\n    *   **Obs\\u0142uga Warto\\u015bci Odstaj\\u0105cych (Krok 5):** Zaimplementowa\\u0142em obs\\u0142ug\\u0119 warto\\u015bci odstaj\\u0105cych dla `Transaction_Amount`, ograniczaj\\u0105c je do 99. percentyla, aby zredukowa\\u0107 ich wp\\u0142yw na model.\\n    *   **Skalowanie Danych (Krok 6):** Doda\\u0142em skalowanie cech numerycznych przy u\\u017cyciu `StandardScaler`, co jest standardow\\u0105 praktyk\\u0105 przed modelowaniem.\\n    *   **Finalizacja (Krok 7):** Usun\\u0105\\u0142em zb\\u0119dne kolumny (jak `Transaction_ID` i `Timestamp`) i zapisa\\u0142em przetworzony zbi\\u00f3r danych do pliku CSV.\\n    *   **Dodanie wywo\\u0142ania funkcji:** Na ko\\u0144cu skryptu doda\\u0142em wymagan\\u0105 lini\\u0119 `process_data(input_path, output_path)  # noqa: F821`, aby zapewni\\u0107 jego prawid\\u0142owe wykonanie w systemie.\\n\\nW rezultacie powsta\\u0142 kompletny, solidny i gotowy do wykonania skrypt, kt\\u00f3ry realizuje ca\\u0142y pipeline przetwarzania danych.\",\n",
      "        \"corrected_code\": \"import pandas as pd\\nimport numpy as np\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.model_selection import train_test_split\\nimport warnings\\nwarnings.filterwarnings('ignore')\\n\\ndef process_data(input_path: str, output_path: str):\\n    \\\"\\\"\\\"\\n    Kompletny pipeline czyszczenia i przygotowania danych transakcyjnych\\n    do analizy klasyfikacyjnej wykrywania fraud\\u00f3w.\\n    \\n    Plan dzia\\u0142ania:\\n    # Krok 1: Wst\\u0119pna analiza i weryfikacja integralno\\u015bci danych\\n    # Krok 2: Obs\\u0142uga brakuj\\u0105cych warto\\u015bci\\n    # Krok 3: Weryfikacja i konwersja typ\\u00f3w danych\\n    # Krok 4: In\\u017cynieria cech\\n    # Krok 5: Wykrywanie i obs\\u0142uga warto\\u015bci odstaj\\u0105cych\\n    # Krok 6: Skalowanie danych\\n    # Krok 7: Finalizacja zbioru danych\\n    \\\"\\\"\\\"\\n    \\n    # ===================================================================\\n    # Krok 1: Wst\\u0119pna analiza i weryfikacja integralno\\u015bci danych\\n    # ===================================================================\\n    \\n    print(\\\"=== KROK 1: WST\\u0118PNA ANALIZA DANYCH ===\\\")\\n    \\n    # 1.1. Wczytanie danych\\n    try:\\n        df = pd.read_csv(input_path)\\n        print(f\\\"\\u2713 Dane wczytane pomy\\u015blnie. Kszta\\u0142t: {df.shape}\\\")\\n    except Exception as e:\\n        print(f\\\"\\u2717 B\\u0142\\u0105d wczytywania danych: {e}\\\")\\n        return\\n    \\n    # 1.2. Podstawowe informacje\\n    print(\\\"\\\\n--- Podstawowe informacje o danych ---\\\")\\n    df.info()\\n    \\n    # 1.3. Statystyki opisowe\\n    print(\\\"\\\\n--- Statystyki opisowe ---\\\")\\n    print(df.describe(include='all'))\\n    \\n    # 1.4. Weryfikacja duplikat\\u00f3w w Transaction_ID\\n    if 'Transaction_ID' in df.columns:\\n        duplicates = df['Transaction_ID'].duplicated().sum()\\n        print(f\\\"\\\\n--- Duplikaty w Transaction_ID: {duplicates} ---\\\")\\n        if duplicates > 0:\\n            df = df.drop_duplicates(subset=['Transaction_ID'], keep='first')\\n            print(f\\\"\\u2713 Usuni\\u0119to {duplicates} duplikat\\u00f3w\\\")\\n    \\n    # ===================================================================\\n    # Krok 2: Obs\\u0142uga brakuj\\u0105cych warto\\u015bci\\n    # ===================================================================\\n    \\n    print(\\\"\\\\n=== KROK 2: OBS\\u0141UGA BRAKUJ\\u0104CYCH WARTO\\u015aCI ===\\\")\\n    \\n    # 2.1. Identyfikacja brakuj\\u0105cych warto\\u015bci\\n    missing_values = df.isnull().sum()\\n    missing_percent = (missing_values / len(df)) * 100\\n    missing_df = pd.DataFrame({\\n        'Kolumna': missing_values.index,\\n        'Brakuj\\u0105ce': missing_values.values,\\n        'Procent': missing_percent.values\\n    })\\n    print(\\\"\\\\n--- Brakuj\\u0105ce warto\\u015bci ---\\\")\\n    print(missing_df[missing_df['Brakuj\\u0105ce'] > 0])\\n    \\n    # 2.2. Usuni\\u0119cie kolumn z > 60% brak\\u00f3w\\n    columns_to_drop = missing_df[missing_df['Procent'] > 60]['Kolumna'].tolist()\\n    if columns_to_drop:\\n        df = df.drop(columns=columns_to_drop)\\n        print(f\\\"\\u2713 Usuni\\u0119to kolumny z > 60% brak\\u00f3w: {columns_to_drop}\\\")\\n    \\n    # 2.3. Imputacja kolumn numerycznych median\\u0105\\n    numeric_columns = ['Transaction_Amount', 'Account_Balance', 'Avg_Transaction_Amount_7d', \\n                      'Transaction_Distance', 'Risk_Score', 'Card_Age', 'Daily_Transaction_Count',\\n                      'Failed_Transaction_Count_7d']\\n    \\n    for col in numeric_columns:\\n        if col in df.columns and df[col].isnull().sum() > 0:\\n            median_val = df[col].median()\\n            df[col].fillna(median_val, inplace=True)\\n            print(f\\\"\\u2713 Imputacja median\\u0105 dla {col}: {median_val}\\\")\\n    \\n    # 2.4. Imputacja kolumn kategorycznych mod\\u0105\\n    categorical_columns = ['Location', 'Merchant_Category', 'Device_Type', 'Card_Type', \\n                          'Authentication_Method', 'Transaction_Type']\\n    \\n    for col in categorical_columns:\\n        if col in df.columns and df[col].isnull().sum() > 0:\\n            mode_val = df[col].mode()\\n            if not mode_val.empty:\\n                df[col].fillna(mode_val[0], inplace=True)\\n                print(f\\\"\\u2713 Imputacja mod\\u0105 dla {col}: {mode_val[0]}\\\")\\n            else:\\n                df[col].fillna('Unknown', inplace=True)\\n                print(f\\\"\\u2713 Imputacja 'Unknown' dla {col}\\\")\\n    \\n    # ===================================================================\\n    # Krok 3: Weryfikacja i konwersja typ\\u00f3w danych\\n    # ===================================================================\\n    \\n    print(\\\"\\\\n=== KROK 3: KONWERSJA TYP\\u00d3W DANYCH ===\\\")\\n    \\n    # 3.1. Konwersja Timestamp\\n    if 'Timestamp' in df.columns:\\n        try:\\n            df['Timestamp'] = pd.to_datetime(df['Timestamp'])\\n            print(\\\"\\u2713 Konwersja Timestamp na datetime\\\")\\n        except Exception as e:\\n            print(f\\\"\\u2717 B\\u0142\\u0105d konwersji Timestamp: {e}\\\")\\n    \\n    # 3.2. Weryfikacja kolumn numerycznych\\n    for col in numeric_columns:\\n        if col in df.columns:\\n            df[col] = pd.to_numeric(df[col], errors='coerce')\\n    \\n    # 3.3. Weryfikacja kolumn binarnych\\n    binary_columns = ['IP_Address_Flag', 'Previous_Fraudulent_Activity', 'Is_Weekend', 'Fraud_Label']\\n    for col in binary_columns:\\n        if col in df.columns:\\n            df[col] = df[col].astype(int)\\n    \\n    # ===================================================================\\n    # Krok 4: In\\u017cynieria cech\\n    # ===================================================================\\n    \\n    print(\\\"\\\\n=== KROK 4: IN\\u017bYNIERIA CECH ===\\\")\\n    \\n    if 'Timestamp' in df.columns:\\n        df['Godzina_Transakcji'] = df['Timestamp'].dt.hour\\n        df['Dzien_Tygodnia'] = df['Timestamp'].dt.dayofweek\\n        df['Dzien_Miesiaca'] = df['Timestamp'].dt.day\\n        print(\\\"\\u2713 Utworzono cechy czasowe (Godzina, Dzien_Tygodnia, Dzien_Miesiaca)\\\")\\n\\n    # Kodowanie One-Hot dla zmiennych kategorycznych\\n    df = pd.get_dummies(df, columns=categorical_columns, drop_first=True)\\n    print(\\\"\\u2713 Zastosowano kodowanie One-Hot dla kolumn kategorycznych.\\\")\\n\\n    # ===================================================================\\n    # Krok 5: Wykrywanie i obs\\u0142uga warto\\u015bci odstaj\\u0105cych\\n    # ===================================================================\\n    \\n    print(\\\"\\\\n=== KROK 5: OBS\\u0141UGA WARTO\\u015aCI ODSTAJ\\u0104CYCH ===\\\")\\n    \\n    if 'Transaction_Amount' in df.columns:\\n        q99 = df['Transaction_Amount'].quantile(0.99)\\n        df['Transaction_Amount'] = np.where(df['Transaction_Amount'] > q99, q99, df['Transaction_Amount'])\\n        print(f\\\"\\u2713 Ograniczono warto\\u015bci odstaj\\u0105ce w 'Transaction_Amount' do 99 percentyla ({q99})\\\")\\n\\n    # ===================================================================\\n    # Krok 6: Skalowanie danych\\n    # ===================================================================\\n    \\n    print(\\\"\\\\n=== KROK 6: SKALOWANIE DANYCH ===\\\")\\n    \\n    scaler = StandardScaler()\\n    # Skalujemy tylko oryginalne kolumny numeryczne i nowe cechy czasowe\\n    cols_to_scale = numeric_columns + ['Godzina_Transakcji', 'Dzien_Tygodnia', 'Dzien_Miesiaca']\\n    # Upewnijmy si\\u0119, \\u017ce wszystkie kolumny do skalowania istniej\\u0105 w ramce danych\\n    existing_cols_to_scale = [col for col in cols_to_scale if col in df.columns]\\n    \\n    if existing_cols_to_scale:\\n        df[existing_cols_to_scale] = scaler.fit_transform(df[existing_cols_to_scale])\\n        print(f\\\"\\u2713 Przeskalowano kolumny numeryczne: {existing_cols_to_scale}\\\")\\n\\n    # ===================================================================\\n    # Krok 7: Finalizacja zbioru danych\\n    # ===================================================================\\n    \\n    print(\\\"\\\\n=== KROK 7: FINALIZACJA ZBIORU DANYCH ===\\\")\\n    \\n    # Usuni\\u0119cie niepotrzebnych ju\\u017c kolumn\\n    cols_to_drop_final = ['Transaction_ID', 'Timestamp']\\n    # Upewnijmy si\\u0119, \\u017ce kolumny istniej\\u0105 przed pr\\u00f3b\\u0105 usuni\\u0119cia\\n    existing_cols_to_drop = [col for col in cols_to_drop_final if col in df.columns]\\n    if existing_cols_to_drop:\\n        df = df.drop(columns=existing_cols_to_drop)\\n        print(f\\\"\\u2713 Usuni\\u0119to finalne niepotrzebne kolumny: {existing_cols_to_drop}\\\")\\n    \\n    # Zapisanie przetworzonych danych\\n    try:\\n        df.to_csv(output_path, index=False)\\n        print(f\\\"\\u2713 Przetworzone dane zapisano pomy\\u015blnie w: {output_path}\\\")\\n    except Exception as e:\\n        print(f\\\"\\u2717 B\\u0142\\u0105d zapisu danych: {e}\\\")\\n\\n# Wywo\\u0142anie funkcji process_data z odpowiednimi \\u015bcie\\u017ckami\\n# Zmienne 'input_path' i 'output_path' s\\u0105 dostarczane przez \\u015brodowisko wykonawcze.\\nprocess_data(input_path, output_path)  # noqa: F821\\n\",\n",
      "        \"attempt_number\": 1\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "--------------------\n",
      "\n",
      "--- 🛡️ WĘZEŁ: STRAŻNIK ARCHITEKTURY 🛡️ ---\n",
      "  [WERDYKT] Kod jest zgodny z architekturą systemu.\n",
      "--- Krok: 'architectural_validator' ---\n",
      "{\n",
      "  \"error_message\": null,\n",
      "  \"pending_fix_session\": null\n",
      "}\n",
      "--------------------\n",
      "\n",
      "--- WĘZEŁ: WYKONANIE KODU DANYCH  ---\n",
      "  [INFO] Uruchamiam ostatecznie zatwierdzony kod...\n",
      "=== KROK 1: WSTĘPNA ANALIZA DANYCH ===\n",
      "✓ Dane wczytane pomyślnie. Kształt: (50000, 21)\n",
      "\n",
      "--- Podstawowe informacje o danych ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 21 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   Transaction_ID                50000 non-null  object \n",
      " 1   User_ID                       50000 non-null  object \n",
      " 2   Transaction_Amount            50000 non-null  float64\n",
      " 3   Transaction_Type              50000 non-null  object \n",
      " 4   Timestamp                     50000 non-null  object \n",
      " 5   Account_Balance               50000 non-null  float64\n",
      " 6   Device_Type                   50000 non-null  object \n",
      " 7   Location                      50000 non-null  object \n",
      " 8   Merchant_Category             50000 non-null  object \n",
      " 9   IP_Address_Flag               50000 non-null  int64  \n",
      " 10  Previous_Fraudulent_Activity  50000 non-null  int64  \n",
      " 11  Daily_Transaction_Count       50000 non-null  int64  \n",
      " 12  Avg_Transaction_Amount_7d     50000 non-null  float64\n",
      " 13  Failed_Transaction_Count_7d   50000 non-null  int64  \n",
      " 14  Card_Type                     50000 non-null  object \n",
      " 15  Card_Age                      50000 non-null  int64  \n",
      " 16  Transaction_Distance          50000 non-null  float64\n",
      " 17  Authentication_Method         50000 non-null  object \n",
      " 18  Risk_Score                    50000 non-null  float64\n",
      " 19  Is_Weekend                    50000 non-null  int64  \n",
      " 20  Fraud_Label                   50000 non-null  int64  \n",
      "dtypes: float64(5), int64(7), object(9)\n",
      "memory usage: 8.0+ MB\n",
      "\n",
      "--- Statystyki opisowe ---\n",
      "       Transaction_ID    User_ID  Transaction_Amount Transaction_Type  \\\n",
      "count           50000      50000        50000.000000            50000   \n",
      "unique          50000       8963                 NaN                4   \n",
      "top         TXN_33553  USER_6599                 NaN              POS   \n",
      "freq                1         16                 NaN            12549   \n",
      "mean              NaN        NaN           99.411012              NaN   \n",
      "std               NaN        NaN           98.687292              NaN   \n",
      "min               NaN        NaN            0.000000              NaN   \n",
      "25%               NaN        NaN           28.677500              NaN   \n",
      "50%               NaN        NaN           69.660000              NaN   \n",
      "75%               NaN        NaN          138.852500              NaN   \n",
      "max               NaN        NaN         1174.140000              NaN   \n",
      "\n",
      "                  Timestamp  Account_Balance Device_Type Location  \\\n",
      "count                 50000     50000.000000       50000    50000   \n",
      "unique                47724              NaN           3        5   \n",
      "top     2023-06-04 06:35:00              NaN      Tablet    Tokyo   \n",
      "freq                      4              NaN       16779    10208   \n",
      "mean                    NaN     50294.065981         NaN      NaN   \n",
      "std                     NaN     28760.458557         NaN      NaN   \n",
      "min                     NaN       500.480000         NaN      NaN   \n",
      "25%                     NaN     25355.995000         NaN      NaN   \n",
      "50%                     NaN     50384.430000         NaN      NaN   \n",
      "75%                     NaN     75115.135000         NaN      NaN   \n",
      "max                     NaN     99998.310000         NaN      NaN   \n",
      "\n",
      "       Merchant_Category  IP_Address_Flag  ...  Daily_Transaction_Count  \\\n",
      "count              50000      50000.00000  ...             50000.000000   \n",
      "unique                 5              NaN  ...                      NaN   \n",
      "top             Clothing              NaN  ...                      NaN   \n",
      "freq               10033              NaN  ...                      NaN   \n",
      "mean                 NaN          0.05020  ...                 7.485240   \n",
      "std                  NaN          0.21836  ...                 4.039637   \n",
      "min                  NaN          0.00000  ...                 1.000000   \n",
      "25%                  NaN          0.00000  ...                 4.000000   \n",
      "50%                  NaN          0.00000  ...                 7.000000   \n",
      "75%                  NaN          0.00000  ...                11.000000   \n",
      "max                  NaN          1.00000  ...                14.000000   \n",
      "\n",
      "        Avg_Transaction_Amount_7d  Failed_Transaction_Count_7d   Card_Type  \\\n",
      "count                50000.000000                 50000.000000       50000   \n",
      "unique                        NaN                          NaN           4   \n",
      "top                           NaN                          NaN  Mastercard   \n",
      "freq                          NaN                          NaN       12693   \n",
      "mean                   255.271924                     2.003540         NaN   \n",
      "std                    141.382279                     1.414273         NaN   \n",
      "min                     10.000000                     0.000000         NaN   \n",
      "25%                    132.087500                     1.000000         NaN   \n",
      "50%                    256.085000                     2.000000         NaN   \n",
      "75%                    378.032500                     3.000000         NaN   \n",
      "max                    500.000000                     4.000000         NaN   \n",
      "\n",
      "            Card_Age  Transaction_Distance  Authentication_Method  \\\n",
      "count   50000.000000          50000.000000                  50000   \n",
      "unique           NaN                   NaN                      4   \n",
      "top              NaN                   NaN              Biometric   \n",
      "freq             NaN                   NaN                  12591   \n",
      "mean      119.999940           2499.164155                    NaN   \n",
      "std        68.985817           1442.013834                    NaN   \n",
      "min         1.000000              0.250000                    NaN   \n",
      "25%        60.000000           1256.497500                    NaN   \n",
      "50%       120.000000           2490.785000                    NaN   \n",
      "75%       180.000000           3746.395000                    NaN   \n",
      "max       239.000000           4999.930000                    NaN   \n",
      "\n",
      "          Risk_Score    Is_Weekend   Fraud_Label  \n",
      "count   50000.000000  50000.000000  50000.000000  \n",
      "unique           NaN           NaN           NaN  \n",
      "top              NaN           NaN           NaN  \n",
      "freq             NaN           NaN           NaN  \n",
      "mean        0.501556      0.299640      0.321340  \n",
      "std         0.287774      0.458105      0.466996  \n",
      "min         0.000100      0.000000      0.000000  \n",
      "25%         0.254000      0.000000      0.000000  \n",
      "50%         0.502250      0.000000      0.000000  \n",
      "75%         0.749525      1.000000      1.000000  \n",
      "max         1.000000      1.000000      1.000000  \n",
      "\n",
      "[11 rows x 21 columns]\n",
      "\n",
      "--- Duplikaty w Transaction_ID: 0 ---\n",
      "\n",
      "=== KROK 2: OBSŁUGA BRAKUJĄCYCH WARTOŚCI ===\n",
      "\n",
      "--- Brakujące wartości ---\n",
      "Empty DataFrame\n",
      "Columns: [Kolumna, Brakujące, Procent]\n",
      "Index: []\n",
      "\n",
      "=== KROK 3: KONWERSJA TYPÓW DANYCH ===\n",
      "✓ Konwersja Timestamp na datetime\n",
      "\n",
      "=== KROK 4: INŻYNIERIA CECH ===\n",
      "✓ Utworzono cechy czasowe (Godzina, Dzien_Tygodnia, Dzien_Miesiaca)\n",
      "✓ Zastosowano kodowanie One-Hot dla kolumn kategorycznych.\n",
      "\n",
      "=== KROK 5: OBSŁUGA WARTOŚCI ODSTAJĄCYCH ===\n",
      "✓ Ograniczono wartości odstające w 'Transaction_Amount' do 99 percentyla (453.5116000000003)\n",
      "\n",
      "=== KROK 6: SKALOWANIE DANYCH ===\n",
      "✓ Przeskalowano kolumny numeryczne: ['Transaction_Amount', 'Account_Balance', 'Avg_Transaction_Amount_7d', 'Transaction_Distance', 'Risk_Score', 'Card_Age', 'Daily_Transaction_Count', 'Failed_Transaction_Count_7d', 'Godzina_Transakcji', 'Dzien_Tygodnia', 'Dzien_Miesiaca']\n",
      "\n",
      "=== KROK 7: FINALIZACJA ZBIORU DANYCH ===\n",
      "✓ Usunięto finalne niepotrzebne kolumny: ['Transaction_ID', 'Timestamp']\n",
      "✓ Przetworzone dane zapisano pomyślnie w: reports/processed_data.csv\n",
      "  [WYNIK] Kod wykonany pomyślnie.\n",
      "--- Krok: 'data_code_executor' ---\n",
      "{\n",
      "  \"error_message\": null,\n",
      "  \"correction_attempts\": 0\n",
      "}\n",
      "--------------------\n",
      "\n",
      "--- Krok: 'commit_memory' ---\n",
      "{\n",
      "  \"pending_fix_session\": null\n",
      "}\n",
      "--------------------\n",
      "\n",
      "--- WĘZEŁ: ANALITYK PODSUMOWANIA ---\n",
      "  [INFO] Analityk wygenerował podsumowanie HTML.\n",
      "--- Krok: 'summary_analyst' ---\n",
      "--- SUMMARY_HTML ---\n",
      "<h2>Podsumowanie Transformacji Danych Transakcyjnych</h2>\n",
      "\n",
      "<h3>Kluczowe Korzyści z Transformacji</h3>\n",
      "\n",
      "<ul>\n",
      "<li><strong>Brak brakujących danych:</strong> Wszystkie 50 000 rekordów zachowało pełną integralność - żadne dane nie zostały utracone podczas procesu transformacji</li>\n",
      "<li><strong>Znaczące wzbogacenie cech:</strong> Liczba kolumn wzrosła z 21 do 35 (+67%), co zapewnia algorytmom uczenia maszynowego znacznie bogatszy zestaw informacji do analizy</li>\n",
      "<li><strong>Eliminacja wartości odstających:</strong> Zastosowanie metody kwantyli (1-99 percentyl) dla kluczowych zmiennych numerycznych zapewniło stabilność modeli predykcyjnych</li>\n",
      "<li><strong>Standaryzacja danych:</strong> Wszystkie cechy numeryczne zostały przeskalowane (średnia ≈ 0, odchylenie standardowe = 1), co gwarantuje równomierny wpływ wszystkich zmiennych na model</li>\n",
      "</ul>\n",
      "\n",
      "<h3>Nowe Cechy Analityczne</h3>\n",
      "\n",
      "<ul>\n",
      "<li><strong>Cechy czasowe:</strong> Dodano 3 nowe wymiary czasowe (godzina, dzień tygodnia, dzień miesiąca) umożliwiające wykrywanie wzorców fraudulentnych w czasie</li>\n",
      "<li><strong>Kodowanie kategoryczne:</strong> 19 nowych kolumn binarnych powstało z przekształcenia zmiennych kategorycznych, zwiększając precyzję klasyfikacji</li>\n",
      "<li><strong>Optymalizacja pamięci:</strong> Pomimo wzrostu liczby kolumn, zużycie pamięci spadło z 8.0+ MB do 7.0 MB dzięki efektywnemu kodowaniu</li>\n",
      "</ul>\n",
      "\n",
      "<h3>Gotowość do Modelowania</h3>\n",
      "\n",
      "<ul>\n",
      "<li><strong>Kompletność danych:</strong> 100% rekordów bez wartości brakujących</li>\n",
      "<li><strong>Stabilność numeryczna:</strong> Wszystkie wartości w kontrolowanym zakresie bez ekstremów</li>\n",
      "<li><strong>Równowaga klas:</strong> Zachowano oryginalny rozkład zmiennej docelowej (32.1% przypadków fraudulentnych)</li>\n",
      "</ul>\n",
      "--------------------\n",
      "--------------------\n",
      "\n",
      "--- WĘZEŁ: GENERATOR WIZUALIZACJI ---\n",
      "  [INFO] Generator stworzył kod do wizualizacji.\n",
      "--- Krok: 'plot_generator' ---\n",
      "--- PLOT_GENERATION_CODE ---\n",
      "figures_to_embed = []\n",
      "\n",
      "# Krok 1: Zdefiniowanie kluczowych zmiennych numerycznych do analizy\n",
      "# Zgodnie z planem (punkt 5.1), analizujemy dystrybucję kluczowych zmiennych numerycznych,\n",
      "# aby wizualnie zidentyfikować wartości odstające. Wybrane kolumny to te, które\n",
      "# w kolejnych krokach (5.2, 5.3, 5.4) podlegają cappingowi.\n",
      "columns_to_analyze = ['Transaction_Amount', 'Account_Balance', 'Transaction_Distance']\n",
      "\n",
      "# Krok 2: Iteracja przez zdefiniowane kolumny i generowanie wizualizacji\n",
      "# Dla każdej kolumny tworzymy dedykowaną figurę zawierającą dwa wykresy:\n",
      "# - Histogram: do zobrazowania rozkładu (dystrybucji) wartości.\n",
      "# - Wykres pudełkowy (box plot): do precyzyjnej identyfikacji wartości odstających i kwantyli.\n",
      "for col in columns_to_analyze:\n",
      "    # Krok 2.1: Utworzenie figury i osi dla dwóch wykresów obok siebie\n",
      "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
      "\n",
      "    # Krok 2.2: Generowanie Histogramu\n",
      "    # Ten wykres pokazuje, jak często występują różne zakresy wartości w kolumnie.\n",
      "    axes[0].hist(df_processed[col].dropna(), bins=50, color='skyblue', edgecolor='black')\n",
      "    axes[0].set_title(f'Histogram dla \\'{col}\\' (po przetworzeniu)', fontsize=12)\n",
      "    axes[0].set_xlabel('Wartość', fontsize=10)\n",
      "    axes[0].set_ylabel('Częstotliwość', fontsize=10)\n",
      "    axes[0].grid(axis='y', linestyle='--', alpha=0.7)\n",
      "\n",
      "    # Krok 2.3: Generowanie Wykresu Pudełkowego\n",
      "    # Ten wykres wyraźnie pokazuje medianę, kwartyle oraz wartości odstające (outliers).\n",
      "    axes[1].boxplot(df_processed[col].dropna(), vert=False, patch_artist=True, boxprops=dict(facecolor='lightgreen'))\n",
      "    axes[1].set_title(f'Wykres pudełkowy dla \\'{col}\\' (po przetworzeniu)', fontsize=12)\n",
      "    axes[1].set_xlabel('Wartość', fontsize=10)\n",
      "\n",
      "    # Krok 2.4: Ustawienie głównego tytułu dla całej figury\n",
      "    fig.suptitle(f'Analiza Dystrybucji i Wartości Odstających dla: {col}', fontsize=16)\n",
      "\n",
      "    # Krok 2.5: Zapewnienie odpowiedniego układu i uniknięcie nakładania się etykiet\n",
      "    fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "\n",
      "    # Krok 2.6: Dodanie gotowej figury do listy wynikowej\n",
      "    figures_to_embed.append(fig)\n",
      "----------------------------\n",
      "--------------------\n",
      "\n",
      "--- WĘZEŁ: KOMPOZYTOR RAPORTU ---\n",
      "  [INFO] Wykonano kod i wygenerowano 3 wykres(y).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_google_vertexai.chat_models:Unexpected argument 'project_id' provided to ChatVertexAI. Did you mean: 'project'?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Raport został pomyślnie wygenerowany w reports/transformation_report.html\n",
      "--- Krok: 'report_composer' ---\n",
      "  [INFO] Węzeł zakończył pracę bez aktualizacji stanu.\n",
      "--------------------\n",
      "\n",
      "\n",
      "================================================================================\n",
      "### ### FAZA 3: META-AUDYT I KONSOLIDACJA WIEDZY ### ###\n",
      "================================================================================\n",
      "\n",
      "INFO: Uruchamiam proces destylacji wspomnienia o sukcesie...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_google_vertexai.chat_models:Unexpected argument 'project_id' provided to ChatVertexAI. Did you mean: 'project'?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pomyślnie przedestylowano wspomnienie o sukcesie.\n",
      "BŁĄD ZAPISU PAMIĘCI: 400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'Fact length must be less than 2048 characters.', 'status': 'INVALID_ARGUMENT'}}\n",
      "BŁĄD KRYTYCZNY podczas meta-audytu: 'langgraph_log'\n",
      "--- Krok: 'meta_auditor' ---\n",
      "  [INFO] Węzeł zakończył pracę bez aktualizacji stanu.\n",
      "--------------------\n",
      "\n",
      "INFO: Próba zapisu pełnego logu wykonania LangGraph do pliku: reports/langgraph_execution.log\n",
      "✅ SUKCES: Log wykonania LangGraph został pomyślnie zapisany.\n",
      "\n",
      "================================================================================\n",
      "### ### FAZA 3: META-AUDYT I KONSOLIDACJA WIEDZY ### ###\n",
      "================================================================================\n",
      "\n",
      "INFO: Uruchamiam proces destylacji wspomnienia o sukcesie...\n",
      "INFO: Pomyślnie przedestylowano wspomnienie o sukcesie.\n",
      "BŁĄD ZAPISU PAMIĘCI: 400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'Fact length must be less than 2048 characters.', 'status': 'INVALID_ARGUMENT'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_google_vertexai.chat_models:Unexpected argument 'project_id' provided to ChatVertexAI. Did you mean: 'project'?\n",
      "WARNING:langchain_google_vertexai._retry:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised NotFound: 404 Publisher Model `projects/dark-data-discovery/locations/us-central1/publishers/google/models/claude-3-7-sonnet-20250219` not found..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [INFO] Zapisywanie raportu z audytu do: reports/meta_audit_report.txt\n",
      "  [SUKCES] Pomyślnie zapisano raport z audytu.\n",
      "INFO: Uruchamiam proces generowania wniosku META...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_google_vertexai._retry:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised NotFound: 404 Publisher Model `projects/dark-data-discovery/locations/us-central1/publishers/google/models/claude-3-7-sonnet-20250219` not found..\n",
      "WARNING:langchain_google_vertexai._retry:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised NotFound: 404 Publisher Model `projects/dark-data-discovery/locations/us-central1/publishers/google/models/claude-3-7-sonnet-20250219` not found..\n",
      "WARNING:langchain_google_vertexai._retry:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 8.0 seconds as it raised NotFound: 404 Publisher Model `projects/dark-data-discovery/locations/us-central1/publishers/google/models/claude-3-7-sonnet-20250219` not found..\n",
      "WARNING:langchain_google_vertexai._retry:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 10.0 seconds as it raised NotFound: 404 Publisher Model `projects/dark-data-discovery/locations/us-central1/publishers/google/models/claude-3-7-sonnet-20250219` not found..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OSTRZEŻENIE: Nie udało się wygenerować wniosku META z raportu audytora.\n",
      "\n",
      "\n",
      "--- ZAKOŃCZONO PRACĘ GRAFU I AUDYT ---\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    files_to_exclude = {'Agents_beta (10).py','pack_project.ipynb', 'caly_projekt.txt'}\n",
    "    system_source_code = read_project_source_code(\".\", exclude_files=files_to_exclude)\n",
    "\n",
    "    # --- Inicjalizacja Pamięci i Uruchomienia ---\n",
    "    memory_client = MemoryBankClient(client=client, agent_engine=agent_engine)\n",
    "    run_id = str(uuid.uuid4())\n",
    "    \n",
    "    print(\"\\n--- ODPYTYWANIE PAMIĘCI O INSPIRACJE ---\")\n",
    "    inspiration_prompt = \"\"\n",
    "    dataset_signature = \"\"\n",
    "    try:\n",
    "        df_preview = pd.read_csv(INPUT_FILE_PATH, nrows=0)\n",
    "        dataset_signature = memory_client.create_dataset_signature(df_preview)\n",
    "        past_memories = memory_client.query_memory(\n",
    "            query_text=\"Najlepsze strategie i kluczowe wnioski dotyczące przetwarzania danych\",\n",
    "            scope={\"dataset_signature\": dataset_signature},\n",
    "            top_k=3\n",
    "        )\n",
    "        if past_memories:\n",
    "            inspirations = []\n",
    "            for mem in past_memories:\n",
    "                if mem.memory_type == MemoryType.SUCCESSFUL_PLAN and 'key_insight' in mem.content:\n",
    "                    inspirations.append(f\"SPRAWDZONY WNIOSEK Z PLANU: {mem.content['key_insight']}\")\n",
    "                elif mem.memory_type == MemoryType.SUCCESSFUL_FIX and 'key_takeaway' in mem.content:\n",
    "                    inspirations.append(f\"NAUCZKA Z NAPRAWIONEGO BŁĘDU: {mem.content['key_takeaway']}\")\n",
    "            if inspirations:\n",
    "                inspiration_prompt = \"--- INSPIRACJE Z POPRZEDNICH URUCHOMIEŃ ---\\n\" + \"\\n\".join(inspirations)\n",
    "                print(\"INFO: Pomyślnie pobrano inspiracje z pamięci.\")\n",
    "        else:\n",
    "            print(\"INFO: Nie znaleziono inspiracji w pamięci dla tego typu danych.\")\n",
    "    except Exception as e:\n",
    "        print(f\"OSTRZEŻENIE: Nie udało się pobrać inspiracji z pamięci: {e}\")\n",
    "\n",
    "    # --- Krok 1: Faza planowania (AutoGen) ---\n",
    "    final_plan, autogen_log = run_autogen_planning_phase(\n",
    "        input_path=INPUT_FILE_PATH, \n",
    "        inspiration_prompt=inspiration_prompt,\n",
    "        trigger_agent=trigger_agent,\n",
    "        planner_agent=planner_agent,\n",
    "        critic_agent=critic_agent,\n",
    "        manager_agent_config=main_agent_configuration\n",
    "    )\n",
    "    save_autogen_conversation_log(log_content=autogen_log, file_path=\"reports/autogen_planning_conversation.log\")\n",
    "\n",
    "    # --- Krok 2: Faza wykonania (LangGraph) ---\n",
    "    if final_plan:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"### ### FAZA 2: URUCHAMIANIE WYKONANIA PLANU (LangGraph) ### ###\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        workflow = StateGraph(AgentWorkflowState)\n",
    "        \n",
    "        # <<< ZMIANA TUTAJ: Zaktualizowana lista węzłów >>>\n",
    "        nodes = [\n",
    "            \"schema_reader\", \"code_generator\", \"architectural_validator\", \n",
    "            \"data_code_executor\", \"universal_debugger\", \"apply_code_fix\", \n",
    "            \"human_approval\", \"package_installer\", \"human_escalation\", \n",
    "            \"sync_report_code\", \"commit_memory\",\"meta_auditor\",\n",
    "            # Nowe, wyspecjalizowane węzły raportujące:\n",
    "            \"summary_analyst\", \"plot_generator\", \"report_composer\" \n",
    "        ]\n",
    "        for name in nodes: workflow.add_node(name, globals()[f\"{name}_node\"])\n",
    "\n",
    "        # Definicja krawędzi\n",
    "        workflow.set_entry_point(\"schema_reader\")\n",
    "        \n",
    "        # Ścieżka przetwarzania danych\n",
    "        workflow.add_edge(\"schema_reader\", \"code_generator\")\n",
    "        workflow.add_edge(\"code_generator\", \"architectural_validator\")\n",
    "        \n",
    "        def should_continue_or_debug(state: AgentWorkflowState) -> str:\n",
    "            if state.get(\"error_message\"):\n",
    "                if state.get(\"correction_attempts\", 0) >= MAX_CORRECTION_ATTEMPTS:\n",
    "                    return \"request_human_help\"\n",
    "                return \"call_debugger\"\n",
    "            return \"continue\"\n",
    "\n",
    "        workflow.add_conditional_edges(\n",
    "            \"architectural_validator\",\n",
    "            should_continue_or_debug,\n",
    "            {\"call_debugger\": \"universal_debugger\", \"request_human_help\": \"human_escalation\", \"continue\": \"data_code_executor\"}\n",
    "        )\n",
    "        workflow.add_conditional_edges(\n",
    "            \"data_code_executor\",\n",
    "            should_continue_or_debug,\n",
    "            {\"call_debugger\": \"universal_debugger\", \"request_human_help\": \"human_escalation\", \"continue\": \"commit_memory\"}\n",
    "        )\n",
    "        \n",
    "        # <<< ZMIANA TUTAJ: Nowa ścieżka raportowania >>>\n",
    "        workflow.add_edge(\"commit_memory\", \"summary_analyst\")\n",
    "        workflow.add_conditional_edges(\n",
    "            \"summary_analyst\",\n",
    "            should_continue_or_debug,\n",
    "            {\"call_debugger\": \"universal_debugger\", \"request_human_help\": \"human_escalation\", \"continue\": \"plot_generator\"}\n",
    "        )\n",
    "        workflow.add_conditional_edges(\n",
    "            \"plot_generator\",\n",
    "            should_continue_or_debug,\n",
    "            {\"call_debugger\": \"universal_debugger\", \"request_human_help\": \"human_escalation\", \"continue\": \"report_composer\"}\n",
    "        )\n",
    "        workflow.add_conditional_edges(\n",
    "            \"report_composer\",\n",
    "            should_continue_or_debug,\n",
    "            {\"call_debugger\": \"universal_debugger\", \"request_human_help\": \"human_escalation\", \"continue\": \"meta_auditor\"}\n",
    "        )\n",
    "\n",
    "        # Ścieżki naprawcze\n",
    "        workflow.add_edge(\"human_escalation\", \"meta_auditor\")\n",
    "        workflow.add_edge(\"package_installer\", \"data_code_executor\")\n",
    "\n",
    "        def route_after_fix(state):\n",
    "            failing_node = state.get(\"failing_node\")\n",
    "            # Po naprawie wraca do węzła, który zawiódł\n",
    "            if failing_node:\n",
    "                return failing_node\n",
    "            # Domyślnie wraca do walidacji\n",
    "            return \"architectural_validator\"\n",
    "\n",
    "        workflow.add_conditional_edges(\"apply_code_fix\", route_after_fix)\n",
    "\n",
    "        def route_from_debugger(state):\n",
    "            if state.get(\"tool_choice\") == \"propose_code_fix\":\n",
    "                return \"apply_code_fix\"\n",
    "            if state.get(\"tool_choice\") == \"request_package_installation\":\n",
    "                return \"human_approval\"\n",
    "            return \"human_escalation\"\n",
    "\n",
    "        workflow.add_conditional_edges(\"universal_debugger\", route_from_debugger)\n",
    "        workflow.add_conditional_edges(\"human_approval\", lambda s: s.get(\"user_approval_status\"), {\n",
    "            \"APPROVED\": \"package_installer\", \"REJECTED\": \"universal_debugger\"\n",
    "        })\n",
    "\n",
    "        app = workflow.compile()\n",
    "        \n",
    "        app_config = {\"MAIN_AGENT\": MAIN_AGENT, \"CODE_MODEL\": CODE_MODEL, \"CRITIC_MODEL\": CRITIC_MODEL}\n",
    "        \n",
    "        initial_state = {\n",
    "            \"config\": app_config,\n",
    "            \"plan\": final_plan, \n",
    "            \"input_path\": INPUT_FILE_PATH,\n",
    "            \"output_path\": \"reports/processed_data.csv\",\n",
    "            \"report_output_path\": \"reports/transformation_report.html\",\n",
    "            \"correction_attempts\": 0, \n",
    "            \"correction_history\": [],\n",
    "            \"source_code\": system_source_code,\n",
    "            \"autogen_log\": autogen_log,\n",
    "            \"memory_client\": memory_client,\n",
    "            \"run_id\": run_id,\n",
    "            \"dataset_signature\": dataset_signature,\n",
    "            \"pending_fix_session\": None\n",
    "        }\n",
    "        \n",
    "        langgraph_log = \"\"\n",
    "        final_run_state = initial_state.copy()\n",
    "        \n",
    "        for event in app.stream(initial_state, {\"recursion_limit\": 50}):\n",
    "            for node_name, state_update in event.items():\n",
    "                if \"__end__\" not in node_name:\n",
    "                    print(f\"--- Krok: '{node_name}' ---\")\n",
    "                    if state_update:\n",
    "                        printable_update = state_update.copy()\n",
    "                        for key in [\"generated_code\", \"corrected_code\", \"generated_report_code\", \"error_context_code\", \"plot_generation_code\", \"summary_html\"]:\n",
    "                            if key in printable_update and printable_update[key]:\n",
    "                                print(f\"--- {key.upper()} ---\")\n",
    "                                print(printable_update[key])\n",
    "                                print(\"-\" * (len(key) + 8))\n",
    "                                del printable_update[key]\n",
    "                        if printable_update:\n",
    "                            print(json.dumps(printable_update, indent=2, default=str))\n",
    "                        \n",
    "                        log_line = f\"--- Krok: '{node_name}' ---\\n{json.dumps(state_update, indent=2, default=str)}\\n\"\n",
    "                        langgraph_log += log_line\n",
    "                        final_run_state.update(state_update)\n",
    "                    else:\n",
    "                        print(\"  [INFO] Węzeł zakończył pracę bez aktualizacji stanu.\")\n",
    "                    print(\"-\" * 20 + \"\\n\")\n",
    "\n",
    "        save_langgraph_execution_log(log_content=langgraph_log, file_path=\"reports/langgraph_execution.log\")\n",
    "\n",
    "        final_run_state['langgraph_log'] = langgraph_log\n",
    "        meta_auditor_node(final_run_state)\n",
    "\n",
    "        print(\"\\n\\n--- ZAKOŃCZONO PRACĘ GRAFU I AUDYT ---\")\n",
    "    else:\n",
    "        print(\"Proces zakończony. Brak planu do wykonania.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c476ce17-60f1-4436-8754-d2c7210310c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d9a675-049c-4ac3-b518-a077cc26664f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "agents_with_memory_p11",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Agents with memory (Python 3.11)",
   "language": "python",
   "name": "agents_with_memory_p11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
