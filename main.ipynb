{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b251096d-f8c3-4dfb-ae10-5d33be45f50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import json\n",
    "import vertexai\n",
    "from vertexai import agent_engines\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, List, Callable, Dict, Optional, Union, Any\n",
    "# Importy z własnych modułów\n",
    "from config import PROJECT_ID, LOCATION, MEMORY_ENGINE_DISPLAY_NAME, INPUT_FILE_PATH,MAIN_AGENT,CRITIC_MODEL,CODE_MODEL, API_TYPE_GEMINI,API_TYPE_SONNET, ANTHROPIC_API_KEY,basic_config_agent\n",
    "from agents.state import AgentWorkflowState\n",
    "from agents.autogen_agents import TriggerAgent,PlannerAgent,CriticAgent\n",
    "from prompts import AutoGen_Agents_Propmpt\n",
    "from agents.langgraph_nodes import * \n",
    "from agents.autogen_agent_utils import run_autogen_planning_phase\n",
    "from memory.memory_bank_client import MemoryBankClient\n",
    "from tools.utils import read_source_code, save_autogen_conversation_log, save_langgraph_execution_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b380a6ac-127b-44dd-9e3b-e0721815cd25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "AGENT_ENGINE_NAME = \"\" # Zostanie wypełniona po pobraniu lub utworzeniu silnika\n",
    "\n",
    "# Inicjalizacja głównego klienta Vertex AI\n",
    "client = vertexai.Client(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ba1b166-e3c9-4d11-9a08-76336faaa064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_agent_engine(display_name: str) :\n",
    "    \"\"\"\n",
    "    Pobiera istniejący Agent Engine po nazwie wyświetlanej lub tworzy nowy, jeśli nie istnieje.\n",
    "    \"\"\"\n",
    "    # 1. Pobierz listę wszystkich istniejących silników w projekcie\n",
    "    all_engines = agent_engines.list()\n",
    "    \n",
    "    # 2. Sprawdź, czy któryś z nich ma pasującą nazwę\n",
    "    for engine in all_engines:\n",
    "        if engine.display_name == display_name:\n",
    "            print(f\"INFO: Znaleziono i połączono z istniejącym Agent Engine: '{display_name}'\")\n",
    "            return engine\n",
    "            \n",
    "    # 3. Jeśli pętla się zakończyła i nic nie znaleziono, stwórz nowy silnik\n",
    "    print(f\"INFO: Nie znaleziono Agent Engine o nazwie '{display_name}'. Tworzenie nowego...\")\n",
    "    try:\n",
    "        new_engine = agent_engines.create(\n",
    "            display_name=display_name\n",
    "        )\n",
    "        print(f\"INFO: Pomyślnie utworzono nowy Agent Engine.\")\n",
    "        return new_engine\n",
    "    except Exception as e:\n",
    "        print(f\"KRYTYCZNY BŁĄD: Nie można utworzyć Agent Engine. Sprawdź konfigurację i uprawnienia. Błąd: {e}\")\n",
    "        exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80731513-5d98-4048-89f8-359410538a59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Znaleziono i połączono z istniejącym Agent Engine: 'memory-gamma-way'\n",
      "projects/815755318672/locations/us-central1/reasoningEngines/3849548538518175744\n"
     ]
    }
   ],
   "source": [
    "agent_engine =get_or_create_agent_engine(MEMORY_ENGINE_DISPLAY_NAME)\n",
    "AGENT_ENGINE_NAME = agent_engine.resource_name\n",
    "print(AGENT_ENGINE_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6982c7b4-a6dd-476f-b361-d36c50174185",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Konfiguracja czatu grupowego ---\n",
    "main_agent_configuration={\"cache_seed\": 42,\"seed\": 42,\"temperature\": 0.0,\n",
    "                        \"config_list\": basic_config_agent(agent_name=MAIN_AGENT, api_type=API_TYPE_GEMINI, location=LOCATION, project_id=PROJECT_ID)}\n",
    "critic_agent_configuration ={\"cache_seed\": 42,\"seed\": 42,\"temperature\": 0.0,\n",
    "                        \"config_list\": basic_config_agent(api_key=ANTHROPIC_API_KEY,agent_name=CRITIC_MODEL, api_type=API_TYPE_SONNET)}\n",
    "trigger_prompt = str(AutoGen_Agents_Propmpt.Trigger_prompt())\n",
    "planner_prompt = str(AutoGen_Agents_Propmpt.Planner_prompt())\n",
    "critic_prompt = str(AutoGen_Agents_Propmpt.Critic_prompt())\n",
    "#---WYWOŁANIE AGENTÓW\n",
    "trigger_agent = TriggerAgent(llm_config=main_agent_configuration, prompt=trigger_prompt)\n",
    "planner_agent = PlannerAgent(llm_config=main_agent_configuration,prompt=planner_prompt)\n",
    "critic_agent = CriticAgent(llm_config=main_agent_configuration,prompt=critic_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9d12db3-dfe5-4c56-9494-8721eccacda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: MemoryBankClient gotowy do pracy z silnikiem: projects/815755318672/locations/us-central1/reasoningEngines/3849548538518175744\n",
      "\n",
      "--- ODPYTYWANIE PAMIĘCI O INSPIRACJE ---\n",
      "INFO: Odpytuję pamięć semantycznie z zapytaniem 'Najlepsze strategie i kluczowe wnioski dotyczące przetwarzania danych' w zakresie {'dataset_signature': 'ae1568fe7dae11d4bacd0c21ed718503'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/olga_zydziak/version_beta/multiagent_system/memory/memory_bank_client.py:95: ExperimentalWarning: The Vertex SDK GenAI agent engines module is experimental, and may change in future versions.\n",
      "  memories_iterator = self.client.agent_engines.retrieve_memories(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Znaleziono i poprawnie przetworzono 0 pasujących wspomnień.\n",
      "INFO: Nie znaleziono inspiracji w pamięci dla tego typu danych.\n",
      "\n",
      "================================================================================\n",
      "### ### FAZA 1: URUCHAMIANIE PLANOWANIA STRATEGICZNEGO (AutoGen) ### ###\n",
      "================================================================================\n",
      "\n",
      "\u001b[33mUserProxy\u001b[0m (to chat_manager):\n",
      "\n",
      "Oto podgląd danych:\n",
      "\n",
      "Kolumny:\n",
      "['Transaction_ID', 'User_ID', 'Transaction_Amount', 'Transaction_Type', 'Timestamp', 'Account_Balance', 'Device_Type', 'Location', 'Merchant_Category', 'IP_Address_Flag', 'Previous_Fraudulent_Activity', 'Daily_Transaction_Count', 'Avg_Transaction_Amount_7d', 'Failed_Transaction_Count_7d', 'Card_Type', 'Card_Age', 'Transaction_Distance', 'Authentication_Method', 'Risk_Score', 'Is_Weekend', 'Fraud_Label']\n",
      "\n",
      "Pierwsze 5 wierszy:\n",
      "  Transaction_ID    User_ID  Transaction_Amount Transaction_Type            Timestamp  Account_Balance Device_Type  Location Merchant_Category  IP_Address_Flag  Previous_Fraudulent_Activity  Daily_Transaction_Count  Avg_Transaction_Amount_7d  Failed_Transaction_Count_7d   Card_Type  Card_Age  Transaction_Distance Authentication_Method  Risk_Score  Is_Weekend  Fraud_Label\n",
      "0      TXN_33553  USER_1834               39.79              POS  2023-08-14 19:30:00         93213.17      Laptop    Sydney            Travel                0                             0                        7                     437.63                            3        Amex        65                883.17             Biometric      0.8494           0            0\n",
      "1       TXN_9427  USER_7875                1.19    Bank Transfer  2023-06-07 04:01:00         75725.25      Mobile  New York          Clothing                0                             0                       13                     478.76                            4  Mastercard       186               2203.36              Password      0.0959           0            1\n",
      "2        TXN_199  USER_2734               28.96           Online  2023-06-20 15:25:00          1588.96      Tablet    Mumbai       Restaurants                0                             0                       14                      50.01                            4        Visa       226               1909.29             Biometric      0.8400           0            1\n",
      "3      TXN_12447  USER_2617              254.32   ATM Withdrawal  2023-12-07 00:31:00         76807.20      Tablet  New York          Clothing                0                             0                        8                     182.48                            4        Visa        76               1311.86                   OTP      0.7935           0            1\n",
      "4      TXN_39489  USER_2014               31.28              POS  2023-11-11 23:44:00         92354.66      Mobile    Mumbai       Electronics                0                             1                       14                     328.69                            4  Mastercard       140                966.98              Password      0.3819           1            1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: TriggerAgent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mTriggerAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "Dane są tabularyczne. Przekazuję do PlannerAgent w celu stworzenia planu analizy.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: PlannerAgent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mPlannerAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "Jako Architekt Planu, po zapoznaniu się ze strukturą i podglądem dostarczonych danych, przedstawiam szczegółowy plan ich przygotowania. Celem jest przekształcenie surowych danych w czysty, spójny i zoptymalizowany zbiór, gotowy do przeprowadzenia analizy eksploracyjnej oraz budowy modeli predykcyjnych (np. wykrywania oszustw).\n",
      "\n",
      "Oto proponowany, numerowany plan działania:\n",
      "\n",
      "***\n",
      "\n",
      "### **Plan Czyszczenia i Przygotowania Danych Transakcyjnych**\n",
      "\n",
      "**Cel:** Przygotowanie danych do analizy i modelowania klasyfikacyjnego w celu identyfikacji transakcji typu \"fraud\".\n",
      "\n",
      "**1. Wstępna Analiza i Weryfikacja Integralności Danych**\n",
      "*   **1.1. Wczytanie danych:** Załadowanie zbioru danych do struktury DataFrame.\n",
      "*   **1.2. Podstawowe informacje:** Użycie funkcji `.info()` w celu weryfikacji wstępnych typów danych i liczby niepustych wartości w każdej kolumnie. Pozwoli to na szybkie zidentyfikowanie kolumn z potencjalnymi brakami.\n",
      "*   **1.3. Statystyki opisowe:** Wygenerowanie statystyk opisowych za pomocą `.describe(include='all')` dla kolumn numerycznych (średnia, mediana, odchylenie standardowe, kwantyle) i kategorycznych (liczba unikalnych wartości, najczęstsza wartość).\n",
      "*   **1.4. Weryfikacja duplikatów:** Sprawdzenie, czy w kolumnie `Transaction_ID` istnieją zduplikowane wartości. Każda transakcja powinna być unikalna. W przypadku znalezienia duplikatów, należy je usunąć.\n",
      "\n",
      "**2. Obsługa Brakujących Wartości (Missing Values)**\n",
      "*   **2.1. Identyfikacja:** Dokładne zliczenie brakujących wartości (`NaN`, `None`) w każdej kolumnie.\n",
      "*   **2.2. Strategia imputacji:**\n",
      "    *   **Kolumny numeryczne (`Transaction_Amount`, `Account_Balance`, `Avg_Transaction_Amount_7d`, `Transaction_Distance`, `Risk_Score`, `Card_Age`):** W przypadku niewielkiej liczby braków (<5%), zastosowanie imputacji **medianą**. Mediana jest bardziej odporna na wartości odstające niż średnia, co jest kluczowe w danych finansowych.\n",
      "    *   **Kolumny kategoryczne (`Location`, `Merchant_Category`, `Device_Type`, `Card_Type`, `Authentication_Method`):** W przypadku braków, zastosowanie imputacji **modą** (najczęściej występującą wartością) lub utworzenie nowej kategorii `'Unknown'`.\n",
      "    *   **Kolumny o wysokim odsetku braków:** Jeśli jakakolwiek kolumna posiada > 60% brakujących wartości, zostanie ona usunięta, ponieważ jej imputacja mogłaby wprowadzić zbyt dużo szumu do modelu.\n",
      "\n",
      "**3. Weryfikacja i Konwersja Typów Danych**\n",
      "*   **3.1. Konwersja kolumny `Timestamp`:** Zmiana typu danych kolumny `Timestamp` z `object` (string) na `datetime`. Jest to kluczowe dla dalszej inżynierii cech.\n",
      "*   **3.2. Weryfikacja kolumn numerycznych:** Upewnienie się, że wszystkie kolumny, które powinny być numeryczne (np. `Transaction_Amount`, `Account_Balance`), mają odpowiedni typ (`float` lub `int`), a nie `object`.\n",
      "*   **3.3. Weryfikacja kolumn binarnych:** Sprawdzenie, czy kolumny binarne (`IP_Address_Flag`, `Previous_Fraudulent_Activity`, `Is_Weekend`, `Fraud_Label`) mają typ `int` (0/1).\n",
      "\n",
      "**4. Inżynieria Cech (Feature Engineering)**\n",
      "*   **4.1. Cechy oparte na dacie i czasie (z `Timestamp`):**\n",
      "    *   `Godzina_Transakcji`: Ekstrakcja godziny z `Timestamp`. Wzorce oszustw często zależą od pory dnia (np. transakcje w środku nocy).\n",
      "    *   `Dzien_Tygodnia`: Ekstrakcja dnia tygodnia (0-6). Może to pomóc w identyfikacji wzorców weekendowych vs. w dni robocze (chociaż istnieje już flaga `Is_Weekend`, ta cecha da więcej szczegółów).\n",
      "    *   `Dzien_Miesiaca`: Ekstrakcja dnia miesiąca. Może być skorelowane z cyklami wypłat.\n",
      "*   **4.2. Cechy relacyjne:**\n",
      "    *   `Stosunek_Kwoty_Do_Salda`: Obliczenie stosunku `Transaction_Amount` do `Account_Balance`. Wysoki stosunek może być sygnałem ostrzegawczym. Należy obsłużyć dzielenie przez zero, jeśli `Account_Balance` może wynosić 0.\n",
      "    *   `Odchylenie_Od_Sredniej_7d`: Obliczenie różnicy `Transaction_Amount - Avg_Transaction_Amount_7d`. Wskaże, jak bardzo bieżąca transakcja odbiega od niedawnego zachowania użytkownika.\n",
      "*   **4.3. Kodowanie zmiennych kategorycznych:**\n",
      "    *   Zastosowanie **One-Hot Encoding** dla zmiennych kategorycznych o niskiej kardynalności (małej liczbie unikalnych wartości), takich jak `Transaction_Type`, `Device_Type`, `Card_Type`, `Authentication_Method`.\n",
      "    *   Dla zmiennych o potencjalnie wysokiej kardynalności (`Location`, `Merchant_Category`), początkowo również zostanie zastosowany One-Hot Encoding, ale z zastrzeżeniem, że w przypadku zbyt dużej liczby nowych kolumn, rozważone zostanie grupowanie rzadszych kategorii lub użycie technik takich jak Target Encoding.\n",
      "\n",
      "**5. Wykrywanie i Obsługa Wartości Odstających (Outliers)**\n",
      "*   **5.1. Identyfikacja:** Analiza dystrybucji kluczowych zmiennych numerycznych (`Transaction_Amount`, `Account_Balance`, `Transaction_Distance`) za pomocą histogramów i wykresów pudełkowych.\n",
      "*   **5.2. Strategia obsługi:** Zastosowanie metody **kwantyli (capping/winsorization)**. Wartości poniżej 1. percentyla i powyżej 99. percentyla zostaną \"spłaszczone\" do wartości granicznych tych percentyli. Jest to bezpieczniejsza metoda niż usuwanie, ponieważ wartości odstające w danych o oszustwach mogą być właśnie tymi, których szukamy. Ta technika ograniczy ich wpływ na skalowanie i model, nie tracąc przy tym informacji o ich wystąpieniu.\n",
      "\n",
      "**6. Skalowanie Danych**\n",
      "*   **6.1. Uzasadnienie:** Wiele algorytmów uczenia maszynowego (np. Regresja Logistyczna, SVM, Sieci Neuronowe) jest wrażliwych na skalę cech. Skalowanie zapewnia, że wszystkie cechy mają podobny zakres, co poprawia zbieżność i wydajność modeli.\n",
      "*   **6.2. Metoda:** Zastosowanie **`StandardScaler`** (standaryzacja Z-score) na wszystkich cechach numerycznych (po obsłudze outlierów i inżynierii cech). Standaryzacja przekształci dane tak, aby miały średnią równą 0 i odchylenie standardowe równe 1.\n",
      "\n",
      "**7. Finalizacja Zbioru Danych**\n",
      "*   **7.1. Usunięcie zbędnych kolumn:** Usunięcie oryginalnych kolumn, które zostały przetworzone lub nie będą używane w modelu, tj. `Transaction_ID`, `User_ID` (chyba że będzie używany do grupowania w bardziej zaawansowanych modelach), `Timestamp` (po ekstrakcji cech).\n",
      "*   **7.2. Ostateczna weryfikacja:** Sprawdzenie finalnego kształtu zbioru danych, typów danych i braku wartości `NaN`.\n",
      "*   **7.3. Podział na zbiór treningowy i testowy:** Podział danych na zbiór treningowy i testowy (np. w proporcji 80/20) z zachowaniem proporcji klasy docelowej (`Fraud_Label`) za pomocą stratyfikacji.\n",
      "\n",
      "***\n",
      "\n",
      "Powyższy plan jest kompleksowy i stanowi solidną podstawę do przygotowania danych. Jestem gotowy na ewentualne uwagi i sugestie od CriticAgenta.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: CriticAgent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mCriticAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "Analizuję przedstawiony plan.\n",
      "\n",
      "Plan jest logiczny i kompleksowy, jednak w kilku miejscach narusza kluczowe zasady prostoty i modułowości, co może utrudnić weryfikację i debugowanie. Wprowadzam następujące uwagi w celu jego poprawy.\n",
      "\n",
      "**1. Zbyt złożona Inżynieria Cech (Naruszenie zasady \"Jedna zmiana na raz\")**\n",
      "\n",
      "*   **Problem:** Kroki 4.1 i 4.2 proponują stworzenie wielu nowych, niezależnych cech w ramach jednego podpunktu. Krok 4.1 tworzy trzy cechy czasowe, a 4.2 dwie cechy relacyjne. Takie podejście utrudnia śledzenie zmian i debugowanie w przypadku, gdyby jedna z transformacji zawiodła.\n",
      "*   **Sugestia:** Rozbij te kroki na mniejsze, atomowe operacje. Każda nowa cecha powinna być tworzona w osobnym, dedykowanym kroku.\n",
      "    *   Zamiast jednego kroku 4.1, stwórz trzy osobne: \"4.1. Stwórz cechę `Godzina_Transakcji`\", \"4.2. Stwórz cechę `Dzien_Tygodnia`\", itd.\n",
      "    *   Podobnie, rozbij krok 4.2 na dwa osobne kroki dla `Stosunek_Kwoty_Do_Salda` i `Odchylenie_Od_Sredniej_7d`. Umożliwi to np. dedykowaną obsługę błędu dzielenia przez zero przy tworzeniu pierwszej z nich.\n",
      "\n",
      "**2. Zbyt agresywne Kodowanie Zmiennych Kategorycznych (Naruszenie zasady \"Jedna zmiana na raz\")**\n",
      "\n",
      "*   **Problem:** Krok 4.3, dotyczący kodowania zmiennych kategorycznych, jest zbyt szeroki. Zastosowanie One-Hot Encoding do wszystkich wymienionych zmiennych naraz, zwłaszcza tych o potencjalnie wysokiej kardynalności (`Location`, `Merchant_Category`), jest ryzykowne. Może to doprowadzić do niekontrolowanej \"eksplozji\" liczby kolumn i utrudnić interpretację.\n",
      "*   **Sugestia:** Podziel ten proces na co najmniej dwa etapy.\n",
      "    *   **Etap 1:** Zastosuj One-Hot Encoding tylko do zmiennych o gwarantowanej niskiej liczbie unikalnych wartości, np. `Transaction_Type`, `Device_Type`, `Card_Type`, `Authentication_Method`.\n",
      "    *   **Etap 2 (osobny krok):** Przeprowadź analizę kardynalności dla `Location` i `Merchant_Category`. Jeśli liczba unikalnych wartości jest wysoka (np. >15), zastosuj strategię redukcji, np. grupując najrzadsze kategorie w jedną wspólną kategorię `'Other'`, a dopiero potem zastosuj One-Hot Encoding. To musi być świadoma decyzja podjęta w osobnym kroku.\n",
      "\n",
      "**3. Grupowa obsługa wartości odstających (Naruszenie zasady \"Jedna zmiana na raz\")**\n",
      "\n",
      "*   **Problem:** Krok 5.2 sugeruje zastosowanie metody kwantyli (capping) do wielu kolumn jednocześnie. Chociaż metoda jest ta sama, jej wpływ na każdą zmienną może być różny. Grupowe przetwarzanie utrudnia weryfikację, czy operacja została poprawnie zastosowana do każdej z kolumn.\n",
      "*   **Sugestia:** Wykonaj operację cappingu w osobnych krokach dla każdej z kluczowych zmiennych (`Transaction_Amount`, `Account_Balance`, `Transaction_Distance`). Zapewni to pełną kontrolę i możliwość weryfikacji wyników dla każdej cechy z osobna.\n",
      "\n",
      "Proszę o zaktualizowanie planu zgodnie z powyższymi zaleceniami, aby był bardziej modułowy, odporny na błędy i łatwiejszy do wdrożenia. Odeślij poprawioną wersję do ponownej recenzji.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: PlannerAgent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mPlannerAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "Dziękuję za cenną recenzję. Zgadzam się, że wprowadzenie większej modułowości i atomowości operacji znacząco poprawi przejrzystość, kontrolę i łatwość debugowania procesu przygotowania danych.\n",
      "\n",
      "Poniżej przedstawiam **nową, kompletną wersję planu**, która uwzględnia wszystkie Twoje sugestie. Zmiany zostały wyraźnie oznaczone.\n",
      "\n",
      "***\n",
      "\n",
      "### **Plan Czyszczenia i Przygotowania Danych Transakcyjnych (Wersja 2)**\n",
      "\n",
      "**Cel:** Przygotowanie danych do analizy i modelowania klasyfikacyjnego w celu identyfikacji transakcji typu \"fraud\".\n",
      "\n",
      "**1. Wstępna Analiza i Weryfikacja Integralności Danych**\n",
      "*   **1.1. Wczytanie danych:** Załadowanie zbioru danych do struktury DataFrame.\n",
      "*   **1.2. Podstawowe informacje:** Użycie funkcji `.info()` w celu weryfikacji wstępnych typów danych i liczby niepustych wartości w każdej kolumnie.\n",
      "*   **1.3. Statystyki opisowe:** Wygenerowanie statystyk opisowych za pomocą `.describe(include='all')` dla kolumn numerycznych i kategorycznych.\n",
      "*   **1.4. Weryfikacja duplikatów:** Sprawdzenie, czy w kolumnie `Transaction_ID` istnieją zduplikowane wartości. W przypadku znalezienia duplikatów, należy je usunąć.\n",
      "\n",
      "**2. Obsługa Brakujących Wartości (Missing Values)**\n",
      "*   **2.1. Identyfikacja:** Dokładne zliczenie brakujących wartości (`NaN`, `None`) w każdej kolumnie.\n",
      "*   **2.2. Strategia imputacji:**\n",
      "    *   **Kolumny numeryczne (`Transaction_Amount`, `Account_Balance`, `Avg_Transaction_Amount_7d`, `Transaction_Distance`, `Risk_Score`, `Card_Age`):** W przypadku niewielkiej liczby braków (<5%), zastosowanie imputacji **medianą**.\n",
      "    *   **Kolumny kategoryczne (`Location`, `Merchant_Category`, `Device_Type`, `Card_Type`, `Authentication_Method`):** W przypadku braków, zastosowanie imputacji **modą** (najczęściej występującą wartością) lub utworzenie nowej kategorii `'Unknown'`.\n",
      "    *   **Kolumny o wysokim odsetku braków:** Jeśli jakakolwiek kolumna posiada > 60% brakujących wartości, zostanie ona usunięta.\n",
      "\n",
      "**3. Weryfikacja i Konwersja Typów Danych**\n",
      "*   **3.1. Konwersja kolumny `Timestamp`:** Zmiana typu danych kolumny `Timestamp` z `object` (string) na `datetime`.\n",
      "*   **3.2. Weryfikacja kolumn numerycznych:** Upewnienie się, że wszystkie kolumny, które powinny być numeryczne, mają odpowiedni typ (`float` lub `int`).\n",
      "*   **3.3. Weryfikacja kolumn binarnych:** Sprawdzenie, czy kolumny binarne (`IP_Address_Flag`, `Previous_Fraudulent_Activity`, `Is_Weekend`, `Fraud_Label`) mają typ `int` (0/1).\n",
      "\n",
      "**4. Inżynieria Cech (Feature Engineering)**\n",
      "*   **4.1. [ZMIANA - Krok atomowy] Stworzenie cechy `Godzina_Transakcji`:** Ekstrakcja godziny (0-23) z kolumny `Timestamp`.\n",
      "*   **4.2. [ZMIANA - Krok atomowy] Stworzenie cechy `Dzien_Tygodnia`:** Ekstrakcja dnia tygodnia (0-6) z kolumny `Timestamp`.\n",
      "*   **4.3. [ZMIANA - Krok atomowy] Stworzenie cechy `Dzien_Miesiaca`:** Ekstrakcja dnia miesiąca (1-31) z kolumny `Timestamp`.\n",
      "*   **4.4. [ZMIANA - Krok atomowy] Stworzenie cechy `Stosunek_Kwoty_Do_Salda`:** Obliczenie stosunku `Transaction_Amount` do `Account_Balance`. W kroku tym zostanie zaimplementowana obsługa przypadków, gdy `Account_Balance` wynosi zero (np. przez zastąpienie wyniku wartością 0 lub bardzo dużą liczbą, aby uniknąć błędu `DivisionByZeroError`).\n",
      "*   **4.5. [ZMIANA - Krok atomowy] Stworzenie cechy `Odchylenie_Od_Sredniej_7d`:** Obliczenie różnicy `Transaction_Amount - Avg_Transaction_Amount_7d`.\n",
      "*   **4.6. [ZMIANA - Podział kodowania] Kodowanie zmiennych kategorycznych o niskiej kardynalności:** Zastosowanie **One-Hot Encoding** dla zmiennych: `Transaction_Type`, `Device_Type`, `Card_Type`, `Authentication_Method`.\n",
      "*   **4.7. [ZMIANA - Podział kodowania] Analiza i kodowanie zmiennych kategorycznych o wysokiej kardynalności:**\n",
      "    *   **4.7.1. Analiza:** Zliczenie unikalnych wartości w kolumnach `Location` i `Merchant_Category`.\n",
      "    *   **4.7.2. Redukcja kardynalności:** Jeśli liczba unikalnych wartości w którejkolwiek z tych kolumn przekracza próg (np. 15), wszystkie kategorie występujące rzadziej niż w 1% przypadków zostaną zgrupowane w jedną, nową kategorię `'Other'`.\n",
      "    *   **4.7.3. Kodowanie:** Zastosowanie **One-Hot Encoding** na przetworzonych kolumnach `Location` i `Merchant_Category`.\n",
      "\n",
      "**5. Wykrywanie i Obsługa Wartości Odstających (Outliers)**\n",
      "*   **5.1. Identyfikacja:** Analiza dystrybucji kluczowych zmiennych numerycznych za pomocą histogramów i wykresów pudełkowych w celu wizualnego potwierdzenia obecności wartości odstających.\n",
      "*   **5.2. [ZMIANA - Krok atomowy] Obsługa wartości odstających w `Transaction_Amount`:** Zastosowanie metody kwantyli (capping). Wartości poniżej 1. percentyla i powyżej 99. percentyla zostaną zastąpione wartościami granicznymi tych percentyli.\n",
      "*   **5.3. [ZMIANA - Krok atomowy] Obsługa wartości odstających w `Account_Balance`:** Zastosowanie metody kwantyli (capping) z progami 1. i 99. percentyla.\n",
      "*   **5.4. [ZMIANA - Krok atomowy] Obsługa wartości odstających w `Transaction_Distance`:** Zastosowanie metody kwantyli (capping) z progami 1. i 99. percentyla.\n",
      "\n",
      "**6. Skalowanie Danych**\n",
      "*   **6.1. Uzasadnienie:** Wiele algorytmów uczenia maszynowego jest wrażliwych na skalę cech. Skalowanie zapewnia, że wszystkie cechy mają podobny zakres, co poprawia zbieżność i wydajność modeli.\n",
      "*   **6.2. Metoda:** Zastosowanie **`StandardScaler`** (standaryzacja Z-score) na wszystkich cechach numerycznych (po obsłudze outlierów i inżynierii cech).\n",
      "\n",
      "**7. Finalizacja Zbioru Danych**\n",
      "*   **7.1. Usunięcie zbędnych kolumn:** Usunięcie oryginalnych kolumn, które zostały przetworzone lub nie będą używane w modelu, tj. `Transaction_ID`, `User_ID`, `Timestamp` oraz oryginalne kolumny kategoryczne po ich zakodowaniu.\n",
      "*   **7.2. Ostateczna weryfikacja:** Sprawdzenie finalnego kształtu zbioru danych, typów danych i braku wartości `NaN`.\n",
      "*   **7.3. Podział na zbiór treningowy i testowy:** Podział danych na zbiór treningowy i testowy (np. w proporcji 80/20) z zachowaniem proporcji klasy docelowej (`Fraud_Label`) za pomocą stratyfikacji.\n",
      "\n",
      "***\n",
      "\n",
      "Przesyłam zaktualizowany, bardziej szczegółowy i modułowy plan do ponownej recenzji.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: CriticAgent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mCriticAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "OSTATECZNY PLAN:\n",
      "### **Plan Czyszczenia i Przygotowania Danych Transakcyjnych (Wersja 2)**\n",
      "\n",
      "**Cel:** Przygotowanie danych do analizy i modelowania klasyfikacyjnego w celu identyfikacji transakcji typu \"fraud\".\n",
      "\n",
      "**1. Wstępna Analiza i Weryfikacja Integralności Danych**\n",
      "*   **1.1. Wczytanie danych:** Załadowanie zbioru danych do struktury DataFrame.\n",
      "*   **1.2. Podstawowe informacje:** Użycie funkcji `.info()` w celu weryfikacji wstępnych typów danych i liczby niepustych wartości w każdej kolumnie.\n",
      "*   **1.3. Statystyki opisowe:** Wygenerowanie statystyk opisowych za pomocą `.describe(include='all')` dla kolumn numerycznych i kategorycznych.\n",
      "*   **1.4. Weryfikacja duplikatów:** Sprawdzenie, czy w kolumnie `Transaction_ID` istnieją zduplikowane wartości. W przypadku znalezienia duplikatów, należy je usunąć.\n",
      "\n",
      "**2. Obsługa Brakujących Wartości (Missing Values)**\n",
      "*   **2.1. Identyfikacja:** Dokładne zliczenie brakujących wartości (`NaN`, `None`) w każdej kolumnie.\n",
      "*   **2.2. Strategia imputacji:**\n",
      "    *   **Kolumny numeryczne (`Transaction_Amount`, `Account_Balance`, `Avg_Transaction_Amount_7d`, `Transaction_Distance`, `Risk_Score`, `Card_Age`):** W przypadku niewielkiej liczby braków (<5%), zastosowanie imputacji **medianą**.\n",
      "    *   **Kolumny kategoryczne (`Location`, `Merchant_Category`, `Device_Type`, `Card_Type`, `Authentication_Method`):** W przypadku braków, zastosowanie imputacji **modą** (najczęściej występującą wartością) lub utworzenie nowej kategorii `'Unknown'`.\n",
      "    *   **Kolumny o wysokim odsetku braków:** Jeśli jakakolwiek kolumna posiada > 60% brakujących wartości, zostanie ona usunięta.\n",
      "\n",
      "**3. Weryfikacja i Konwersja Typów Danych**\n",
      "*   **3.1. Konwersja kolumny `Timestamp`:** Zmiana typu danych kolumny `Timestamp` z `object` (string) na `datetime`.\n",
      "*   **3.2. Weryfikacja kolumn numerycznych:** Upewnienie się, że wszystkie kolumny, które powinny być numeryczne, mają odpowiedni typ (`float` lub `int`).\n",
      "*   **3.3. Weryfikacja kolumn binarnych:** Sprawdzenie, czy kolumny binarne (`IP_Address_Flag`, `Previous_Fraudulent_Activity`, `Is_Weekend`, `Fraud_Label`) mają typ `int` (0/1).\n",
      "\n",
      "**4. Inżynieria Cech (Feature Engineering)**\n",
      "*   **4.1. [ZMIANA - Krok atomowy] Stworzenie cechy `Godzina_Transakcji`:** Ekstrakcja godziny (0-23) z kolumny `Timestamp`.\n",
      "*   **4.2. [ZMIANA - Krok atomowy] Stworzenie cechy `Dzien_Tygodnia`:** Ekstrakcja dnia tygodnia (0-6) z kolumny `Timestamp`.\n",
      "*   **4.3. [ZMIANA - Krok atomowy] Stworzenie cechy `Dzien_Miesiaca`:** Ekstrakcja dnia miesiąca (1-31) z kolumny `Timestamp`.\n",
      "*   **4.4. [ZMIANA - Krok atomowy] Stworzenie cechy `Stosunek_Kwoty_Do_Salda`:** Obliczenie stosunku `Transaction_Amount` do `Account_Balance`. W kroku tym zostanie zaimplementowana obsługa przypadków, gdy `Account_Balance` wynosi zero (np. przez zastąpienie wyniku wartością 0 lub bardzo dużą liczbą, aby uniknąć błędu `DivisionByZeroError`).\n",
      "*   **4.5. [ZMIANA - Krok atomowy] Stworzenie cechy `Odchylenie_Od_Sredniej_7d`:** Obliczenie różnicy `Transaction_Amount - Avg_Transaction_Amount_7d`.\n",
      "*   **4.6. [ZMIANA - Podział kodowania] Kodowanie zmiennych kategorycznych o niskiej kardynalności:** Zastosowanie **One-Hot Encoding** dla zmiennych: `Transaction_Type`, `Device_Type`, `Card_Type`, `Authentication_Method`.\n",
      "*   **4.7. [ZMIANA - Podział kodowania] Analiza i kodowanie zmiennych kategorycznych o wysokiej kardynalności:**\n",
      "    *   **4.7.1. Analiza:** Zliczenie unikalnych wartości w kolumnach `Location` i `Merchant_Category`.\n",
      "    *   **4.7.2. Redukcja kardynalności:** Jeśli liczba unikalnych wartości w którejkolwiek z tych kolumn przekracza próg (np. 15), wszystkie kategorie występujące rzadziej niż w 1% przypadków zostaną zgrupowane w jedną, nową kategorię `'Other'`.\n",
      "    *   **4.7.3. Kodowanie:** Zastosowanie **One-Hot Encoding** na przetworzonych kolumnach `Location` i `Merchant_Category`.\n",
      "\n",
      "**5. Wykrywanie i Obsługa Wartości Odstających (Outliers)**\n",
      "*   **5.1. Identyfikacja:** Analiza dystrybucji kluczowych zmiennych numerycznych za pomocą histogramów i wykresów pudełkowych w celu wizualnego potwierdzenia obecności wartości odstających.\n",
      "*   **5.2. [ZMIANA - Krok atomowy] Obsługa wartości odstających w `Transaction_Amount`:** Zastosowanie metody kwantyli (capping). Wartości poniżej 1. percentyla i powyżej 99. percentyla zostaną zastąpione wartościami granicznymi tych percentyli.\n",
      "*   **5.3. [ZMIANA - Krok atomowy] Obsługa wartości odstających w `Account_Balance`:** Zastosowanie metody kwantyli (capping) z progami 1. i 99. percentyla.\n",
      "*   **5.4. [ZMIANA - Krok atomowy] Obsługa wartości odstających w `Transaction_Distance`:** Zastosowanie metody kwantyli (capping) z progami 1. i 99. percentyla.\n",
      "\n",
      "**6. Skalowanie Danych**\n",
      "*   **6.1. Uzasadnienie:** Wiele algorytmów uczenia maszynowego jest wrażliwych na skalę cech. Skalowanie zapewnia, że wszystkie cechy mają podobny zakres, co poprawia zbieżność i wydajność modeli.\n",
      "*   **6.2. Metoda:** Zastosowanie **`StandardScaler`** (standaryzacja Z-score) na wszystkich cechach numerycznych (po obsłudze outlierów i inżynierii cech).\n",
      "\n",
      "**7. Finalizacja Zbioru Danych**\n",
      "*   **7.1. Usunięcie zbędnych kolumn:** Usunięcie oryginalnych kolumn, które zostały przetworzone lub nie będą używane w modelu, tj. `Transaction_ID`, `User_ID`, `Timestamp` oraz oryginalne kolumny kategoryczne po ich zakodowaniu.\n",
      "*   **7.2. Ostateczna weryfikacja:** Sprawdzenie finalnego kształtu zbioru danych, typów danych i braku wartości `NaN`.\n",
      "*   **7.3. Podział na zbiór treningowy i testowy:** Podział danych na zbiór treningowy i testowy (np. w proporcji 80/20) z zachowaniem proporcji klasy docelowej (`Fraud_Label`) za pomocą stratyfikacji.\n",
      "\n",
      "PLAN_AKCEPTOWANY_PRZEJSCIE_DO_IMPLEMENTACJI\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (853d1c33-f96e-48f1-a312-6bd5798bd7b6): No next speaker selected\u001b[0m\n",
      "Faza planowania zakończona. Ostateczny plan został zaakceptowany.\n",
      "INFO: Próba zapisu pełnego logu rozmowy do pliku: reports/autogen_planning_conversation.log\n",
      "✅ SUKCES: Log rozmowy został pomyślnie zapisany.\n",
      "\n",
      "================================================================================\n",
      "### ### FAZA 2: URUCHAMIANIE WYKONANIA PLANU (LangGraph) ### ###\n",
      "================================================================================\n",
      "\n",
      "--- WĘZEŁ: ANALIZATOR SCHEMATU DANYCH ---\n",
      "DEBUG: Próbuję odczytać plik ze ścieżki: gs://super_model/data/structural_data/synthetic_fraud_dataset.csv\n",
      "INFO: Wygenerowano sygnaturę danych: ae1568fe7dae11d4bacd0c21ed718503\n",
      "--- Krok: 'schema_reader' ---\n",
      "{\n",
      "  \"available_columns\": [\n",
      "    \"Transaction_ID\",\n",
      "    \"User_ID\",\n",
      "    \"Transaction_Amount\",\n",
      "    \"Transaction_Type\",\n",
      "    \"Timestamp\",\n",
      "    \"Account_Balance\",\n",
      "    \"Device_Type\",\n",
      "    \"Location\",\n",
      "    \"Merchant_Category\",\n",
      "    \"IP_Address_Flag\",\n",
      "    \"Previous_Fraudulent_Activity\",\n",
      "    \"Daily_Transaction_Count\",\n",
      "    \"Avg_Transaction_Amount_7d\",\n",
      "    \"Failed_Transaction_Count_7d\",\n",
      "    \"Card_Type\",\n",
      "    \"Card_Age\",\n",
      "    \"Transaction_Distance\",\n",
      "    \"Authentication_Method\",\n",
      "    \"Risk_Score\",\n",
      "    \"Is_Weekend\",\n",
      "    \"Fraud_Label\"\n",
      "  ],\n",
      "  \"dataset_signature\": \"ae1568fe7dae11d4bacd0c21ed718503\"\n",
      "}\n",
      "--------------------\n",
      "\n",
      "---  WĘZEŁ: GENERATOR KODU ---\n",
      "\n",
      "Agent-Analityk wygenerował następujący kod:\n",
      "--------------------------------------------------\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "\n",
      "def process_data(input_path: str, output_path: str):\n",
      "    # 1. Wstępna Analiza i Weryfikacja Integralności Danych\n",
      "    # 1.1. Wczytanie danych\n",
      "    df = pd.read_csv(input_path)\n",
      "    \n",
      "    # 1.2. Podstawowe informacje\n",
      "    print(\"Podstawowe informacje o danych:\")\n",
      "    print(df.info())\n",
      "    \n",
      "    # 1.3. Statystyki opisowe\n",
      "    print(\"\\nStatystyki opisowe:\")\n",
      "    print(df.describe(include='all'))\n",
      "    \n",
      "    # 1.4. Weryfikacja duplikatów\n",
      "    duplicates = df['Transaction_ID'].duplicated().sum()\n",
      "    print(f\"\\nLiczba duplikatów w Transaction_ID: {duplicates}\")\n",
      "    if duplicates > 0:\n",
      "        df = df.drop_duplicates(subset=['Transaction_ID'])\n",
      "        print(f\"Usunięto {duplicates} duplikatów\")\n",
      "    \n",
      "    # 2. Obsługa Brakujących Wartości\n",
      "    # 2.1. Identyfikacja\n",
      "    missing_values = df.isnull().sum()\n",
      "    print(\"\\nBrakujące wartości:\")\n",
      "    print(missing_values[missing_values > 0])\n",
      "    \n",
      "    # 2.2. Strategia imputacji\n",
      "    # Usunięcie kolumn z > 60% braków\n",
      "    threshold = 0.6 * len(df)\n",
      "    columns_to_drop = missing_values[missing_values > threshold].index.tolist()\n",
      "    if columns_to_drop:\n",
      "        df = df.drop(columns=columns_to_drop)\n",
      "        print(f\"Usunięto kolumny z > 60% braków: {columns_to_drop}\")\n",
      "    \n",
      "    # Kolumny numeryczne - imputacja medianą\n",
      "    numeric_columns = ['Transaction_Amount', 'Account_Balance', 'Avg_Transaction_Amount_7d', \n",
      "                      'Transaction_Distance', 'Risk_Score', 'Card_Age', 'Daily_Transaction_Count',\n",
      "                      'Failed_Transaction_Count_7d']\n",
      "    for col in numeric_columns:\n",
      "        if col in df.columns and df[col].isnull().sum() > 0:\n",
      "            median_value = df[col].median()\n",
      "            df[col].fillna(median_value, inplace=True)\n",
      "    \n",
      "    # Kolumny kategoryczne - imputacja modą lub 'Unknown'\n",
      "    categorical_columns = ['Location', 'Merchant_Category', 'Device_Type', 'Card_Type', \n",
      "                          'Authentication_Method', 'Transaction_Type']\n",
      "    for col in categorical_columns:\n",
      "        if col in df.columns and df[col].isnull().sum() > 0:\n",
      "            mode_value = df[col].mode()\n",
      "            if len(mode_value) > 0:\n",
      "                df[col].fillna(mode_value[0], inplace=True)\n",
      "            else:\n",
      "                df[col].fillna('Unknown', inplace=True)\n",
      "    \n",
      "    # 3. Weryfikacja i Konwersja Typów Danych\n",
      "    # 3.1. Konwersja kolumny Timestamp\n",
      "    if 'Timestamp' in df.columns:\n",
      "        df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
      "    \n",
      "    # 3.2. Weryfikacja kolumn numerycznych\n",
      "    for col in numeric_columns:\n",
      "        if col in df.columns:\n",
      "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
      "    \n",
      "    # 3.3. Weryfikacja kolumn binarnych\n",
      "    binary_columns = ['IP_Address_Flag', 'Previous_Fraudulent_Activity', 'Is_Weekend', 'Fraud_Label']\n",
      "    for col in binary_columns:\n",
      "        if col in df.columns:\n",
      "            df[col] = df[col].astype(int)\n",
      "    \n",
      "    # 4. Inżynieria Cech\n",
      "    # 4.1. Stworzenie cechy Godzina_Transakcji\n",
      "    if 'Timestamp' in df.columns:\n",
      "        df['Godzina_Transakcji'] = df['Timestamp'].dt.hour\n",
      "    \n",
      "    # 4.2. Stworzenie cechy Dzien_Tygodnia\n",
      "    if 'Timestamp' in df.columns:\n",
      "        df['Dzien_Tygodnia'] = df['Timestamp'].dt.dayofweek\n",
      "    \n",
      "    # 4.3. Stworzenie cechy Dzien_Miesiaca\n",
      "    if 'Timestamp' in df.columns:\n",
      "        df['Dzien_Miesiaca'] = df['Timestamp'].dt.day\n",
      "    \n",
      "    # 4.4. Stworzenie cechy Stosunek_Kwoty_Do_Salda\n",
      "    if 'Transaction_Amount' in df.columns and 'Account_Balance' in df.columns:\n",
      "        df['Stosunek_Kwoty_Do_Salda'] = np.where(\n",
      "            df['Account_Balance'] == 0, \n",
      "            0, \n",
      "            df['Transaction_Amount'] / df['Account_Balance']\n",
      "        )\n",
      "    \n",
      "    # 4.5. Stworzenie cechy Odchylenie_Od_Sredniej_7d\n",
      "    if 'Transaction_Amount' in df.columns and 'Avg_Transaction_Amount_7d' in df.columns:\n",
      "        df['Odchylenie_Od_Sredniej_7d'] = df['Transaction_Amount'] - df['Avg_Transaction_Amount_7d']\n",
      "    \n",
      "    # 4.6. Kodowanie zmiennych kategorycznych o niskiej kardynalności\n",
      "    low_cardinality_cols = ['Transaction_Type', 'Device_Type', 'Card_Type', 'Authentication_Method']\n",
      "    for col in low_cardinality_cols:\n",
      "        if col in df.columns:\n",
      "            dummies = pd.get_dummies(df[col], prefix=col, drop_first=True)\n",
      "            df = pd.concat([df, dummies], axis=1)\n",
      "    \n",
      "    # 4.7. Analiza i kodowanie zmiennych kategorycznych o wysokiej kardynalności\n",
      "    high_cardinality_cols = ['Location', 'Merchant_Category']\n",
      "    for col in high_cardinality_cols:\n",
      "        if col in df.columns:\n",
      "            # 4.7.1. Analiza\n",
      "            unique_count = df[col].nunique()\n",
      "            print(f\"\\nLiczba unikalnych wartości w {col}: {unique_count}\")\n",
      "            \n",
      "            # 4.7.2. Redukcja kardynalności\n",
      "            if unique_count > 15:\n",
      "                value_counts = df[col].value_counts()\n",
      "                threshold_count = len(df) * 0.01  # 1% przypadków\n",
      "                rare_categories = value_counts[value_counts < threshold_count].index\n",
      "                df[col] = df[col].replace(rare_categories, 'Other')\n",
      "                print(f\"Zgrupowano {len(rare_categories)} rzadkich kategorii w 'Other' dla {col}\")\n",
      "            \n",
      "            # 4.7.3. Kodowanie\n",
      "            dummies = pd.get_dummies(df[col], prefix=col, drop_first=True)\n",
      "            df = pd.concat([df, dummies], axis=1)\n",
      "    \n",
      "    # 5. Wykrywanie i Obsługa Wartości Odstających\n",
      "    # 5.1. Identyfikacja (informacyjnie)\n",
      "    outlier_columns = ['Transaction_Amount', 'Account_Balance', 'Transaction_Distance']\n",
      "    \n",
      "    # 5.2. Obsługa wartości odstających w Transaction_Amount\n",
      "    if 'Transaction_Amount' in df.columns:\n",
      "        q1 = df['Transaction_Amount'].quantile(0.01)\n",
      "        q99 = df['Transaction_Amount'].quantile(0.99)\n",
      "        df['Transaction_Amount'] = df['Transaction_Amount'].clip(lower=q1, upper=q\n",
      "--------------------------------------------------\n",
      "--- Krok: 'code_generator' ---\n",
      "--- GENERATED_CODE ---\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "\n",
      "def process_data(input_path: str, output_path: str):\n",
      "    # 1. Wstępna Analiza i Weryfikacja Integralności Danych\n",
      "    # 1.1. Wczytanie danych\n",
      "    df = pd.read_csv(input_path)\n",
      "    \n",
      "    # 1.2. Podstawowe informacje\n",
      "    print(\"Podstawowe informacje o danych:\")\n",
      "    print(df.info())\n",
      "    \n",
      "    # 1.3. Statystyki opisowe\n",
      "    print(\"\\nStatystyki opisowe:\")\n",
      "    print(df.describe(include='all'))\n",
      "    \n",
      "    # 1.4. Weryfikacja duplikatów\n",
      "    duplicates = df['Transaction_ID'].duplicated().sum()\n",
      "    print(f\"\\nLiczba duplikatów w Transaction_ID: {duplicates}\")\n",
      "    if duplicates > 0:\n",
      "        df = df.drop_duplicates(subset=['Transaction_ID'])\n",
      "        print(f\"Usunięto {duplicates} duplikatów\")\n",
      "    \n",
      "    # 2. Obsługa Brakujących Wartości\n",
      "    # 2.1. Identyfikacja\n",
      "    missing_values = df.isnull().sum()\n",
      "    print(\"\\nBrakujące wartości:\")\n",
      "    print(missing_values[missing_values > 0])\n",
      "    \n",
      "    # 2.2. Strategia imputacji\n",
      "    # Usunięcie kolumn z > 60% braków\n",
      "    threshold = 0.6 * len(df)\n",
      "    columns_to_drop = missing_values[missing_values > threshold].index.tolist()\n",
      "    if columns_to_drop:\n",
      "        df = df.drop(columns=columns_to_drop)\n",
      "        print(f\"Usunięto kolumny z > 60% braków: {columns_to_drop}\")\n",
      "    \n",
      "    # Kolumny numeryczne - imputacja medianą\n",
      "    numeric_columns = ['Transaction_Amount', 'Account_Balance', 'Avg_Transaction_Amount_7d', \n",
      "                      'Transaction_Distance', 'Risk_Score', 'Card_Age', 'Daily_Transaction_Count',\n",
      "                      'Failed_Transaction_Count_7d']\n",
      "    for col in numeric_columns:\n",
      "        if col in df.columns and df[col].isnull().sum() > 0:\n",
      "            median_value = df[col].median()\n",
      "            df[col].fillna(median_value, inplace=True)\n",
      "    \n",
      "    # Kolumny kategoryczne - imputacja modą lub 'Unknown'\n",
      "    categorical_columns = ['Location', 'Merchant_Category', 'Device_Type', 'Card_Type', \n",
      "                          'Authentication_Method', 'Transaction_Type']\n",
      "    for col in categorical_columns:\n",
      "        if col in df.columns and df[col].isnull().sum() > 0:\n",
      "            mode_value = df[col].mode()\n",
      "            if len(mode_value) > 0:\n",
      "                df[col].fillna(mode_value[0], inplace=True)\n",
      "            else:\n",
      "                df[col].fillna('Unknown', inplace=True)\n",
      "    \n",
      "    # 3. Weryfikacja i Konwersja Typów Danych\n",
      "    # 3.1. Konwersja kolumny Timestamp\n",
      "    if 'Timestamp' in df.columns:\n",
      "        df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
      "    \n",
      "    # 3.2. Weryfikacja kolumn numerycznych\n",
      "    for col in numeric_columns:\n",
      "        if col in df.columns:\n",
      "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
      "    \n",
      "    # 3.3. Weryfikacja kolumn binarnych\n",
      "    binary_columns = ['IP_Address_Flag', 'Previous_Fraudulent_Activity', 'Is_Weekend', 'Fraud_Label']\n",
      "    for col in binary_columns:\n",
      "        if col in df.columns:\n",
      "            df[col] = df[col].astype(int)\n",
      "    \n",
      "    # 4. Inżynieria Cech\n",
      "    # 4.1. Stworzenie cechy Godzina_Transakcji\n",
      "    if 'Timestamp' in df.columns:\n",
      "        df['Godzina_Transakcji'] = df['Timestamp'].dt.hour\n",
      "    \n",
      "    # 4.2. Stworzenie cechy Dzien_Tygodnia\n",
      "    if 'Timestamp' in df.columns:\n",
      "        df['Dzien_Tygodnia'] = df['Timestamp'].dt.dayofweek\n",
      "    \n",
      "    # 4.3. Stworzenie cechy Dzien_Miesiaca\n",
      "    if 'Timestamp' in df.columns:\n",
      "        df['Dzien_Miesiaca'] = df['Timestamp'].dt.day\n",
      "    \n",
      "    # 4.4. Stworzenie cechy Stosunek_Kwoty_Do_Salda\n",
      "    if 'Transaction_Amount' in df.columns and 'Account_Balance' in df.columns:\n",
      "        df['Stosunek_Kwoty_Do_Salda'] = np.where(\n",
      "            df['Account_Balance'] == 0, \n",
      "            0, \n",
      "            df['Transaction_Amount'] / df['Account_Balance']\n",
      "        )\n",
      "    \n",
      "    # 4.5. Stworzenie cechy Odchylenie_Od_Sredniej_7d\n",
      "    if 'Transaction_Amount' in df.columns and 'Avg_Transaction_Amount_7d' in df.columns:\n",
      "        df['Odchylenie_Od_Sredniej_7d'] = df['Transaction_Amount'] - df['Avg_Transaction_Amount_7d']\n",
      "    \n",
      "    # 4.6. Kodowanie zmiennych kategorycznych o niskiej kardynalności\n",
      "    low_cardinality_cols = ['Transaction_Type', 'Device_Type', 'Card_Type', 'Authentication_Method']\n",
      "    for col in low_cardinality_cols:\n",
      "        if col in df.columns:\n",
      "            dummies = pd.get_dummies(df[col], prefix=col, drop_first=True)\n",
      "            df = pd.concat([df, dummies], axis=1)\n",
      "    \n",
      "    # 4.7. Analiza i kodowanie zmiennych kategorycznych o wysokiej kardynalności\n",
      "    high_cardinality_cols = ['Location', 'Merchant_Category']\n",
      "    for col in high_cardinality_cols:\n",
      "        if col in df.columns:\n",
      "            # 4.7.1. Analiza\n",
      "            unique_count = df[col].nunique()\n",
      "            print(f\"\\nLiczba unikalnych wartości w {col}: {unique_count}\")\n",
      "            \n",
      "            # 4.7.2. Redukcja kardynalności\n",
      "            if unique_count > 15:\n",
      "                value_counts = df[col].value_counts()\n",
      "                threshold_count = len(df) * 0.01  # 1% przypadków\n",
      "                rare_categories = value_counts[value_counts < threshold_count].index\n",
      "                df[col] = df[col].replace(rare_categories, 'Other')\n",
      "                print(f\"Zgrupowano {len(rare_categories)} rzadkich kategorii w 'Other' dla {col}\")\n",
      "            \n",
      "            # 4.7.3. Kodowanie\n",
      "            dummies = pd.get_dummies(df[col], prefix=col, drop_first=True)\n",
      "            df = pd.concat([df, dummies], axis=1)\n",
      "    \n",
      "    # 5. Wykrywanie i Obsługa Wartości Odstających\n",
      "    # 5.1. Identyfikacja (informacyjnie)\n",
      "    outlier_columns = ['Transaction_Amount', 'Account_Balance', 'Transaction_Distance']\n",
      "    \n",
      "    # 5.2. Obsługa wartości odstających w Transaction_Amount\n",
      "    if 'Transaction_Amount' in df.columns:\n",
      "        q1 = df['Transaction_Amount'].quantile(0.01)\n",
      "        q99 = df['Transaction_Amount'].quantile(0.99)\n",
      "        df['Transaction_Amount'] = df['Transaction_Amount'].clip(lower=q1, upper=q\n",
      "----------------------\n",
      "--------------------\n",
      "\n",
      "--- 🛡️ WĘZEŁ: STRAŻNIK ARCHITEKTURY 🛡️ ---\n",
      "  [WERDYKT] ❌ Kod łamie zasady architektury: Skrypt nie kończy się wymaganym wywołaniem `process_data(input_path, output_path)  # noqa: F821`.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'MAX_CORRECTION_ATTEMPTS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 165\u001b[39m\n\u001b[32m    162\u001b[39m langgraph_log = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    163\u001b[39m final_run_state = initial_state.copy()\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mapp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrecursion_limit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnode_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_update\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m__end__\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnode_name\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/agents_with_memory_py11/lib/python3.11/site-packages/langgraph/pregel/__init__.py:2534\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2532\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2533\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2534\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2535\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2536\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2537\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2538\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2539\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2540\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2541\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2542\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2543\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2544\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/agents_with_memory_py11/lib/python3.11/site-packages/langgraph/graph/branch.py:169\u001b[39m, in \u001b[36mBranch._route\u001b[39m\u001b[34m(self, input, config, reader, writer)\u001b[39m\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    168\u001b[39m     value = \u001b[38;5;28minput\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._finish(writer, \u001b[38;5;28minput\u001b[39m, result, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/agents_with_memory_py11/lib/python3.11/site-packages/langgraph/utils/runnable.py:370\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    368\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    369\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m370\u001b[39m         ret = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    372\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 69\u001b[39m, in \u001b[36mshould_continue_or_debug\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Sprawdza, czy w stanie jest błąd i decyduje o dalszej ścieżce.\"\"\"\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m state.get(\u001b[33m\"\u001b[39m\u001b[33merror_message\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m state.get(\u001b[33m\"\u001b[39m\u001b[33mcorrection_attempts\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m0\u001b[39m) >= \u001b[43mMAX_CORRECTION_ATTEMPTS\u001b[49m:\n\u001b[32m     70\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mrequest_human_help\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcall_debugger\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'MAX_CORRECTION_ATTEMPTS' is not defined",
      "During task with name 'architectural_validator' and id 'ba7d1664-7f7e-785c-4849-4673c551c031'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(\"reports\", exist_ok=True)\n",
    "    system_source_code = read_source_code(\"Agents_beta.ipynb\") # Pamiętaj o poprawnej nazwie pliku\n",
    "\n",
    "    # --- Inicjalizacja Pamięci i Uruchomienia ---\n",
    "    memory_client = MemoryBankClient(client=client, agent_engine=agent_engine)\n",
    "    run_id = str(uuid.uuid4())\n",
    "    \n",
    "    print(\"\\n--- ODPYTYWANIE PAMIĘCI O INSPIRACJE ---\")\n",
    "    inspiration_prompt = \"\"\n",
    "    dataset_signature = \"\"\n",
    "    try:\n",
    "        df_preview = pd.read_csv(INPUT_FILE_PATH, nrows=0)\n",
    "        dataset_signature = memory_client.create_dataset_signature(df_preview)\n",
    "        past_memories = memory_client.query_memory(\n",
    "            query_text=\"Najlepsze strategie i kluczowe wnioski dotyczące przetwarzania danych\",\n",
    "            scope={\"dataset_signature\": dataset_signature},\n",
    "            top_k=3\n",
    "        )\n",
    "        if past_memories:\n",
    "            inspirations = []\n",
    "            for mem in past_memories:\n",
    "                if mem.memory_type == MemoryType.SUCCESSFUL_PLAN and 'key_insight' in mem.content:\n",
    "                    inspirations.append(f\"SPRAWDZONY WNIOSEK Z PLANU: {mem.content['key_insight']}\")\n",
    "                elif mem.memory_type == MemoryType.SUCCESSFUL_FIX and 'key_takeaway' in mem.content:\n",
    "                    inspirations.append(f\"NAUCZKA Z NAPRAWIONEGO BŁĘDU: {mem.content['key_takeaway']}\")\n",
    "            if inspirations:\n",
    "                inspiration_prompt = \"--- INSPIRACJE Z POPRZEDNICH URUCHOMIEŃ ---\\n\" + \"\\n\".join(inspirations)\n",
    "                print(\"INFO: Pomyślnie pobrano inspiracje z pamięci.\")\n",
    "        else:\n",
    "            print(\"INFO: Nie znaleziono inspiracji w pamięci dla tego typu danych.\")\n",
    "    except Exception as e:\n",
    "        print(f\"OSTRZEŻENIE: Nie udało się pobrać inspiracji z pamięci: {e}\")\n",
    "\n",
    "    # --- Krok 1: Faza planowania AutoGen ---\n",
    "    final_plan, autogen_log = run_autogen_planning_phase(input_path=INPUT_FILE_PATH, inspiration_prompt=inspiration_prompt,\n",
    "                                                         trigger_agent=trigger_agent,planner_agent=planner_agent,critic_agent=critic_agent,manager_agent_config=main_agent_configuration)\n",
    "\n",
    "    # Zapis logu z planowania (zawsze)\n",
    "    save_autogen_conversation_log(log_content=autogen_log, file_path=\"reports/autogen_planning_conversation.log\")\n",
    "\n",
    "    # --- Krok 2: Faza wykonania LangGraph ---\n",
    "    if final_plan:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"### ### FAZA 2: URUCHAMIANIE WYKONANIA PLANU (LangGraph) ### ###\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        workflow = StateGraph(AgentWorkflowState)\n",
    "        \n",
    "        # ZMIANA: Dodajemy nowy węzeł commit_memory_node do listy\n",
    "        nodes = [\n",
    "            \"schema_reader\", \"code_generator\", \"architectural_validator\", \n",
    "            \"data_code_executor\", \"universal_debugger\", \"apply_code_fix\", \n",
    "            \"human_approval\", \"package_installer\", \"reporting_agent\", \n",
    "            \"report_executor\", \"human_escalation\", \"sync_report_code\",\n",
    "            \"commit_memory\" # NOWY WĘZEŁ\n",
    "        ]\n",
    "        for name in nodes: workflow.add_node(name, globals()[f\"{name}_node\"])\n",
    "\n",
    "        # --- Definicja Krawędzi Grafu ---\n",
    "        workflow.set_entry_point(\"schema_reader\")\n",
    "        workflow.add_edge(\"schema_reader\", \"code_generator\")\n",
    "        workflow.add_edge(\"code_generator\", \"architectural_validator\")\n",
    "\n",
    "        # Funkcja routująca, której możemy używać wielokrotnie\n",
    "        def should_continue_or_debug(state: AgentWorkflowState) -> str:\n",
    "            \"\"\"Sprawdza, czy w stanie jest błąd i decyduje o dalszej ścieżce.\"\"\"\n",
    "            if state.get(\"error_message\"):\n",
    "                if state.get(\"correction_attempts\", 0) >= MAX_CORRECTION_ATTEMPTS:\n",
    "                    return \"request_human_help\"\n",
    "                return \"call_debugger\"\n",
    "            # Jeśli nie ma błędu, kontynuuj normalną ścieżkę\n",
    "            return \"continue\"\n",
    "\n",
    "        # 1. KRAWĘDŹ WARUNKOWA po walidatorze architektury (KLUCZOWA ZMIANA)\n",
    "        workflow.add_conditional_edges(\n",
    "            \"architectural_validator\",\n",
    "            should_continue_or_debug,\n",
    "            {\n",
    "                \"call_debugger\": \"universal_debugger\",\n",
    "                \"request_human_help\": \"human_escalation\",\n",
    "                \"continue\": \"data_code_executor\" # Przejdź dalej tylko jeśli jest OK\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # 2. KRAWĘDŹ WARUNKOWA po wykonaniu kodu danych\n",
    "        workflow.add_conditional_edges(\n",
    "            \"data_code_executor\",\n",
    "            should_continue_or_debug,\n",
    "            {\n",
    "                \"call_debugger\": \"universal_debugger\",\n",
    "                \"request_human_help\": \"human_escalation\",\n",
    "                \"continue\": \"commit_memory\" # Jeśli sukces, idź do zapisu w pamięci, a NIE do END\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Ścieżka sukcesu i pozostałe krawędzie\n",
    "        workflow.add_edge(\"commit_memory\", \"reporting_agent\")\n",
    "        workflow.add_edge(\"reporting_agent\", \"report_executor\")\n",
    "\n",
    "        # Krawędź warunkowa po wykonaniu raportu\n",
    "        workflow.add_conditional_edges(\n",
    "            \"report_executor\",\n",
    "            should_continue_or_debug,\n",
    "            {\n",
    "                \"call_debugger\": \"universal_debugger\",\n",
    "                \"request_human_help\": \"human_escalation\",\n",
    "                \"continue\": END # Dopiero tutaj kończymy pracę po sukcesie\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Ścieżki naprawcze i eskalacji (bez zmian)\n",
    "        workflow.add_edge(\"human_escalation\", END)\n",
    "        workflow.add_edge(\"package_installer\", \"data_code_executor\") # Wracamy do wykonania po instalacji\n",
    "\n",
    "        def route_after_fix(state):\n",
    "            failing_node = state.get(\"failing_node\")\n",
    "            if failing_node == \"report_executor\":\n",
    "                return \"sync_report_code\"\n",
    "            # Po każdej innej naprawie wracamy do walidacji architektonicznej\n",
    "            return \"architectural_validator\"\n",
    "\n",
    "        workflow.add_edge(\"sync_report_code\", \"report_executor\")\n",
    "        workflow.add_conditional_edges(\"apply_code_fix\", route_after_fix)\n",
    "\n",
    "        def route_from_debugger(state):\n",
    "            if state.get(\"tool_choice\") == \"propose_code_fix\":\n",
    "                return \"apply_code_fix\"\n",
    "            if state.get(\"tool_choice\") == \"request_package_installation\":\n",
    "                return \"human_approval\"\n",
    "            return \"human_escalation\"\n",
    "\n",
    "        workflow.add_conditional_edges(\"universal_debugger\", route_from_debugger)\n",
    "        workflow.add_conditional_edges(\"human_approval\", lambda s: s.get(\"user_approval_status\"), {\n",
    "            \"APPROVED\": \"package_installer\",\n",
    "            \"REJECTED\": \"universal_debugger\"\n",
    "        })\n",
    "\n",
    "        \n",
    "        \n",
    "        app_config ={\"MAIN_AGENT\" : MAIN_AGENT, \"CODE_MODEL\": CODE_MODEL, \"CRITIC_MODEL\":CRITIC_MODEL}\n",
    "        \n",
    "        \n",
    "        app = workflow.compile()\n",
    "        \n",
    "        initial_state = {\n",
    "            \"config\":app_config,\n",
    "            \"plan\": final_plan, \n",
    "            \"input_path\": INPUT_FILE_PATH,\n",
    "            \"output_path\": \"reports/processed_data.csv\",\n",
    "            \"report_output_path\": \"reports/transformation_report.html\",\n",
    "            \"correction_attempts\": 0, \n",
    "            \"source_code\": system_source_code,\n",
    "            \"autogen_log\": autogen_log,\n",
    "            \"memory_client\": memory_client,\n",
    "            \"run_id\": run_id,\n",
    "            \"dataset_signature\": dataset_signature,\n",
    "            \"pending_fix_session\": None # ZMIANA: Dodanie nowego pola do stanu początkowego\n",
    "        }\n",
    "        \n",
    "        # --- Uruchomienie grafu z przechwytywaniem logów ---\n",
    "        langgraph_log = \"\"\n",
    "        final_run_state = initial_state.copy()\n",
    "        \n",
    "        for event in app.stream(initial_state, {\"recursion_limit\": 50}):\n",
    "            for node_name, state_update in event.items():\n",
    "                if \"__end__\" not in node_name:\n",
    "                    print(f\"--- Krok: '{node_name}' ---\")\n",
    "                    if state_update: # Zabezpieczenie przed błędem 'NoneType'\n",
    "                        printable_update = state_update.copy()\n",
    "                        for key in [\"generated_code\", \"corrected_code\", \"generated_report_code\", \"error_context_code\"]:\n",
    "                            if key in printable_update and printable_update[key]:\n",
    "                                print(f\"--- {key.upper()} ---\")\n",
    "                                print(printable_update[key])\n",
    "                                print(\"-\" * (len(key) + 8))\n",
    "                                del printable_update[key]\n",
    "                        if printable_update:\n",
    "                            print(json.dumps(printable_update, indent=2, default=str))\n",
    "                        \n",
    "                        log_line = f\"--- Krok: '{node_name}' ---\\n{json.dumps(state_update, indent=2, default=str)}\\n\"\n",
    "                        langgraph_log += log_line\n",
    "                        final_run_state.update(state_update)\n",
    "                    else:\n",
    "                        print(\"  [INFO] Węzeł zakończył pracę bez aktualizacji stanu.\")\n",
    "                    print(\"-\" * 20 + \"\\n\")\n",
    "\n",
    "        # Zapis logu z wykonania (po zakończeniu pętli)\n",
    "        save_langgraph_execution_log(log_content=langgraph_log, file_path=\"reports/langgraph_execution.log\")\n",
    "\n",
    "        # Uruchomienie audytora\n",
    "        final_run_state['langgraph_log'] = langgraph_log\n",
    "        meta_auditor_node(final_run_state)\n",
    "\n",
    "        print(\"\\n\\n--- ZAKOŃCZONO PRACĘ GRAFU I AUDYT ---\")\n",
    "    else:\n",
    "        print(\"Proces zakończony. Brak planu do wykonania.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c476ce17-60f1-4436-8754-d2c7210310c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "agents_with_memory_p11",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Agents with memory (Python 3.11)",
   "language": "python",
   "name": "agents_with_memory_p11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
