{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b251096d-f8c3-4dfb-ae10-5d33be45f50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import json\n",
    "import vertexai\n",
    "from vertexai import agent_engines\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, List, Callable, Dict, Optional, Union, Any\n",
    "# Importy z własnych modułów\n",
    "from config import PROJECT_ID, LOCATION, MEMORY_ENGINE_DISPLAY_NAME, INPUT_FILE_PATH,MAIN_AGENT,CRITIC_MODEL,CODE_MODEL, API_TYPE_GEMINI,API_TYPE_SONNET, ANTHROPIC_API_KEY,basic_config_agent\n",
    "from agents.state import AgentWorkflowState\n",
    "from agents.autogen_agents import TriggerAgent,PlannerAgent,CriticAgent\n",
    "from prompts import LangchainAgentsPrompts,AutoGenAgentsPrompts\n",
    "from agents.langgraph_nodes import * \n",
    "from agents.autogen_agent_utils import run_autogen_planning_phase\n",
    "from memory.memory_bank_client import MemoryBankClient\n",
    "from tools.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b380a6ac-127b-44dd-9e3b-e0721815cd25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "AGENT_ENGINE_NAME = \"\" # Zostanie wypełniona po pobraniu lub utworzeniu silnika\n",
    "\n",
    "# Inicjalizacja głównego klienta Vertex AI\n",
    "client = vertexai.Client(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ba1b166-e3c9-4d11-9a08-76336faaa064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_agent_engine(display_name: str) :\n",
    "    \"\"\"\n",
    "    Pobiera istniejący Agent Engine po nazwie wyświetlanej lub tworzy nowy, jeśli nie istnieje.\n",
    "    \"\"\"\n",
    "    # 1. Pobierz listę wszystkich istniejących silników w projekcie\n",
    "    all_engines = agent_engines.list()\n",
    "    \n",
    "    # 2. Sprawdź, czy któryś z nich ma pasującą nazwę\n",
    "    for engine in all_engines:\n",
    "        if engine.display_name == display_name:\n",
    "            print(f\"INFO: Znaleziono i połączono z istniejącym Agent Engine: '{display_name}'\")\n",
    "            return engine\n",
    "            \n",
    "    # 3. Jeśli pętla się zakończyła i nic nie znaleziono, stwórz nowy silnik\n",
    "    print(f\"INFO: Nie znaleziono Agent Engine o nazwie '{display_name}'. Tworzenie nowego...\")\n",
    "    try:\n",
    "        new_engine = agent_engines.create(\n",
    "            display_name=display_name\n",
    "        )\n",
    "        print(f\"INFO: Pomyślnie utworzono nowy Agent Engine.\")\n",
    "        return new_engine\n",
    "    except Exception as e:\n",
    "        print(f\"KRYTYCZNY BŁĄD: Nie można utworzyć Agent Engine. Sprawdź konfigurację i uprawnienia. Błąd: {e}\")\n",
    "        exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80731513-5d98-4048-89f8-359410538a59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Znaleziono i połączono z istniejącym Agent Engine: 'memory-gamma-way'\n",
      "projects/815755318672/locations/us-central1/reasoningEngines/3849548538518175744\n"
     ]
    }
   ],
   "source": [
    "agent_engine =get_or_create_agent_engine(MEMORY_ENGINE_DISPLAY_NAME)\n",
    "AGENT_ENGINE_NAME = agent_engine.resource_name\n",
    "print(AGENT_ENGINE_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6982c7b4-a6dd-476f-b361-d36c50174185",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Konfiguracja czatu grupowego ---\n",
    "main_agent_configuration={\"cache_seed\": 42,\"seed\": 42,\"temperature\": 0.0,\n",
    "                        \"config_list\": basic_config_agent(agent_name=MAIN_AGENT, api_type=API_TYPE_GEMINI, location=LOCATION, project_id=PROJECT_ID)}\n",
    "critic_agent_configuration ={\"cache_seed\": 42,\"seed\": 42,\"temperature\": 0.0,\n",
    "                        \"config_list\": basic_config_agent(api_key=ANTHROPIC_API_KEY,agent_name=CRITIC_MODEL, api_type=API_TYPE_SONNET)}\n",
    "trigger_prompt = str(AutoGenAgentsPrompts.Trigger_prompt())\n",
    "planner_prompt = str(AutoGenAgentsPrompts.Planner_prompt())\n",
    "critic_prompt = str(AutoGenAgentsPrompts.Critic_prompt())\n",
    "#---WYWOŁANIE AGENTÓW\n",
    "trigger_agent = TriggerAgent(llm_config=main_agent_configuration, prompt=trigger_prompt)\n",
    "planner_agent = PlannerAgent(llm_config=main_agent_configuration,prompt=planner_prompt)\n",
    "critic_agent = CriticAgent(llm_config=main_agent_configuration,prompt=critic_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffc80bb-b8e4-424c-b3b7-96dc37607fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9d12db3-dfe5-4c56-9494-8721eccacda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: MemoryBankClient gotowy do pracy z silnikiem: projects/815755318672/locations/us-central1/reasoningEngines/3849548538518175744\n",
      "\n",
      "--- ODPYTYWANIE PAMIĘCI O INSPIRACJE ---\n",
      "INFO: Odpytuję pamięć semantycznie z zapytaniem 'Najlepsze strategie i kluczowe wnioski dotyczące przetwarzania danych' w zakresie {'dataset_signature': 'ae1568fe7dae11d4bacd0c21ed718503'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/olga_zydziak/version_beta/multiagent_system/memory/memory_bank_client.py:95: ExperimentalWarning: The Vertex SDK GenAI agent engines module is experimental, and may change in future versions.\n",
      "  memories_iterator = self.client.agent_engines.retrieve_memories(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "udany plan: id='d9bd7bbf-d8e9-4418-8d1b-ffdfcb8ef4ac' run_id='30c84df2-164d-462e-8605-cc23d1c40a59' timestamp=datetime.datetime(2025, 8, 4, 9, 16, 20, 565870) memory_type=<MemoryType.SUCCESSFUL_PLAN: 'SUCCESSFUL_PLAN'> dataset_signature='ae1568fe7dae11d4bacd0c21ed718503' source_node='meta_auditor_node' content={'plan_summary': 'N/A - distillation failed'} metadata={'importance_score': 0.8}\n",
      "udany plan: id='afdf0d49-4d7e-4bd5-a83d-c9a0c6fcf180' run_id='9d0372a5-31ee-4086-aa8c-a61be76c14a4' timestamp=datetime.datetime(2025, 8, 3, 23, 42, 27, 979172) memory_type=<MemoryType.SUCCESSFUL_PLAN: 'SUCCESSFUL_PLAN'> dataset_signature='ae1568fe7dae11d4bacd0c21ed718503' source_node='meta_auditor_node' content={'plan_summary': 'N/A - distillation failed'} metadata={'importance_score': 0.8}\n",
      "udany plan: id='98d5815d-a4dd-41e7-82f4-31352aec9302' run_id='1d4eb978-fe14-47e3-bb65-97355dd8f1bb' timestamp=datetime.datetime(2025, 8, 4, 10, 57, 48, 385507) memory_type=<MemoryType.SUCCESSFUL_PLAN: 'SUCCESSFUL_PLAN'> dataset_signature='ae1568fe7dae11d4bacd0c21ed718503' source_node='meta_auditor_node' content={'plan_summary': 'N/A - distillation failed'} metadata={'importance_score': 0.8}\n",
      "INFO: Znaleziono i poprawnie przetworzono 3 pasujących wspomnień.\n",
      "--- DORADCA POLITYKI SYSTEMOWEJ: Sprawdzanie pamięci... ---\n",
      "INFO: Odpytuję pamięć semantycznie z zapytaniem 'Najważniejsze rekomendacje dotyczące ulepszenia promptów lub logiki systemu' w zakresie {'dataset_signature': 'ae1568fe7dae11d4bacd0c21ed718503'}\n",
      "udany plan: id='a241a6a3-ed23-4da5-a734-7c708f0e0f27' run_id='1d4eb978-fe14-47e3-bb65-97355dd8f1bb' timestamp=datetime.datetime(2025, 8, 4, 10, 57, 48, 658511) memory_type=<MemoryType.SUCCESSFUL_PLAN: 'SUCCESSFUL_PLAN'> dataset_signature='ae1568fe7dae11d4bacd0c21ed718503' source_node='meta_auditor_node' content={'plan_summary': 'N/A - distillation failed'} metadata={'importance_score': 0.8}\n",
      "udany plan: id='afdf0d49-4d7e-4bd5-a83d-c9a0c6fcf180' run_id='9d0372a5-31ee-4086-aa8c-a61be76c14a4' timestamp=datetime.datetime(2025, 8, 3, 23, 42, 27, 979172) memory_type=<MemoryType.SUCCESSFUL_PLAN: 'SUCCESSFUL_PLAN'> dataset_signature='ae1568fe7dae11d4bacd0c21ed718503' source_node='meta_auditor_node' content={'plan_summary': 'N/A - distillation failed'} metadata={'importance_score': 0.8}\n",
      "udany plan: id='d9bd7bbf-d8e9-4418-8d1b-ffdfcb8ef4ac' run_id='30c84df2-164d-462e-8605-cc23d1c40a59' timestamp=datetime.datetime(2025, 8, 4, 9, 16, 20, 565870) memory_type=<MemoryType.SUCCESSFUL_PLAN: 'SUCCESSFUL_PLAN'> dataset_signature='ae1568fe7dae11d4bacd0c21ed718503' source_node='meta_auditor_node' content={'plan_summary': 'N/A - distillation failed'} metadata={'importance_score': 0.8}\n",
      "INFO: Znaleziono i poprawnie przetworzono 3 pasujących wspomnień.\n",
      "  [INFO] Brak aktywnych polityk systemowych.\n",
      "\n",
      "================================================================================\n",
      "### ### FAZA 1: URUCHAMIANIE PLANOWANIA STRATEGICZNEGO (AutoGen) ### ###\n",
      "================================================================================\n",
      "\n",
      "\u001b[33mUserProxy\u001b[0m (to chat_manager):\n",
      "\n",
      "Oto podgląd danych:\n",
      "\n",
      "Kolumny:\n",
      "['Transaction_ID', 'User_ID', 'Transaction_Amount', 'Transaction_Type', 'Timestamp', 'Account_Balance', 'Device_Type', 'Location', 'Merchant_Category', 'IP_Address_Flag', 'Previous_Fraudulent_Activity', 'Daily_Transaction_Count', 'Avg_Transaction_Amount_7d', 'Failed_Transaction_Count_7d', 'Card_Type', 'Card_Age', 'Transaction_Distance', 'Authentication_Method', 'Risk_Score', 'Is_Weekend', 'Fraud_Label']\n",
      "\n",
      "Pierwsze 5 wierszy:\n",
      "  Transaction_ID    User_ID  Transaction_Amount Transaction_Type            Timestamp  Account_Balance Device_Type  Location Merchant_Category  IP_Address_Flag  Previous_Fraudulent_Activity  Daily_Transaction_Count  Avg_Transaction_Amount_7d  Failed_Transaction_Count_7d   Card_Type  Card_Age  Transaction_Distance Authentication_Method  Risk_Score  Is_Weekend  Fraud_Label\n",
      "0      TXN_33553  USER_1834               39.79              POS  2023-08-14 19:30:00         93213.17      Laptop    Sydney            Travel                0                             0                        7                     437.63                            3        Amex        65                883.17             Biometric      0.8494           0            0\n",
      "1       TXN_9427  USER_7875                1.19    Bank Transfer  2023-06-07 04:01:00         75725.25      Mobile  New York          Clothing                0                             0                       13                     478.76                            4  Mastercard       186               2203.36              Password      0.0959           0            1\n",
      "2        TXN_199  USER_2734               28.96           Online  2023-06-20 15:25:00          1588.96      Tablet    Mumbai       Restaurants                0                             0                       14                      50.01                            4        Visa       226               1909.29             Biometric      0.8400           0            1\n",
      "3      TXN_12447  USER_2617              254.32   ATM Withdrawal  2023-12-07 00:31:00         76807.20      Tablet  New York          Clothing                0                             0                        8                     182.48                            4        Visa        76               1311.86                   OTP      0.7935           0            1\n",
      "4      TXN_39489  USER_2014               31.28              POS  2023-11-11 23:44:00         92354.66      Mobile    Mumbai       Electronics                0                             1                       14                     328.69                            4  Mastercard       140                966.98              Password      0.3819           1            1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: TriggerAgent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mTriggerAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "Dane są tabularyczne. Przekazuję do PlannerAgent w celu stworzenia planu analizy.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: PlannerAgent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mPlannerAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "Jako Architekt Planu, po zapoznaniu się ze strukturą i podglądem dostarczonych danych, przedstawiam szczegółowy plan ich przygotowania. Celem jest przekształcenie surowych danych w czysty, spójny i zoptymalizowany zbiór, gotowy do przeprowadzenia analizy eksploracyjnej oraz budowy modeli predykcyjnych (np. wykrywania oszustw).\n",
      "\n",
      "Oto proponowany, numerowany plan działania:\n",
      "\n",
      "***\n",
      "\n",
      "### **Plan Czyszczenia i Przygotowania Danych Transakcyjnych**\n",
      "\n",
      "**Cel:** Przygotowanie danych do analizy i modelowania klasyfikacyjnego w celu identyfikacji transakcji typu \"fraud\".\n",
      "\n",
      "**1. Wstępna Analiza i Weryfikacja Integralności Danych**\n",
      "*   **1.1. Wczytanie danych:** Załadowanie zbioru danych do struktury DataFrame.\n",
      "*   **1.2. Podstawowe informacje:** Użycie funkcji `.info()` w celu weryfikacji wstępnych typów danych i liczby niepustych wartości w każdej kolumnie. Pozwoli to na szybkie zidentyfikowanie kolumn z potencjalnymi brakami.\n",
      "*   **1.3. Statystyki opisowe:** Wygenerowanie statystyk opisowych za pomocą `.describe(include='all')` dla kolumn numerycznych (średnia, mediana, odchylenie standardowe, kwantyle) i kategorycznych (liczba unikalnych wartości, najczęstsza wartość).\n",
      "*   **1.4. Weryfikacja duplikatów:** Sprawdzenie, czy w kolumnie `Transaction_ID` istnieją zduplikowane wartości. Każda transakcja powinna być unikalna. W przypadku znalezienia duplikatów, należy je usunąć.\n",
      "\n",
      "**2. Obsługa Brakujących Wartości (Missing Values)**\n",
      "*   **2.1. Identyfikacja:** Dokładne zliczenie brakujących wartości (`NaN`, `None`) w każdej kolumnie.\n",
      "*   **2.2. Strategia imputacji:**\n",
      "    *   **Kolumny numeryczne (`Transaction_Amount`, `Account_Balance`, `Avg_Transaction_Amount_7d`, `Transaction_Distance`, `Risk_Score`, `Card_Age`):** W przypadku niewielkiej liczby braków (<5%), zastosowanie imputacji **medianą**. Mediana jest bardziej odporna na wartości odstające niż średnia, co jest kluczowe w danych finansowych.\n",
      "    *   **Kolumny kategoryczne (`Location`, `Merchant_Category`, `Device_Type`, `Card_Type`, `Authentication_Method`):** W przypadku braków, zastosowanie imputacji **modą** (najczęściej występującą wartością) lub utworzenie nowej kategorii `'Unknown'`.\n",
      "    *   **Kolumny o wysokim odsetku braków:** Jeśli jakakolwiek kolumna posiada > 60% brakujących wartości, zostanie ona usunięta, ponieważ jej imputacja mogłaby wprowadzić zbyt dużo szumu do modelu.\n",
      "\n",
      "**3. Weryfikacja i Konwersja Typów Danych**\n",
      "*   **3.1. Konwersja kolumny `Timestamp`:** Zmiana typu danych kolumny `Timestamp` z `object` (string) na `datetime`. Jest to kluczowe dla dalszej inżynierii cech.\n",
      "*   **3.2. Weryfikacja kolumn numerycznych:** Upewnienie się, że wszystkie kolumny, które powinny być numeryczne (np. `Transaction_Amount`, `Account_Balance`), mają odpowiedni typ (`float` lub `int`), a nie `object`.\n",
      "*   **3.3. Weryfikacja kolumn binarnych:** Sprawdzenie, czy kolumny binarne (`IP_Address_Flag`, `Previous_Fraudulent_Activity`, `Is_Weekend`, `Fraud_Label`) mają typ `int` (0/1).\n",
      "\n",
      "**4. Inżynieria Cech (Feature Engineering)**\n",
      "*   **4.1. Cechy oparte na dacie i czasie (z `Timestamp`):**\n",
      "    *   `Godzina_Transakcji`: Ekstrakcja godziny z `Timestamp`. Wzorce oszustw często zależą od pory dnia (np. transakcje w środku nocy).\n",
      "    *   `Dzien_Tygodnia`: Ekstrakcja dnia tygodnia (0-6). Może to pomóc w identyfikacji wzorców weekendowych vs. w dni robocze (chociaż istnieje już flaga `Is_Weekend`, ta cecha da więcej szczegółów).\n",
      "    *   `Dzien_Miesiaca`: Ekstrakcja dnia miesiąca. Może być skorelowane z cyklami wypłat.\n",
      "*   **4.2. Cechy relacyjne:**\n",
      "    *   `Stosunek_Kwoty_Do_Salda`: Obliczenie stosunku `Transaction_Amount` do `Account_Balance`. Wysoki stosunek może być sygnałem ostrzegawczym. Należy obsłużyć dzielenie przez zero, jeśli `Account_Balance` może wynosić 0.\n",
      "    *   `Odchylenie_Od_Sredniej_7d`: Obliczenie różnicy `Transaction_Amount - Avg_Transaction_Amount_7d`. Wskaże, jak bardzo bieżąca transakcja odbiega od niedawnego zachowania użytkownika.\n",
      "*   **4.3. Kodowanie zmiennych kategorycznych:**\n",
      "    *   Zastosowanie **One-Hot Encoding** dla zmiennych kategorycznych o niskiej kardynalności (małej liczbie unikalnych wartości), takich jak `Transaction_Type`, `Device_Type`, `Card_Type`, `Authentication_Method`.\n",
      "    *   Dla zmiennych o potencjalnie wysokiej kardynalności (`Location`, `Merchant_Category`), początkowo również zostanie zastosowany One-Hot Encoding, ale z zastrzeżeniem, że w przypadku zbyt dużej liczby nowych kolumn, rozważone zostanie grupowanie rzadszych kategorii lub użycie technik takich jak Target Encoding.\n",
      "\n",
      "**5. Wykrywanie i Obsługa Wartości Odstających (Outliers)**\n",
      "*   **5.1. Identyfikacja:** Analiza dystrybucji kluczowych zmiennych numerycznych (`Transaction_Amount`, `Account_Balance`, `Transaction_Distance`) za pomocą histogramów i wykresów pudełkowych.\n",
      "*   **5.2. Strategia obsługi:** Zastosowanie metody **kwantyli (capping/winsorization)**. Wartości poniżej 1. percentyla i powyżej 99. percentyla zostaną \"spłaszczone\" do wartości granicznych tych percentyli. Jest to bezpieczniejsza metoda niż usuwanie, ponieważ wartości odstające w danych o oszustwach mogą być właśnie tymi, których szukamy. Ta technika ograniczy ich wpływ na skalowanie i model, nie tracąc przy tym informacji o ich wystąpieniu.\n",
      "\n",
      "**6. Skalowanie Danych**\n",
      "*   **6.1. Uzasadnienie:** Wiele algorytmów uczenia maszynowego (np. Regresja Logistyczna, SVM, Sieci Neuronowe) jest wrażliwych na skalę cech. Skalowanie zapewnia, że wszystkie cechy mają podobny zakres, co poprawia zbieżność i wydajność modeli.\n",
      "*   **6.2. Metoda:** Zastosowanie **`StandardScaler`** (standaryzacja Z-score) na wszystkich cechach numerycznych (po obsłudze outlierów i inżynierii cech). Standaryzacja przekształci dane tak, aby miały średnią równą 0 i odchylenie standardowe równe 1.\n",
      "\n",
      "**7. Finalizacja Zbioru Danych**\n",
      "*   **7.1. Usunięcie zbędnych kolumn:** Usunięcie oryginalnych kolumn, które zostały przetworzone lub nie będą używane w modelu, tj. `Transaction_ID`, `User_ID` (chyba że będzie używany do grupowania w bardziej zaawansowanych modelach), `Timestamp` (po ekstrakcji cech).\n",
      "*   **7.2. Ostateczna weryfikacja:** Sprawdzenie finalnego kształtu zbioru danych, typów danych i braku wartości `NaN`.\n",
      "*   **7.3. Podział na zbiór treningowy i testowy:** Podział danych na zbiór treningowy i testowy (np. w proporcji 80/20) z zachowaniem proporcji klasy docelowej (`Fraud_Label`) za pomocą stratyfikacji.\n",
      "\n",
      "***\n",
      "\n",
      "Powyższy plan jest kompleksowy i stanowi solidną podstawę do przygotowania danych. Jestem gotowy na ewentualne uwagi i sugestie od CriticAgenta.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: CriticAgent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mCriticAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "Analizuję przedstawiony plan.\n",
      "\n",
      "Plan jest logiczny i kompleksowy, jednak w kilku miejscach narusza kluczowe zasady prostoty i modułowości, co może utrudnić weryfikację i debugowanie. Wprowadzam następujące uwagi w celu jego poprawy.\n",
      "\n",
      "**1. Zbyt złożona Inżynieria Cech (Naruszenie zasady \"Jedna zmiana na raz\")**\n",
      "\n",
      "*   **Problem:** Kroki 4.1 i 4.2 proponują stworzenie wielu nowych, niezależnych cech w ramach jednego podpunktu. Krok 4.1 tworzy trzy cechy czasowe, a 4.2 dwie cechy relacyjne. Takie podejście utrudnia śledzenie zmian i debugowanie w przypadku, gdyby jedna z transformacji zawiodła.\n",
      "*   **Sugestia:** Rozbij te kroki na mniejsze, atomowe operacje. Każda nowa cecha powinna być tworzona w osobnym, dedykowanym kroku.\n",
      "    *   Zamiast jednego kroku 4.1, stwórz trzy osobne: \"4.1. Stwórz cechę `Godzina_Transakcji`\", \"4.2. Stwórz cechę `Dzien_Tygodnia`\", itd.\n",
      "    *   Podobnie, rozbij krok 4.2 na dwa osobne kroki dla `Stosunek_Kwoty_Do_Salda` i `Odchylenie_Od_Sredniej_7d`. Umożliwi to np. dedykowaną obsługę błędu dzielenia przez zero przy tworzeniu pierwszej z nich.\n",
      "\n",
      "**2. Zbyt agresywne Kodowanie Zmiennych Kategorycznych (Naruszenie zasady \"Jedna zmiana na raz\")**\n",
      "\n",
      "*   **Problem:** Krok 4.3, dotyczący kodowania zmiennych kategorycznych, jest zbyt szeroki. Zastosowanie One-Hot Encoding do wszystkich wymienionych zmiennych naraz, zwłaszcza tych o potencjalnie wysokiej kardynalności (`Location`, `Merchant_Category`), jest ryzykowne. Może to doprowadzić do niekontrolowanej \"eksplozji\" liczby kolumn i utrudnić interpretację.\n",
      "*   **Sugestia:** Podziel ten proces na co najmniej dwa etapy.\n",
      "    *   **Etap 1:** Zastosuj One-Hot Encoding tylko do zmiennych o gwarantowanej niskiej liczbie unikalnych wartości, np. `Transaction_Type`, `Device_Type`, `Card_Type`, `Authentication_Method`.\n",
      "    *   **Etap 2 (osobny krok):** Przeprowadź analizę kardynalności dla `Location` i `Merchant_Category`. Jeśli liczba unikalnych wartości jest wysoka (np. >15), zastosuj strategię redukcji, np. grupując najrzadsze kategorie w jedną wspólną kategorię `'Other'`, a dopiero potem zastosuj One-Hot Encoding. To musi być świadoma decyzja podjęta w osobnym kroku.\n",
      "\n",
      "**3. Grupowa obsługa wartości odstających (Naruszenie zasady \"Jedna zmiana na raz\")**\n",
      "\n",
      "*   **Problem:** Krok 5.2 sugeruje zastosowanie metody kwantyli (capping) do wielu kolumn jednocześnie. Chociaż metoda jest ta sama, jej wpływ na każdą zmienną może być różny. Grupowe przetwarzanie utrudnia weryfikację, czy operacja została poprawnie zastosowana do każdej z kolumn.\n",
      "*   **Sugestia:** Wykonaj operację cappingu w osobnych krokach dla każdej z kluczowych zmiennych (`Transaction_Amount`, `Account_Balance`, `Transaction_Distance`). Zapewni to pełną kontrolę i możliwość weryfikacji wyników dla każdej cechy z osobna.\n",
      "\n",
      "Proszę o zaktualizowanie planu zgodnie z powyższymi zaleceniami, aby był bardziej modułowy, odporny na błędy i łatwiejszy do wdrożenia. Odeślij poprawioną wersję do ponownej recenzji.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: PlannerAgent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mPlannerAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "Dziękuję za cenną recenzję. Zgadzam się, że wprowadzenie większej modułowości i atomowości operacji znacząco poprawi przejrzystość, kontrolę i łatwość debugowania procesu przygotowania danych.\n",
      "\n",
      "Poniżej przedstawiam **nową, kompletną wersję planu**, która uwzględnia wszystkie Twoje sugestie. Zmiany zostały wyraźnie oznaczone.\n",
      "\n",
      "***\n",
      "\n",
      "### **Plan Czyszczenia i Przygotowania Danych Transakcyjnych (Wersja 2)**\n",
      "\n",
      "**Cel:** Przygotowanie danych do analizy i modelowania klasyfikacyjnego w celu identyfikacji transakcji typu \"fraud\".\n",
      "\n",
      "**1. Wstępna Analiza i Weryfikacja Integralności Danych**\n",
      "*   **1.1. Wczytanie danych:** Załadowanie zbioru danych do struktury DataFrame.\n",
      "*   **1.2. Podstawowe informacje:** Użycie funkcji `.info()` w celu weryfikacji wstępnych typów danych i liczby niepustych wartości w każdej kolumnie.\n",
      "*   **1.3. Statystyki opisowe:** Wygenerowanie statystyk opisowych za pomocą `.describe(include='all')` dla kolumn numerycznych i kategorycznych.\n",
      "*   **1.4. Weryfikacja duplikatów:** Sprawdzenie, czy w kolumnie `Transaction_ID` istnieją zduplikowane wartości. W przypadku znalezienia duplikatów, należy je usunąć.\n",
      "\n",
      "**2. Obsługa Brakujących Wartości (Missing Values)**\n",
      "*   **2.1. Identyfikacja:** Dokładne zliczenie brakujących wartości (`NaN`, `None`) w każdej kolumnie.\n",
      "*   **2.2. Strategia imputacji:**\n",
      "    *   **Kolumny numeryczne (`Transaction_Amount`, `Account_Balance`, `Avg_Transaction_Amount_7d`, `Transaction_Distance`, `Risk_Score`, `Card_Age`):** W przypadku niewielkiej liczby braków (<5%), zastosowanie imputacji **medianą**.\n",
      "    *   **Kolumny kategoryczne (`Location`, `Merchant_Category`, `Device_Type`, `Card_Type`, `Authentication_Method`):** W przypadku braków, zastosowanie imputacji **modą** (najczęściej występującą wartością) lub utworzenie nowej kategorii `'Unknown'`.\n",
      "    *   **Kolumny o wysokim odsetku braków:** Jeśli jakakolwiek kolumna posiada > 60% brakujących wartości, zostanie ona usunięta.\n",
      "\n",
      "**3. Weryfikacja i Konwersja Typów Danych**\n",
      "*   **3.1. Konwersja kolumny `Timestamp`:** Zmiana typu danych kolumny `Timestamp` z `object` (string) na `datetime`.\n",
      "*   **3.2. Weryfikacja kolumn numerycznych:** Upewnienie się, że wszystkie kolumny, które powinny być numeryczne, mają odpowiedni typ (`float` lub `int`).\n",
      "*   **3.3. Weryfikacja kolumn binarnych:** Sprawdzenie, czy kolumny binarne (`IP_Address_Flag`, `Previous_Fraudulent_Activity`, `Is_Weekend`, `Fraud_Label`) mają typ `int` (0/1).\n",
      "\n",
      "**4. Inżynieria Cech (Feature Engineering)**\n",
      "*   **4.1. [ZMIANA - Krok atomowy] Stworzenie cechy `Godzina_Transakcji`:** Ekstrakcja godziny (0-23) z kolumny `Timestamp`.\n",
      "*   **4.2. [ZMIANA - Krok atomowy] Stworzenie cechy `Dzien_Tygodnia`:** Ekstrakcja dnia tygodnia (0-6) z kolumny `Timestamp`.\n",
      "*   **4.3. [ZMIANA - Krok atomowy] Stworzenie cechy `Dzien_Miesiaca`:** Ekstrakcja dnia miesiąca (1-31) z kolumny `Timestamp`.\n",
      "*   **4.4. [ZMIANA - Krok atomowy] Stworzenie cechy `Stosunek_Kwoty_Do_Salda`:** Obliczenie stosunku `Transaction_Amount` do `Account_Balance`. W kroku tym zostanie zaimplementowana obsługa przypadków, gdy `Account_Balance` wynosi zero (np. przez zastąpienie wyniku wartością 0 lub bardzo dużą liczbą, aby uniknąć błędu `DivisionByZeroError`).\n",
      "*   **4.5. [ZMIANA - Krok atomowy] Stworzenie cechy `Odchylenie_Od_Sredniej_7d`:** Obliczenie różnicy `Transaction_Amount - Avg_Transaction_Amount_7d`.\n",
      "*   **4.6. [ZMIANA - Podział kodowania] Kodowanie zmiennych kategorycznych o niskiej kardynalności:** Zastosowanie **One-Hot Encoding** dla zmiennych: `Transaction_Type`, `Device_Type`, `Card_Type`, `Authentication_Method`.\n",
      "*   **4.7. [ZMIANA - Podział kodowania] Analiza i kodowanie zmiennych kategorycznych o wysokiej kardynalności:**\n",
      "    *   **4.7.1. Analiza:** Zliczenie unikalnych wartości w kolumnach `Location` i `Merchant_Category`.\n",
      "    *   **4.7.2. Redukcja kardynalności:** Jeśli liczba unikalnych wartości w którejkolwiek z tych kolumn przekracza próg (np. 15), wszystkie kategorie występujące rzadziej niż w 1% przypadków zostaną zgrupowane w jedną, nową kategorię `'Other'`.\n",
      "    *   **4.7.3. Kodowanie:** Zastosowanie **One-Hot Encoding** na przetworzonych kolumnach `Location` i `Merchant_Category`.\n",
      "\n",
      "**5. Wykrywanie i Obsługa Wartości Odstających (Outliers)**\n",
      "*   **5.1. Identyfikacja:** Analiza dystrybucji kluczowych zmiennych numerycznych za pomocą histogramów i wykresów pudełkowych w celu wizualnego potwierdzenia obecności wartości odstających.\n",
      "*   **5.2. [ZMIANA - Krok atomowy] Obsługa wartości odstających w `Transaction_Amount`:** Zastosowanie metody kwantyli (capping). Wartości poniżej 1. percentyla i powyżej 99. percentyla zostaną zastąpione wartościami granicznymi tych percentyli.\n",
      "*   **5.3. [ZMIANA - Krok atomowy] Obsługa wartości odstających w `Account_Balance`:** Zastosowanie metody kwantyli (capping) z progami 1. i 99. percentyla.\n",
      "*   **5.4. [ZMIANA - Krok atomowy] Obsługa wartości odstających w `Transaction_Distance`:** Zastosowanie metody kwantyli (capping) z progami 1. i 99. percentyla.\n",
      "\n",
      "**6. Skalowanie Danych**\n",
      "*   **6.1. Uzasadnienie:** Wiele algorytmów uczenia maszynowego jest wrażliwych na skalę cech. Skalowanie zapewnia, że wszystkie cechy mają podobny zakres, co poprawia zbieżność i wydajność modeli.\n",
      "*   **6.2. Metoda:** Zastosowanie **`StandardScaler`** (standaryzacja Z-score) na wszystkich cechach numerycznych (po obsłudze outlierów i inżynierii cech).\n",
      "\n",
      "**7. Finalizacja Zbioru Danych**\n",
      "*   **7.1. Usunięcie zbędnych kolumn:** Usunięcie oryginalnych kolumn, które zostały przetworzone lub nie będą używane w modelu, tj. `Transaction_ID`, `User_ID`, `Timestamp` oraz oryginalne kolumny kategoryczne po ich zakodowaniu.\n",
      "*   **7.2. Ostateczna weryfikacja:** Sprawdzenie finalnego kształtu zbioru danych, typów danych i braku wartości `NaN`.\n",
      "*   **7.3. Podział na zbiór treningowy i testowy:** Podział danych na zbiór treningowy i testowy (np. w proporcji 80/20) z zachowaniem proporcji klasy docelowej (`Fraud_Label`) za pomocą stratyfikacji.\n",
      "\n",
      "***\n",
      "\n",
      "Przesyłam zaktualizowany, bardziej szczegółowy i modułowy plan do ponownej recenzji.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: CriticAgent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mCriticAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "OSTATECZNY PLAN:\n",
      "### **Plan Czyszczenia i Przygotowania Danych Transakcyjnych (Wersja 2)**\n",
      "\n",
      "**Cel:** Przygotowanie danych do analizy i modelowania klasyfikacyjnego w celu identyfikacji transakcji typu \"fraud\".\n",
      "\n",
      "**1. Wstępna Analiza i Weryfikacja Integralności Danych**\n",
      "*   **1.1. Wczytanie danych:** Załadowanie zbioru danych do struktury DataFrame.\n",
      "*   **1.2. Podstawowe informacje:** Użycie funkcji `.info()` w celu weryfikacji wstępnych typów danych i liczby niepustych wartości w każdej kolumnie.\n",
      "*   **1.3. Statystyki opisowe:** Wygenerowanie statystyk opisowych za pomocą `.describe(include='all')` dla kolumn numerycznych i kategorycznych.\n",
      "*   **1.4. Weryfikacja duplikatów:** Sprawdzenie, czy w kolumnie `Transaction_ID` istnieją zduplikowane wartości. W przypadku znalezienia duplikatów, należy je usunąć.\n",
      "\n",
      "**2. Obsługa Brakujących Wartości (Missing Values)**\n",
      "*   **2.1. Identyfikacja:** Dokładne zliczenie brakujących wartości (`NaN`, `None`) w każdej kolumnie.\n",
      "*   **2.2. Strategia imputacji:**\n",
      "    *   **Kolumny numeryczne (`Transaction_Amount`, `Account_Balance`, `Avg_Transaction_Amount_7d`, `Transaction_Distance`, `Risk_Score`, `Card_Age`):** W przypadku niewielkiej liczby braków (<5%), zastosowanie imputacji **medianą**.\n",
      "    *   **Kolumny kategoryczne (`Location`, `Merchant_Category`, `Device_Type`, `Card_Type`, `Authentication_Method`):** W przypadku braków, zastosowanie imputacji **modą** (najczęściej występującą wartością) lub utworzenie nowej kategorii `'Unknown'`.\n",
      "    *   **Kolumny o wysokim odsetku braków:** Jeśli jakakolwiek kolumna posiada > 60% brakujących wartości, zostanie ona usunięta.\n",
      "\n",
      "**3. Weryfikacja i Konwersja Typów Danych**\n",
      "*   **3.1. Konwersja kolumny `Timestamp`:** Zmiana typu danych kolumny `Timestamp` z `object` (string) na `datetime`.\n",
      "*   **3.2. Weryfikacja kolumn numerycznych:** Upewnienie się, że wszystkie kolumny, które powinny być numeryczne, mają odpowiedni typ (`float` lub `int`).\n",
      "*   **3.3. Weryfikacja kolumn binarnych:** Sprawdzenie, czy kolumny binarne (`IP_Address_Flag`, `Previous_Fraudulent_Activity`, `Is_Weekend`, `Fraud_Label`) mają typ `int` (0/1).\n",
      "\n",
      "**4. Inżynieria Cech (Feature Engineering)**\n",
      "*   **4.1. [ZMIANA - Krok atomowy] Stworzenie cechy `Godzina_Transakcji`:** Ekstrakcja godziny (0-23) z kolumny `Timestamp`.\n",
      "*   **4.2. [ZMIANA - Krok atomowy] Stworzenie cechy `Dzien_Tygodnia`:** Ekstrakcja dnia tygodnia (0-6) z kolumny `Timestamp`.\n",
      "*   **4.3. [ZMIANA - Krok atomowy] Stworzenie cechy `Dzien_Miesiaca`:** Ekstrakcja dnia miesiąca (1-31) z kolumny `Timestamp`.\n",
      "*   **4.4. [ZMIANA - Krok atomowy] Stworzenie cechy `Stosunek_Kwoty_Do_Salda`:** Obliczenie stosunku `Transaction_Amount` do `Account_Balance`. W kroku tym zostanie zaimplementowana obsługa przypadków, gdy `Account_Balance` wynosi zero (np. przez zastąpienie wyniku wartością 0 lub bardzo dużą liczbą, aby uniknąć błędu `DivisionByZeroError`).\n",
      "*   **4.5. [ZMIANA - Krok atomowy] Stworzenie cechy `Odchylenie_Od_Sredniej_7d`:** Obliczenie różnicy `Transaction_Amount - Avg_Transaction_Amount_7d`.\n",
      "*   **4.6. [ZMIANA - Podział kodowania] Kodowanie zmiennych kategorycznych o niskiej kardynalności:** Zastosowanie **One-Hot Encoding** dla zmiennych: `Transaction_Type`, `Device_Type`, `Card_Type`, `Authentication_Method`.\n",
      "*   **4.7. [ZMIANA - Podział kodowania] Analiza i kodowanie zmiennych kategorycznych o wysokiej kardynalności:**\n",
      "    *   **4.7.1. Analiza:** Zliczenie unikalnych wartości w kolumnach `Location` i `Merchant_Category`.\n",
      "    *   **4.7.2. Redukcja kardynalności:** Jeśli liczba unikalnych wartości w którejkolwiek z tych kolumn przekracza próg (np. 15), wszystkie kategorie występujące rzadziej niż w 1% przypadków zostaną zgrupowane w jedną, nową kategorię `'Other'`.\n",
      "    *   **4.7.3. Kodowanie:** Zastosowanie **One-Hot Encoding** na przetworzonych kolumnach `Location` i `Merchant_Category`.\n",
      "\n",
      "**5. Wykrywanie i Obsługa Wartości Odstających (Outliers)**\n",
      "*   **5.1. Identyfikacja:** Analiza dystrybucji kluczowych zmiennych numerycznych za pomocą histogramów i wykresów pudełkowych w celu wizualnego potwierdzenia obecności wartości odstających.\n",
      "*   **5.2. [ZMIANA - Krok atomowy] Obsługa wartości odstających w `Transaction_Amount`:** Zastosowanie metody kwantyli (capping). Wartości poniżej 1. percentyla i powyżej 99. percentyla zostaną zastąpione wartościami granicznymi tych percentyli.\n",
      "*   **5.3. [ZMIANA - Krok atomowy] Obsługa wartości odstających w `Account_Balance`:** Zastosowanie metody kwantyli (capping) z progami 1. i 99. percentyla.\n",
      "*   **5.4. [ZMIANA - Krok atomowy] Obsługa wartości odstających w `Transaction_Distance`:** Zastosowanie metody kwantyli (capping) z progami 1. i 99. percentyla.\n",
      "\n",
      "**6. Skalowanie Danych**\n",
      "*   **6.1. Uzasadnienie:** Wiele algorytmów uczenia maszynowego jest wrażliwych na skalę cech. Skalowanie zapewnia, że wszystkie cechy mają podobny zakres, co poprawia zbieżność i wydajność modeli.\n",
      "*   **6.2. Metoda:** Zastosowanie **`StandardScaler`** (standaryzacja Z-score) na wszystkich cechach numerycznych (po obsłudze outlierów i inżynierii cech).\n",
      "\n",
      "**7. Finalizacja Zbioru Danych**\n",
      "*   **7.1. Usunięcie zbędnych kolumn:** Usunięcie oryginalnych kolumn, które zostały przetworzone lub nie będą używane w modelu, tj. `Transaction_ID`, `User_ID`, `Timestamp` oraz oryginalne kolumny kategoryczne po ich zakodowaniu.\n",
      "*   **7.2. Ostateczna weryfikacja:** Sprawdzenie finalnego kształtu zbioru danych, typów danych i braku wartości `NaN`.\n",
      "*   **7.3. Podział na zbiór treningowy i testowy:** Podział danych na zbiór treningowy i testowy (np. w proporcji 80/20) z zachowaniem proporcji klasy docelowej (`Fraud_Label`) za pomocą stratyfikacji.\n",
      "\n",
      "PLAN_AKCEPTOWANY_PRZEJSCIE_DO_IMPLEMENTACJI\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (72ce0cb9-83c4-495a-a6e3-d43334e1b7bd): No next speaker selected\u001b[0m\n",
      "Faza planowania zakończona. Ostateczny plan został zaakceptowany.\n",
      "INFO: Próba zapisu pełnego logu rozmowy do pliku: reports/autogen_planning_conversation.log\n",
      "✅ SUKCES: Log rozmowy został pomyślnie zapisany.\n",
      "\n",
      "================================================================================\n",
      "### ### FAZA 2: URUCHAMIANIE WYKONANIA PLANU (LangGraph) ### ###\n",
      "================================================================================\n",
      "\n",
      "--- WĘZEŁ: ANALIZATOR SCHEMATU DANYCH ---\n",
      "DEBUG: Próbuję odczytać plik ze ścieżki: gs://super_model/data/structural_data/synthetic_fraud_dataset.csv\n",
      "INFO: Wygenerowano sygnaturę danych: ae1568fe7dae11d4bacd0c21ed718503\n",
      "--- Krok: 'schema_reader' ---\n",
      "{\n",
      "  \"available_columns\": [\n",
      "    \"Transaction_ID\",\n",
      "    \"User_ID\",\n",
      "    \"Transaction_Amount\",\n",
      "    \"Transaction_Type\",\n",
      "    \"Timestamp\",\n",
      "    \"Account_Balance\",\n",
      "    \"Device_Type\",\n",
      "    \"Location\",\n",
      "    \"Merchant_Category\",\n",
      "    \"IP_Address_Flag\",\n",
      "    \"Previous_Fraudulent_Activity\",\n",
      "    \"Daily_Transaction_Count\",\n",
      "    \"Avg_Transaction_Amount_7d\",\n",
      "    \"Failed_Transaction_Count_7d\",\n",
      "    \"Card_Type\",\n",
      "    \"Card_Age\",\n",
      "    \"Transaction_Distance\",\n",
      "    \"Authentication_Method\",\n",
      "    \"Risk_Score\",\n",
      "    \"Is_Weekend\",\n",
      "    \"Fraud_Label\"\n",
      "  ],\n",
      "  \"dataset_signature\": \"ae1568fe7dae11d4bacd0c21ed718503\"\n",
      "}\n",
      "--------------------\n",
      "\n",
      "---  WĘZEŁ: GENERATOR KODU ---\n",
      "\n",
      "Agent-Analityk wygenerował następujący kod:\n",
      "--------------------------------------------------\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "\n",
      "def process_data(input_path: str, output_path: str):\n",
      "    \"\"\"\n",
      "    Kompletny pipeline czyszczenia i przygotowania danych transakcyjnych\n",
      "    do analizy klasyfikacyjnej wykrywania fraudów.\n",
      "    \n",
      "    Plan działania:\n",
      "    # Krok 1: Wstępna analiza i weryfikacja integralności danych\n",
      "    # Krok 2: Obsługa brakujących wartości\n",
      "    # Krok 3: Weryfikacja i konwersja typów danych\n",
      "    # Krok 4: Inżynieria cech\n",
      "    # Krok 5: Wykrywanie i obsługa wartości odstających\n",
      "    # Krok 6: Skalowanie danych\n",
      "    # Krok 7: Finalizacja zbioru danych\n",
      "    \"\"\"\n",
      "    \n",
      "    # ===================================================================\n",
      "    # Krok 1: Wstępna analiza i weryfikacja integralności danych\n",
      "    # ===================================================================\n",
      "    \n",
      "    print(\"=== KROK 1: WSTĘPNA ANALIZA DANYCH ===\")\n",
      "    \n",
      "    # 1.1. Wczytanie danych\n",
      "    try:\n",
      "        df = pd.read_csv(input_path)\n",
      "        print(f\"✓ Dane wczytane pomyślnie. Kształt: {df.shape}\")\n",
      "    except Exception as e:\n",
      "        print(f\"✗ Błąd wczytywania danych: {e}\")\n",
      "        return\n",
      "    \n",
      "    # 1.2. Podstawowe informacje\n",
      "    print(\"\\n--- Podstawowe informacje o danych ---\")\n",
      "    print(df.info())\n",
      "    \n",
      "    # 1.3. Statystyki opisowe\n",
      "    print(\"\\n--- Statystyki opisowe ---\")\n",
      "    print(df.describe(include='all'))\n",
      "    \n",
      "    # 1.4. Weryfikacja duplikatów w Transaction_ID\n",
      "    if 'Transaction_ID' in df.columns:\n",
      "        duplicates = df['Transaction_ID'].duplicated().sum()\n",
      "        print(f\"\\n--- Duplikaty w Transaction_ID: {duplicates} ---\")\n",
      "        if duplicates > 0:\n",
      "            df = df.drop_duplicates(subset=['Transaction_ID'], keep='first')\n",
      "            print(f\"✓ Usunięto {duplicates} duplikatów\")\n",
      "    \n",
      "    # ===================================================================\n",
      "    # Krok 2: Obsługa brakujących wartości\n",
      "    # ===================================================================\n",
      "    \n",
      "    print(\"\\n=== KROK 2: OBSŁUGA BRAKUJĄCYCH WARTOŚCI ===\")\n",
      "    \n",
      "    # 2.1. Identyfikacja brakujących wartości\n",
      "    missing_values = df.isnull().sum()\n",
      "    missing_percent = (missing_values / len(df)) * 100\n",
      "    missing_df = pd.DataFrame({\n",
      "        'Kolumna': missing_values.index,\n",
      "        'Brakujące': missing_values.values,\n",
      "        'Procent': missing_percent.values\n",
      "    })\n",
      "    print(\"\\n--- Brakujące wartości ---\")\n",
      "    print(missing_df[missing_df['Brakujące'] > 0])\n",
      "    \n",
      "    # 2.2. Usunięcie kolumn z > 60% braków\n",
      "    columns_to_drop = missing_df[missing_df['Procent'] > 60]['Kolumna'].tolist()\n",
      "    if columns_to_drop:\n",
      "        df = df.drop(columns=columns_to_drop)\n",
      "        print(f\"✓ Usunięto kolumny z > 60% braków: {columns_to_drop}\")\n",
      "    \n",
      "    # 2.3. Imputacja kolumn numerycznych medianą\n",
      "    numeric_columns = ['Transaction_Amount', 'Account_Balance', 'Avg_Transaction_Amount_7d', \n",
      "                      'Transaction_Distance', 'Risk_Score', 'Card_Age', 'Daily_Transaction_Count',\n",
      "                      'Failed_Transaction_Count_7d']\n",
      "    \n",
      "    for col in numeric_columns:\n",
      "        if col in df.columns and df[col].isnull().sum() > 0:\n",
      "            if (df[col].isnull().sum() / len(df)) < 0.05:  # < 5% braków\n",
      "                median_val = df[col].median()\n",
      "                df[col].fillna(median_val, inplace=True)\n",
      "                print(f\"✓ Imputacja medianą dla {col}: {median_val}\")\n",
      "    \n",
      "    # 2.4. Imputacja kolumn kategorycznych modą\n",
      "    categorical_columns = ['Location', 'Merchant_Category', 'Device_Type', 'Card_Type', \n",
      "                          'Authentication_Method', 'Transaction_Type']\n",
      "    \n",
      "    for col in categorical_columns:\n",
      "        if col in df.columns and df[col].isnull().sum() > 0:\n",
      "            mode_val = df[col].mode()\n",
      "            if len(mode_val) > 0:\n",
      "                df[col].fillna(mode_val[0], inplace=True)\n",
      "                print(f\"✓ Imputacja modą dla {col}: {mode_val[0]}\")\n",
      "            else:\n",
      "                df[col].fillna('Unknown', inplace=True)\n",
      "                print(f\"✓ Imputacja 'Unknown' dla {col}\")\n",
      "    \n",
      "    # ===================================================================\n",
      "    # Krok 3: Weryfikacja i konwersja typów danych\n",
      "    # ===================================================================\n",
      "    \n",
      "    print(\"\\n=== KROK 3: KONWERSJA TYPÓW DANYCH ===\")\n",
      "    \n",
      "    # 3.1. Konwersja Timestamp\n",
      "    if 'Timestamp' in df.columns:\n",
      "        try:\n",
      "            df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
      "            print(\"✓ Konwersja Timestamp na datetime\")\n",
      "        except Exception as e:\n",
      "            print(f\"✗ Błąd konwersji Timestamp: {e}\")\n",
      "    \n",
      "    # 3.2. Weryfikacja kolumn numerycznych\n",
      "    for col in numeric_columns:\n",
      "        if col in df.columns:\n",
      "            try:\n",
      "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
      "            except Exception as e:\n",
      "                print(f\"✗ Błąd konwersji {col}: {e}\")\n",
      "    \n",
      "    # 3.3. Weryfikacja kolumn binarnych\n",
      "    binary_columns = ['IP_Address_Flag', 'Previous_Fraudulent_Activity', 'Is_Weekend', 'Fraud_Label']\n",
      "    for col in binary_columns:\n",
      "        if col in df.columns:\n",
      "            try:\n",
      "                df[col] = df[col].astype(int)\n",
      "                print(f\"✓ Konwersja {col} na int\")\n",
      "            except Exception as e:\n",
      "                print(f\"✗ Błąd konwersji {col}: {e}\")\n",
      "    \n",
      "    # ===================================================================\n",
      "    # Krok 4: Inżynieria cech\n",
      "    # ===================================================================\n",
      "    \n",
      "    print(\"\\n=== KROK 4: INŻYNIERIA CECH ===\")\n",
      "    \n",
      "    # 4.1. Stworzenie cechy Godzina_Transakcji\n",
      "    if 'Timestamp' in df.columns:\n",
      "        df['Godzina_Transakcji'] = df['Timestamp'].dt.hour\n",
      "        print(\"✓ Utworzono cechę: Godzina_Transakcji\")\n",
      "    \n",
      "    # 4.2. Stworzenie cechy Dzien_Tygodnia\n",
      "    if 'Timestamp' in df.columns:\n",
      "        df['Dzien_Tygodnia'] = df['Timestamp'].dt.dayofweek\n",
      "        print(\"✓ Utworzono cechę: Dzien_Tygodnia\")\n",
      "    \n",
      "    # 4.3. Stworzenie cechy Dzien_Miesiaca\n",
      "    if 'Timestamp' in df.columns:\n",
      "        df['\n",
      "--------------------------------------------------\n",
      "--- Krok: 'code_generator' ---\n",
      "--- GENERATED_CODE ---\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "\n",
      "def process_data(input_path: str, output_path: str):\n",
      "    \"\"\"\n",
      "    Kompletny pipeline czyszczenia i przygotowania danych transakcyjnych\n",
      "    do analizy klasyfikacyjnej wykrywania fraudów.\n",
      "    \n",
      "    Plan działania:\n",
      "    # Krok 1: Wstępna analiza i weryfikacja integralności danych\n",
      "    # Krok 2: Obsługa brakujących wartości\n",
      "    # Krok 3: Weryfikacja i konwersja typów danych\n",
      "    # Krok 4: Inżynieria cech\n",
      "    # Krok 5: Wykrywanie i obsługa wartości odstających\n",
      "    # Krok 6: Skalowanie danych\n",
      "    # Krok 7: Finalizacja zbioru danych\n",
      "    \"\"\"\n",
      "    \n",
      "    # ===================================================================\n",
      "    # Krok 1: Wstępna analiza i weryfikacja integralności danych\n",
      "    # ===================================================================\n",
      "    \n",
      "    print(\"=== KROK 1: WSTĘPNA ANALIZA DANYCH ===\")\n",
      "    \n",
      "    # 1.1. Wczytanie danych\n",
      "    try:\n",
      "        df = pd.read_csv(input_path)\n",
      "        print(f\"✓ Dane wczytane pomyślnie. Kształt: {df.shape}\")\n",
      "    except Exception as e:\n",
      "        print(f\"✗ Błąd wczytywania danych: {e}\")\n",
      "        return\n",
      "    \n",
      "    # 1.2. Podstawowe informacje\n",
      "    print(\"\\n--- Podstawowe informacje o danych ---\")\n",
      "    print(df.info())\n",
      "    \n",
      "    # 1.3. Statystyki opisowe\n",
      "    print(\"\\n--- Statystyki opisowe ---\")\n",
      "    print(df.describe(include='all'))\n",
      "    \n",
      "    # 1.4. Weryfikacja duplikatów w Transaction_ID\n",
      "    if 'Transaction_ID' in df.columns:\n",
      "        duplicates = df['Transaction_ID'].duplicated().sum()\n",
      "        print(f\"\\n--- Duplikaty w Transaction_ID: {duplicates} ---\")\n",
      "        if duplicates > 0:\n",
      "            df = df.drop_duplicates(subset=['Transaction_ID'], keep='first')\n",
      "            print(f\"✓ Usunięto {duplicates} duplikatów\")\n",
      "    \n",
      "    # ===================================================================\n",
      "    # Krok 2: Obsługa brakujących wartości\n",
      "    # ===================================================================\n",
      "    \n",
      "    print(\"\\n=== KROK 2: OBSŁUGA BRAKUJĄCYCH WARTOŚCI ===\")\n",
      "    \n",
      "    # 2.1. Identyfikacja brakujących wartości\n",
      "    missing_values = df.isnull().sum()\n",
      "    missing_percent = (missing_values / len(df)) * 100\n",
      "    missing_df = pd.DataFrame({\n",
      "        'Kolumna': missing_values.index,\n",
      "        'Brakujące': missing_values.values,\n",
      "        'Procent': missing_percent.values\n",
      "    })\n",
      "    print(\"\\n--- Brakujące wartości ---\")\n",
      "    print(missing_df[missing_df['Brakujące'] > 0])\n",
      "    \n",
      "    # 2.2. Usunięcie kolumn z > 60% braków\n",
      "    columns_to_drop = missing_df[missing_df['Procent'] > 60]['Kolumna'].tolist()\n",
      "    if columns_to_drop:\n",
      "        df = df.drop(columns=columns_to_drop)\n",
      "        print(f\"✓ Usunięto kolumny z > 60% braków: {columns_to_drop}\")\n",
      "    \n",
      "    # 2.3. Imputacja kolumn numerycznych medianą\n",
      "    numeric_columns = ['Transaction_Amount', 'Account_Balance', 'Avg_Transaction_Amount_7d', \n",
      "                      'Transaction_Distance', 'Risk_Score', 'Card_Age', 'Daily_Transaction_Count',\n",
      "                      'Failed_Transaction_Count_7d']\n",
      "    \n",
      "    for col in numeric_columns:\n",
      "        if col in df.columns and df[col].isnull().sum() > 0:\n",
      "            if (df[col].isnull().sum() / len(df)) < 0.05:  # < 5% braków\n",
      "                median_val = df[col].median()\n",
      "                df[col].fillna(median_val, inplace=True)\n",
      "                print(f\"✓ Imputacja medianą dla {col}: {median_val}\")\n",
      "    \n",
      "    # 2.4. Imputacja kolumn kategorycznych modą\n",
      "    categorical_columns = ['Location', 'Merchant_Category', 'Device_Type', 'Card_Type', \n",
      "                          'Authentication_Method', 'Transaction_Type']\n",
      "    \n",
      "    for col in categorical_columns:\n",
      "        if col in df.columns and df[col].isnull().sum() > 0:\n",
      "            mode_val = df[col].mode()\n",
      "            if len(mode_val) > 0:\n",
      "                df[col].fillna(mode_val[0], inplace=True)\n",
      "                print(f\"✓ Imputacja modą dla {col}: {mode_val[0]}\")\n",
      "            else:\n",
      "                df[col].fillna('Unknown', inplace=True)\n",
      "                print(f\"✓ Imputacja 'Unknown' dla {col}\")\n",
      "    \n",
      "    # ===================================================================\n",
      "    # Krok 3: Weryfikacja i konwersja typów danych\n",
      "    # ===================================================================\n",
      "    \n",
      "    print(\"\\n=== KROK 3: KONWERSJA TYPÓW DANYCH ===\")\n",
      "    \n",
      "    # 3.1. Konwersja Timestamp\n",
      "    if 'Timestamp' in df.columns:\n",
      "        try:\n",
      "            df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
      "            print(\"✓ Konwersja Timestamp na datetime\")\n",
      "        except Exception as e:\n",
      "            print(f\"✗ Błąd konwersji Timestamp: {e}\")\n",
      "    \n",
      "    # 3.2. Weryfikacja kolumn numerycznych\n",
      "    for col in numeric_columns:\n",
      "        if col in df.columns:\n",
      "            try:\n",
      "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
      "            except Exception as e:\n",
      "                print(f\"✗ Błąd konwersji {col}: {e}\")\n",
      "    \n",
      "    # 3.3. Weryfikacja kolumn binarnych\n",
      "    binary_columns = ['IP_Address_Flag', 'Previous_Fraudulent_Activity', 'Is_Weekend', 'Fraud_Label']\n",
      "    for col in binary_columns:\n",
      "        if col in df.columns:\n",
      "            try:\n",
      "                df[col] = df[col].astype(int)\n",
      "                print(f\"✓ Konwersja {col} na int\")\n",
      "            except Exception as e:\n",
      "                print(f\"✗ Błąd konwersji {col}: {e}\")\n",
      "    \n",
      "    # ===================================================================\n",
      "    # Krok 4: Inżynieria cech\n",
      "    # ===================================================================\n",
      "    \n",
      "    print(\"\\n=== KROK 4: INŻYNIERIA CECH ===\")\n",
      "    \n",
      "    # 4.1. Stworzenie cechy Godzina_Transakcji\n",
      "    if 'Timestamp' in df.columns:\n",
      "        df['Godzina_Transakcji'] = df['Timestamp'].dt.hour\n",
      "        print(\"✓ Utworzono cechę: Godzina_Transakcji\")\n",
      "    \n",
      "    # 4.2. Stworzenie cechy Dzien_Tygodnia\n",
      "    if 'Timestamp' in df.columns:\n",
      "        df['Dzien_Tygodnia'] = df['Timestamp'].dt.dayofweek\n",
      "        print(\"✓ Utworzono cechę: Dzien_Tygodnia\")\n",
      "    \n",
      "    # 4.3. Stworzenie cechy Dzien_Miesiaca\n",
      "    if 'Timestamp' in df.columns:\n",
      "        df['\n",
      "----------------------\n",
      "--------------------\n",
      "\n",
      "--- 🛡️ WĘZEŁ: STRAŻNIK ARCHITEKTURY 🛡️ ---\n",
      "  [WERDYKT] ❌ Kod łamie zasady architektury: Skrypt nie kończy się wymaganym wywołaniem `process_data(input_path, output_path)  # noqa: F821`.\n",
      "--- Krok: 'architectural_validator' ---\n",
      "--- ERROR_CONTEXT_CODE ---\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "\n",
      "def process_data(input_path: str, output_path: str):\n",
      "    \"\"\"\n",
      "    Kompletny pipeline czyszczenia i przygotowania danych transakcyjnych\n",
      "    do analizy klasyfikacyjnej wykrywania fraudów.\n",
      "    \n",
      "    Plan działania:\n",
      "    # Krok 1: Wstępna analiza i weryfikacja integralności danych\n",
      "    # Krok 2: Obsługa brakujących wartości\n",
      "    # Krok 3: Weryfikacja i konwersja typów danych\n",
      "    # Krok 4: Inżynieria cech\n",
      "    # Krok 5: Wykrywanie i obsługa wartości odstających\n",
      "    # Krok 6: Skalowanie danych\n",
      "    # Krok 7: Finalizacja zbioru danych\n",
      "    \"\"\"\n",
      "    \n",
      "    # ===================================================================\n",
      "    # Krok 1: Wstępna analiza i weryfikacja integralności danych\n",
      "    # ===================================================================\n",
      "    \n",
      "    print(\"=== KROK 1: WSTĘPNA ANALIZA DANYCH ===\")\n",
      "    \n",
      "    # 1.1. Wczytanie danych\n",
      "    try:\n",
      "        df = pd.read_csv(input_path)\n",
      "        print(f\"✓ Dane wczytane pomyślnie. Kształt: {df.shape}\")\n",
      "    except Exception as e:\n",
      "        print(f\"✗ Błąd wczytywania danych: {e}\")\n",
      "        return\n",
      "    \n",
      "    # 1.2. Podstawowe informacje\n",
      "    print(\"\\n--- Podstawowe informacje o danych ---\")\n",
      "    print(df.info())\n",
      "    \n",
      "    # 1.3. Statystyki opisowe\n",
      "    print(\"\\n--- Statystyki opisowe ---\")\n",
      "    print(df.describe(include='all'))\n",
      "    \n",
      "    # 1.4. Weryfikacja duplikatów w Transaction_ID\n",
      "    if 'Transaction_ID' in df.columns:\n",
      "        duplicates = df['Transaction_ID'].duplicated().sum()\n",
      "        print(f\"\\n--- Duplikaty w Transaction_ID: {duplicates} ---\")\n",
      "        if duplicates > 0:\n",
      "            df = df.drop_duplicates(subset=['Transaction_ID'], keep='first')\n",
      "            print(f\"✓ Usunięto {duplicates} duplikatów\")\n",
      "    \n",
      "    # ===================================================================\n",
      "    # Krok 2: Obsługa brakujących wartości\n",
      "    # ===================================================================\n",
      "    \n",
      "    print(\"\\n=== KROK 2: OBSŁUGA BRAKUJĄCYCH WARTOŚCI ===\")\n",
      "    \n",
      "    # 2.1. Identyfikacja brakujących wartości\n",
      "    missing_values = df.isnull().sum()\n",
      "    missing_percent = (missing_values / len(df)) * 100\n",
      "    missing_df = pd.DataFrame({\n",
      "        'Kolumna': missing_values.index,\n",
      "        'Brakujące': missing_values.values,\n",
      "        'Procent': missing_percent.values\n",
      "    })\n",
      "    print(\"\\n--- Brakujące wartości ---\")\n",
      "    print(missing_df[missing_df['Brakujące'] > 0])\n",
      "    \n",
      "    # 2.2. Usunięcie kolumn z > 60% braków\n",
      "    columns_to_drop = missing_df[missing_df['Procent'] > 60]['Kolumna'].tolist()\n",
      "    if columns_to_drop:\n",
      "        df = df.drop(columns=columns_to_drop)\n",
      "        print(f\"✓ Usunięto kolumny z > 60% braków: {columns_to_drop}\")\n",
      "    \n",
      "    # 2.3. Imputacja kolumn numerycznych medianą\n",
      "    numeric_columns = ['Transaction_Amount', 'Account_Balance', 'Avg_Transaction_Amount_7d', \n",
      "                      'Transaction_Distance', 'Risk_Score', 'Card_Age', 'Daily_Transaction_Count',\n",
      "                      'Failed_Transaction_Count_7d']\n",
      "    \n",
      "    for col in numeric_columns:\n",
      "        if col in df.columns and df[col].isnull().sum() > 0:\n",
      "            if (df[col].isnull().sum() / len(df)) < 0.05:  # < 5% braków\n",
      "                median_val = df[col].median()\n",
      "                df[col].fillna(median_val, inplace=True)\n",
      "                print(f\"✓ Imputacja medianą dla {col}: {median_val}\")\n",
      "    \n",
      "    # 2.4. Imputacja kolumn kategorycznych modą\n",
      "    categorical_columns = ['Location', 'Merchant_Category', 'Device_Type', 'Card_Type', \n",
      "                          'Authentication_Method', 'Transaction_Type']\n",
      "    \n",
      "    for col in categorical_columns:\n",
      "        if col in df.columns and df[col].isnull().sum() > 0:\n",
      "            mode_val = df[col].mode()\n",
      "            if len(mode_val) > 0:\n",
      "                df[col].fillna(mode_val[0], inplace=True)\n",
      "                print(f\"✓ Imputacja modą dla {col}: {mode_val[0]}\")\n",
      "            else:\n",
      "                df[col].fillna('Unknown', inplace=True)\n",
      "                print(f\"✓ Imputacja 'Unknown' dla {col}\")\n",
      "    \n",
      "    # ===================================================================\n",
      "    # Krok 3: Weryfikacja i konwersja typów danych\n",
      "    # ===================================================================\n",
      "    \n",
      "    print(\"\\n=== KROK 3: KONWERSJA TYPÓW DANYCH ===\")\n",
      "    \n",
      "    # 3.1. Konwersja Timestamp\n",
      "    if 'Timestamp' in df.columns:\n",
      "        try:\n",
      "            df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
      "            print(\"✓ Konwersja Timestamp na datetime\")\n",
      "        except Exception as e:\n",
      "            print(f\"✗ Błąd konwersji Timestamp: {e}\")\n",
      "    \n",
      "    # 3.2. Weryfikacja kolumn numerycznych\n",
      "    for col in numeric_columns:\n",
      "        if col in df.columns:\n",
      "            try:\n",
      "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
      "            except Exception as e:\n",
      "                print(f\"✗ Błąd konwersji {col}: {e}\")\n",
      "    \n",
      "    # 3.3. Weryfikacja kolumn binarnych\n",
      "    binary_columns = ['IP_Address_Flag', 'Previous_Fraudulent_Activity', 'Is_Weekend', 'Fraud_Label']\n",
      "    for col in binary_columns:\n",
      "        if col in df.columns:\n",
      "            try:\n",
      "                df[col] = df[col].astype(int)\n",
      "                print(f\"✓ Konwersja {col} na int\")\n",
      "            except Exception as e:\n",
      "                print(f\"✗ Błąd konwersji {col}: {e}\")\n",
      "    \n",
      "    # ===================================================================\n",
      "    # Krok 4: Inżynieria cech\n",
      "    # ===================================================================\n",
      "    \n",
      "    print(\"\\n=== KROK 4: INŻYNIERIA CECH ===\")\n",
      "    \n",
      "    # 4.1. Stworzenie cechy Godzina_Transakcji\n",
      "    if 'Timestamp' in df.columns:\n",
      "        df['Godzina_Transakcji'] = df['Timestamp'].dt.hour\n",
      "        print(\"✓ Utworzono cechę: Godzina_Transakcji\")\n",
      "    \n",
      "    # 4.2. Stworzenie cechy Dzien_Tygodnia\n",
      "    if 'Timestamp' in df.columns:\n",
      "        df['Dzien_Tygodnia'] = df['Timestamp'].dt.dayofweek\n",
      "        print(\"✓ Utworzono cechę: Dzien_Tygodnia\")\n",
      "    \n",
      "    # 4.3. Stworzenie cechy Dzien_Miesiaca\n",
      "    if 'Timestamp' in df.columns:\n",
      "        df['\n",
      "--------------------------\n",
      "{\n",
      "  \"error_message\": \"B\\u0142\\u0105d Walidacji Architektonicznej: Skrypt nie ko\\u0144czy si\\u0119 wymaganym wywo\\u0142aniem `process_data(input_path, output_path)  # noqa: F821`.\",\n",
      "  \"failing_node\": \"architectural_validator\",\n",
      "  \"correction_attempts\": 1\n",
      "}\n",
      "--------------------\n",
      "\n",
      "--- WĘZEŁ: INTELIGENTNY DEBUGGER (Błąd w: architectural_validator) ---\n",
      "  [DIAGNOZA] Debugger wybrał narzędzie: 'propose_code_fix' z argumentami: {'corrected_code': 'import pandas as pd\\nimport numpy as np\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.model_selection import train_test_split\\nimport warnings\\nwarnings.filterwarnings(\\'ignore\\')\\n\\ndef process_data(input_path: str, output_path: str):\\n    \"\"\"\\n    Kompletny pipeline czyszczenia i przygotowania danych transakcyjnych\\n    do analizy klasyfikacyjnej wykrywania fraudów.\\n    \\n    Plan działania:\\n    # Krok 1: Wstępna analiza i weryfikacja integralności danych\\n    # Krok 2: Obsługa brakujących wartości\\n    # Krok 3: Weryfikacja i konwersja typów danych\\n    # Krok 4: Inżynieria cech\\n    # Krok 5: Wykrywanie i obsługa wartości odstających\\n    # Krok 6: Skalowanie danych i kodowanie zmiennych kategorycznych\\n    # Krok 7: Finalizacja zbioru danych\\n    \"\"\"\\n    \\n    # ===================================================================\\n    # Krok 1: Wstępna analiza i weryfikacja integralności danych\\n    # ===================================================================\\n    \\n    print(\"=== KROK 1: WSTĘPNA ANALIZA DANYCH ===\")\\n    \\n    # 1.1. Wczytanie danych\\n    try:\\n        df = pd.read_csv(input_path)\\n        print(f\"✓ Dane wczytane pomyślnie. Kształt: {df.shape}\")\\n    except Exception as e:\\n        print(f\"✗ Błąd wczytywania danych: {e}\")\\n        return\\n    \\n    # 1.2. Podstawowe informacje\\n    print(\"\\\\n--- Podstawowe informacje o danych ---\")\\n    df.info()\\n    \\n    # 1.3. Weryfikacja duplikatów w Transaction_ID\\n    if \\'Transaction_ID\\' in df.columns:\\n        duplicates = df[\\'Transaction_ID\\'].duplicated().sum()\\n        print(f\"\\\\n--- Duplikaty w Transaction_ID: {duplicates} ---\")\\n        if duplicates > 0:\\n            df = df.drop_duplicates(subset=[\\'Transaction_ID\\'], keep=\\'first\\')\\n            print(f\"✓ Usunięto {duplicates} duplikatów\")\\n    \\n    # ===================================================================\\n    # Krok 2: Obsługa brakujących wartości\\n    # ===================================================================\\n    \\n    print(\"\\\\n=== KROK 2: OBSŁUGA BRAKUJĄCYCH WARTOŚCI ===\")\\n    \\n    missing_values = df.isnull().sum()\\n    missing_percent = (missing_values / len(df)) * 100\\n    missing_df = pd.DataFrame({\\'Brakujące\\': missing_values, \\'Procent\\': missing_percent})\\n    print(\"\\\\n--- Brakujące wartości ---\")\\n    print(missing_df[missing_df[\\'Brakujące\\'] > 0])\\n    \\n    # Imputacja medianą dla kluczowych kolumn numerycznych\\n    numeric_cols_impute = [\\'Transaction_Amount\\', \\'Account_Balance\\', \\'Risk_Score\\']\\n    for col in numeric_cols_impute:\\n        if col in df.columns and df[col].isnull().any():\\n            median_val = df[col].median()\\n            df[col].fillna(median_val, inplace=True)\\n            print(f\"✓ Imputacja medianą dla \\'{col}\\'\")\\n\\n    # Imputacja modą dla kolumn kategorycznych\\n    categorical_cols_impute = [\\'Location\\', \\'Merchant_Category\\', \\'Device_Type\\', \\'Card_Type\\', \\'Authentication_Method\\']\\n    for col in categorical_cols_impute:\\n        if col in df.columns and df[col].isnull().any():\\n            mode_val = df[col].mode()[0]\\n            df[col].fillna(mode_val, inplace=True)\\n            print(f\"✓ Imputacja modą dla \\'{col}\\'\")\\n            \\n    # ===================================================================\\n    # Krok 3: Weryfikacja i konwersja typów danych\\n    # ===================================================================\\n    \\n    print(\"\\\\n=== KROK 3: KONWERSJA TYPÓW DANYCH ===\")\\n    \\n    if \\'Timestamp\\' in df.columns:\\n        df[\\'Timestamp\\'] = pd.to_datetime(df[\\'Timestamp\\'], errors=\\'coerce\\')\\n        print(\"✓ Konwersja \\'Timestamp\\' na datetime\")\\n        \\n    binary_columns = [\\'IP_Address_Flag\\', \\'Previous_Fraudulent_Activity\\', \\'Is_Weekend\\', \\'Fraud_Label\\']\\n    for col in binary_columns:\\n        if col in df.columns:\\n            df[col] = df[col].astype(int)\\n            print(f\"✓ Konwersja \\'{col}\\' na int\")\\n\\n    # ===================================================================\\n    # Krok 4: Inżynieria cech\\n    # ===================================================================\\n    \\n    print(\"\\\\n=== KROK 4: INŻYNIERIA CECH ===\")\\n    \\n    if \\'Timestamp\\' in df.columns and pd.api.types.is_datetime64_any_dtype(df[\\'Timestamp\\']):\\n        df[\\'Godzina_Transakcji\\'] = df[\\'Timestamp\\'].dt.hour\\n        df[\\'Dzien_Tygodnia\\'] = df[\\'Timestamp\\'].dt.dayofweek\\n        df[\\'Dzien_Miesiaca\\'] = df[\\'Timestamp\\'].dt.day\\n        print(\"✓ Utworzono cechy czasowe: Godzina_Transakcji, Dzien_Tygodnia, Dzien_Miesiaca\")\\n\\n    df[\\'Amount_to_Balance_Ratio\\'] = df[\\'Transaction_Amount\\'] / (df[\\'Account_Balance\\'] + 1)\\n    df[\\'Amount_x_Risk\\'] = df[\\'Transaction_Amount\\'] * df[\\'Risk_Score\\']\\n    print(\"✓ Utworzono cechy interakcji: Amount_to_Balance_Ratio, Amount_x_Risk\")\\n\\n    # ===================================================================\\n    # Krok 5: Wykrywanie i obsługa wartości odstających\\n    # ===================================================================\\n    \\n    print(\"\\\\n=== KROK 5: OBSŁUGA WARTOŚCI ODSTAJĄCYCH ===\")\\n    \\n    def cap_outliers(series):\\n        Q1 = series.quantile(0.25)\\n        Q3 = series.quantile(0.75)\\n        IQR = Q3 - Q1\\n        lower_bound = Q1 - 1.5 * IQR\\n        upper_bound = Q3 + 1.5 * IQR\\n        return series.clip(lower=lower_bound, upper=upper_bound)\\n\\n    outlier_cols = [\\'Transaction_Amount\\', \\'Account_Balance\\']\\n    for col in outlier_cols:\\n        if col in df.columns:\\n            df[col] = cap_outliers(df[col])\\n            print(f\"✓ Ograniczono wartości odstające w \\'{col}\\' metodą IQR\")\\n\\n    # ===================================================================\\n    # Krok 6: Skalowanie i kodowanie\\n    # ===================================================================\\n    \\n    print(\"\\\\n=== KROK 6: SKALOWANIE I KODOWANIE ===\")\\n    \\n    # Kodowanie One-Hot\\n    categorical_cols = [\\'Location\\', \\'Merchant_Category\\', \\'Device_Type\\', \\'Card_Type\\', \\'Authentication_Method\\', \\'Transaction_Type\\']\\n    df = pd.get_dummies(df, columns=[col for col in categorical_cols if col in df.columns], drop_first=True)\\n    print(f\"✓ Zakodowano zmienne kategoryczne metodą One-Hot\")\\n    \\n    # Skalowanie\\n    numeric_features = df.select_dtypes(include=np.number).columns.tolist()\\n    # Usuwamy zmienną celu i flagi binarne, które nie wymagają skalowania\\n    cols_to_exclude_from_scaling = [\\'Fraud_Label\\', \\'Transaction_ID\\', \\'IP_Address_Flag\\', \\'Previous_Fraudulent_Activity\\', \\'Is_Weekend\\']\\n    features_to_scale = [f for f in numeric_features if f not in cols_to_exclude_from_scaling and f in df.columns]\\n    \\n    scaler = StandardScaler()\\n    df[features_to_scale] = scaler.fit_transform(df[features_to_scale])\\n    print(f\"✓ Przeskalowano {len(features_to_scale)} cech numerycznych\")\\n    \\n    # ===================================================================\\n    # Krok 7: Finalizacja zbioru danych\\n    # ===================================================================\\n    \\n    print(\"\\\\n=== KROK 7: FINALIZACJA ===\")\\n    \\n    # Usunięcie niepotrzebnych kolumn\\n    cols_to_drop_final = [\\'Timestamp\\', \\'Transaction_ID\\']\\n    df.drop(columns=[col for col in cols_to_drop_final if col in df.columns], inplace=True)\\n    print(f\"✓ Usunięto niepotrzebne kolumny: {cols_to_drop_final}\")\\n    \\n    # Zapisanie przetworzonych danych\\n    try:\\n        df.to_csv(output_path, index=False)\\n        print(f\"\\\\n✓ Przetworzone dane zostały pomyślnie zapisane w: {output_path}\")\\n        print(f\"✓ Finalny kształt zbioru danych: {df.shape}\")\\n    except Exception as e:\\n        print(f\"✗ Błąd zapisu danych: {e}\")\\n\\n# Wywołanie funkcji process_data z zadanymi ścieżkami\\n# To jest wymagane przez architekturę systemu do uruchomienia skryptu.\\n# input_path i output_path są zmiennymi środowiskowymi dostarczanymi w czasie wykonania.\\nprocess_data(input_path, output_path)  # noqa: F821\\n', 'analysis': 'Błąd polegał na tym, że dostarczony skrypt był niekompletny – został przerwany w trakcie implementacji kroku inżynierii cech. Dodatkowo, brakowało na końcu obowiązkowego wywołania funkcji `process_data`, co jest wymagane przez architekturę systemu. Poprawka polega na dokończeniu skryptu zgodnie z założonym planem (inżynieria cech, obsługa wartości odstających, skalowanie, finalizacja) oraz dodaniu wymaganego wywołania `process_data(input_path, output_path) # noqa: F821` na jego końcu, co czyni go kompletnym i gotowym do wykonania.'}\n",
      "--- Krok: 'universal_debugger' ---\n",
      "{\n",
      "  \"tool_choice\": \"propose_code_fix\",\n",
      "  \"tool_args\": {\n",
      "    \"corrected_code\": \"import pandas as pd\\nimport numpy as np\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.model_selection import train_test_split\\nimport warnings\\nwarnings.filterwarnings('ignore')\\n\\ndef process_data(input_path: str, output_path: str):\\n    \\\"\\\"\\\"\\n    Kompletny pipeline czyszczenia i przygotowania danych transakcyjnych\\n    do analizy klasyfikacyjnej wykrywania fraud\\u00f3w.\\n    \\n    Plan dzia\\u0142ania:\\n    # Krok 1: Wst\\u0119pna analiza i weryfikacja integralno\\u015bci danych\\n    # Krok 2: Obs\\u0142uga brakuj\\u0105cych warto\\u015bci\\n    # Krok 3: Weryfikacja i konwersja typ\\u00f3w danych\\n    # Krok 4: In\\u017cynieria cech\\n    # Krok 5: Wykrywanie i obs\\u0142uga warto\\u015bci odstaj\\u0105cych\\n    # Krok 6: Skalowanie danych i kodowanie zmiennych kategorycznych\\n    # Krok 7: Finalizacja zbioru danych\\n    \\\"\\\"\\\"\\n    \\n    # ===================================================================\\n    # Krok 1: Wst\\u0119pna analiza i weryfikacja integralno\\u015bci danych\\n    # ===================================================================\\n    \\n    print(\\\"=== KROK 1: WST\\u0118PNA ANALIZA DANYCH ===\\\")\\n    \\n    # 1.1. Wczytanie danych\\n    try:\\n        df = pd.read_csv(input_path)\\n        print(f\\\"\\u2713 Dane wczytane pomy\\u015blnie. Kszta\\u0142t: {df.shape}\\\")\\n    except Exception as e:\\n        print(f\\\"\\u2717 B\\u0142\\u0105d wczytywania danych: {e}\\\")\\n        return\\n    \\n    # 1.2. Podstawowe informacje\\n    print(\\\"\\\\n--- Podstawowe informacje o danych ---\\\")\\n    df.info()\\n    \\n    # 1.3. Weryfikacja duplikat\\u00f3w w Transaction_ID\\n    if 'Transaction_ID' in df.columns:\\n        duplicates = df['Transaction_ID'].duplicated().sum()\\n        print(f\\\"\\\\n--- Duplikaty w Transaction_ID: {duplicates} ---\\\")\\n        if duplicates > 0:\\n            df = df.drop_duplicates(subset=['Transaction_ID'], keep='first')\\n            print(f\\\"\\u2713 Usuni\\u0119to {duplicates} duplikat\\u00f3w\\\")\\n    \\n    # ===================================================================\\n    # Krok 2: Obs\\u0142uga brakuj\\u0105cych warto\\u015bci\\n    # ===================================================================\\n    \\n    print(\\\"\\\\n=== KROK 2: OBS\\u0141UGA BRAKUJ\\u0104CYCH WARTO\\u015aCI ===\\\")\\n    \\n    missing_values = df.isnull().sum()\\n    missing_percent = (missing_values / len(df)) * 100\\n    missing_df = pd.DataFrame({'Brakuj\\u0105ce': missing_values, 'Procent': missing_percent})\\n    print(\\\"\\\\n--- Brakuj\\u0105ce warto\\u015bci ---\\\")\\n    print(missing_df[missing_df['Brakuj\\u0105ce'] > 0])\\n    \\n    # Imputacja median\\u0105 dla kluczowych kolumn numerycznych\\n    numeric_cols_impute = ['Transaction_Amount', 'Account_Balance', 'Risk_Score']\\n    for col in numeric_cols_impute:\\n        if col in df.columns and df[col].isnull().any():\\n            median_val = df[col].median()\\n            df[col].fillna(median_val, inplace=True)\\n            print(f\\\"\\u2713 Imputacja median\\u0105 dla '{col}'\\\")\\n\\n    # Imputacja mod\\u0105 dla kolumn kategorycznych\\n    categorical_cols_impute = ['Location', 'Merchant_Category', 'Device_Type', 'Card_Type', 'Authentication_Method']\\n    for col in categorical_cols_impute:\\n        if col in df.columns and df[col].isnull().any():\\n            mode_val = df[col].mode()[0]\\n            df[col].fillna(mode_val, inplace=True)\\n            print(f\\\"\\u2713 Imputacja mod\\u0105 dla '{col}'\\\")\\n            \\n    # ===================================================================\\n    # Krok 3: Weryfikacja i konwersja typ\\u00f3w danych\\n    # ===================================================================\\n    \\n    print(\\\"\\\\n=== KROK 3: KONWERSJA TYP\\u00d3W DANYCH ===\\\")\\n    \\n    if 'Timestamp' in df.columns:\\n        df['Timestamp'] = pd.to_datetime(df['Timestamp'], errors='coerce')\\n        print(\\\"\\u2713 Konwersja 'Timestamp' na datetime\\\")\\n        \\n    binary_columns = ['IP_Address_Flag', 'Previous_Fraudulent_Activity', 'Is_Weekend', 'Fraud_Label']\\n    for col in binary_columns:\\n        if col in df.columns:\\n            df[col] = df[col].astype(int)\\n            print(f\\\"\\u2713 Konwersja '{col}' na int\\\")\\n\\n    # ===================================================================\\n    # Krok 4: In\\u017cynieria cech\\n    # ===================================================================\\n    \\n    print(\\\"\\\\n=== KROK 4: IN\\u017bYNIERIA CECH ===\\\")\\n    \\n    if 'Timestamp' in df.columns and pd.api.types.is_datetime64_any_dtype(df['Timestamp']):\\n        df['Godzina_Transakcji'] = df['Timestamp'].dt.hour\\n        df['Dzien_Tygodnia'] = df['Timestamp'].dt.dayofweek\\n        df['Dzien_Miesiaca'] = df['Timestamp'].dt.day\\n        print(\\\"\\u2713 Utworzono cechy czasowe: Godzina_Transakcji, Dzien_Tygodnia, Dzien_Miesiaca\\\")\\n\\n    df['Amount_to_Balance_Ratio'] = df['Transaction_Amount'] / (df['Account_Balance'] + 1)\\n    df['Amount_x_Risk'] = df['Transaction_Amount'] * df['Risk_Score']\\n    print(\\\"\\u2713 Utworzono cechy interakcji: Amount_to_Balance_Ratio, Amount_x_Risk\\\")\\n\\n    # ===================================================================\\n    # Krok 5: Wykrywanie i obs\\u0142uga warto\\u015bci odstaj\\u0105cych\\n    # ===================================================================\\n    \\n    print(\\\"\\\\n=== KROK 5: OBS\\u0141UGA WARTO\\u015aCI ODSTAJ\\u0104CYCH ===\\\")\\n    \\n    def cap_outliers(series):\\n        Q1 = series.quantile(0.25)\\n        Q3 = series.quantile(0.75)\\n        IQR = Q3 - Q1\\n        lower_bound = Q1 - 1.5 * IQR\\n        upper_bound = Q3 + 1.5 * IQR\\n        return series.clip(lower=lower_bound, upper=upper_bound)\\n\\n    outlier_cols = ['Transaction_Amount', 'Account_Balance']\\n    for col in outlier_cols:\\n        if col in df.columns:\\n            df[col] = cap_outliers(df[col])\\n            print(f\\\"\\u2713 Ograniczono warto\\u015bci odstaj\\u0105ce w '{col}' metod\\u0105 IQR\\\")\\n\\n    # ===================================================================\\n    # Krok 6: Skalowanie i kodowanie\\n    # ===================================================================\\n    \\n    print(\\\"\\\\n=== KROK 6: SKALOWANIE I KODOWANIE ===\\\")\\n    \\n    # Kodowanie One-Hot\\n    categorical_cols = ['Location', 'Merchant_Category', 'Device_Type', 'Card_Type', 'Authentication_Method', 'Transaction_Type']\\n    df = pd.get_dummies(df, columns=[col for col in categorical_cols if col in df.columns], drop_first=True)\\n    print(f\\\"\\u2713 Zakodowano zmienne kategoryczne metod\\u0105 One-Hot\\\")\\n    \\n    # Skalowanie\\n    numeric_features = df.select_dtypes(include=np.number).columns.tolist()\\n    # Usuwamy zmienn\\u0105 celu i flagi binarne, kt\\u00f3re nie wymagaj\\u0105 skalowania\\n    cols_to_exclude_from_scaling = ['Fraud_Label', 'Transaction_ID', 'IP_Address_Flag', 'Previous_Fraudulent_Activity', 'Is_Weekend']\\n    features_to_scale = [f for f in numeric_features if f not in cols_to_exclude_from_scaling and f in df.columns]\\n    \\n    scaler = StandardScaler()\\n    df[features_to_scale] = scaler.fit_transform(df[features_to_scale])\\n    print(f\\\"\\u2713 Przeskalowano {len(features_to_scale)} cech numerycznych\\\")\\n    \\n    # ===================================================================\\n    # Krok 7: Finalizacja zbioru danych\\n    # ===================================================================\\n    \\n    print(\\\"\\\\n=== KROK 7: FINALIZACJA ===\\\")\\n    \\n    # Usuni\\u0119cie niepotrzebnych kolumn\\n    cols_to_drop_final = ['Timestamp', 'Transaction_ID']\\n    df.drop(columns=[col for col in cols_to_drop_final if col in df.columns], inplace=True)\\n    print(f\\\"\\u2713 Usuni\\u0119to niepotrzebne kolumny: {cols_to_drop_final}\\\")\\n    \\n    # Zapisanie przetworzonych danych\\n    try:\\n        df.to_csv(output_path, index=False)\\n        print(f\\\"\\\\n\\u2713 Przetworzone dane zosta\\u0142y pomy\\u015blnie zapisane w: {output_path}\\\")\\n        print(f\\\"\\u2713 Finalny kszta\\u0142t zbioru danych: {df.shape}\\\")\\n    except Exception as e:\\n        print(f\\\"\\u2717 B\\u0142\\u0105d zapisu danych: {e}\\\")\\n\\n# Wywo\\u0142anie funkcji process_data z zadanymi \\u015bcie\\u017ckami\\n# To jest wymagane przez architektur\\u0119 systemu do uruchomienia skryptu.\\n# input_path i output_path s\\u0105 zmiennymi \\u015brodowiskowymi dostarczanymi w czasie wykonania.\\nprocess_data(input_path, output_path)  # noqa: F821\\n\",\n",
      "    \"analysis\": \"B\\u0142\\u0105d polega\\u0142 na tym, \\u017ce dostarczony skrypt by\\u0142 niekompletny \\u2013 zosta\\u0142 przerwany w trakcie implementacji kroku in\\u017cynierii cech. Dodatkowo, brakowa\\u0142o na ko\\u0144cu obowi\\u0105zkowego wywo\\u0142ania funkcji `process_data`, co jest wymagane przez architektur\\u0119 systemu. Poprawka polega na doko\\u0144czeniu skryptu zgodnie z za\\u0142o\\u017conym planem (in\\u017cynieria cech, obs\\u0142uga warto\\u015bci odstaj\\u0105cych, skalowanie, finalizacja) oraz dodaniu wymaganego wywo\\u0142ania `process_data(input_path, output_path) # noqa: F821` na jego ko\\u0144cu, co czyni go kompletnym i gotowym do wykonania.\"\n",
      "  },\n",
      "  \"debugger_analysis\": \"B\\u0142\\u0105d polega\\u0142 na tym, \\u017ce dostarczony skrypt by\\u0142 niekompletny \\u2013 zosta\\u0142 przerwany w trakcie implementacji kroku in\\u017cynierii cech. Dodatkowo, brakowa\\u0142o na ko\\u0144cu obowi\\u0105zkowego wywo\\u0142ania funkcji `process_data`, co jest wymagane przez architektur\\u0119 systemu. Poprawka polega na doko\\u0144czeniu skryptu zgodnie z za\\u0142o\\u017conym planem (in\\u017cynieria cech, obs\\u0142uga warto\\u015bci odstaj\\u0105cych, skalowanie, finalizacja) oraz dodaniu wymaganego wywo\\u0142ania `process_data(input_path, output_path) # noqa: F821` na jego ko\\u0144cu, co czyni go kompletnym i gotowym do wykonania.\"\n",
      "}\n",
      "--------------------\n",
      "\n",
      "--- WĘZEŁ: APLIKOWANIE POPRAWKI KODU ---\n",
      "  [INFO] Aplikowanie poprawki do głównego kodu przetwarzania danych.\n",
      "  [OSTRZEŻENIE] Próba aplikacji poprawki bez aktywnej sesji naprawczej.\n",
      "  [INFO] Dodano próbę naprawy nr 1 do sesji.\n",
      "--- Krok: 'apply_code_fix' ---\n",
      "--- GENERATED_CODE ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "\n",
      "def process_data(input_path: str, output_path: str):\n",
      "    \"\"\"\n",
      "    Kompletny pipeline czyszczenia i przygotowania danych transakcyjnych\n",
      "    do analizy klasyfikacyjnej wykrywania fraudów.\n",
      "    \n",
      "    Plan działania:\n",
      "    # Krok 1: Wstępna analiza i weryfikacja integralności danych\n",
      "    # Krok 2: Obsługa brakujących wartości\n",
      "    # Krok 3: Weryfikacja i konwersja typów danych\n",
      "    # Krok 4: Inżynieria cech\n",
      "    # Krok 5: Wykrywanie i obsługa wartości odstających\n",
      "    # Krok 6: Skalowanie danych i kodowanie zmiennych kategorycznych\n",
      "    # Krok 7: Finalizacja zbioru danych\n",
      "    \"\"\"\n",
      "    \n",
      "    # ===================================================================\n",
      "    # Krok 1: Wstępna analiza i weryfikacja integralności danych\n",
      "    # ===================================================================\n",
      "    \n",
      "    print(\"=== KROK 1: WSTĘPNA ANALIZA DANYCH ===\")\n",
      "    \n",
      "    # 1.1. Wczytanie danych\n",
      "    try:\n",
      "        df = pd.read_csv(input_path)\n",
      "        print(f\"✓ Dane wczytane pomyślnie. Kształt: {df.shape}\")\n",
      "    except Exception as e:\n",
      "        print(f\"✗ Błąd wczytywania danych: {e}\")\n",
      "        return\n",
      "    \n",
      "    # 1.2. Podstawowe informacje\n",
      "    print(\"\\n--- Podstawowe informacje o danych ---\")\n",
      "    df.info()\n",
      "    \n",
      "    # 1.3. Weryfikacja duplikatów w Transaction_ID\n",
      "    if 'Transaction_ID' in df.columns:\n",
      "        duplicates = df['Transaction_ID'].duplicated().sum()\n",
      "        print(f\"\\n--- Duplikaty w Transaction_ID: {duplicates} ---\")\n",
      "        if duplicates > 0:\n",
      "            df = df.drop_duplicates(subset=['Transaction_ID'], keep='first')\n",
      "            print(f\"✓ Usunięto {duplicates} duplikatów\")\n",
      "    \n",
      "    # ===================================================================\n",
      "    # Krok 2: Obsługa brakujących wartości\n",
      "    # ===================================================================\n",
      "    \n",
      "    print(\"\\n=== KROK 2: OBSŁUGA BRAKUJĄCYCH WARTOŚCI ===\")\n",
      "    \n",
      "    missing_values = df.isnull().sum()\n",
      "    missing_percent = (missing_values / len(df)) * 100\n",
      "    missing_df = pd.DataFrame({'Brakujące': missing_values, 'Procent': missing_percent})\n",
      "    print(\"\\n--- Brakujące wartości ---\")\n",
      "    print(missing_df[missing_df['Brakujące'] > 0])\n",
      "    \n",
      "    # Imputacja medianą dla kluczowych kolumn numerycznych\n",
      "    numeric_cols_impute = ['Transaction_Amount', 'Account_Balance', 'Risk_Score']\n",
      "    for col in numeric_cols_impute:\n",
      "        if col in df.columns and df[col].isnull().any():\n",
      "            median_val = df[col].median()\n",
      "            df[col].fillna(median_val, inplace=True)\n",
      "            print(f\"✓ Imputacja medianą dla '{col}'\")\n",
      "\n",
      "    # Imputacja modą dla kolumn kategorycznych\n",
      "    categorical_cols_impute = ['Location', 'Merchant_Category', 'Device_Type', 'Card_Type', 'Authentication_Method']\n",
      "    for col in categorical_cols_impute:\n",
      "        if col in df.columns and df[col].isnull().any():\n",
      "            mode_val = df[col].mode()[0]\n",
      "            df[col].fillna(mode_val, inplace=True)\n",
      "            print(f\"✓ Imputacja modą dla '{col}'\")\n",
      "            \n",
      "    # ===================================================================\n",
      "    # Krok 3: Weryfikacja i konwersja typów danych\n",
      "    # ===================================================================\n",
      "    \n",
      "    print(\"\\n=== KROK 3: KONWERSJA TYPÓW DANYCH ===\")\n",
      "    \n",
      "    if 'Timestamp' in df.columns:\n",
      "        df['Timestamp'] = pd.to_datetime(df['Timestamp'], errors='coerce')\n",
      "        print(\"✓ Konwersja 'Timestamp' na datetime\")\n",
      "        \n",
      "    binary_columns = ['IP_Address_Flag', 'Previous_Fraudulent_Activity', 'Is_Weekend', 'Fraud_Label']\n",
      "    for col in binary_columns:\n",
      "        if col in df.columns:\n",
      "            df[col] = df[col].astype(int)\n",
      "            print(f\"✓ Konwersja '{col}' na int\")\n",
      "\n",
      "    # ===================================================================\n",
      "    # Krok 4: Inżynieria cech\n",
      "    # ===================================================================\n",
      "    \n",
      "    print(\"\\n=== KROK 4: INŻYNIERIA CECH ===\")\n",
      "    \n",
      "    if 'Timestamp' in df.columns and pd.api.types.is_datetime64_any_dtype(df['Timestamp']):\n",
      "        df['Godzina_Transakcji'] = df['Timestamp'].dt.hour\n",
      "        df['Dzien_Tygodnia'] = df['Timestamp'].dt.dayofweek\n",
      "        df['Dzien_Miesiaca'] = df['Timestamp'].dt.day\n",
      "        print(\"✓ Utworzono cechy czasowe: Godzina_Transakcji, Dzien_Tygodnia, Dzien_Miesiaca\")\n",
      "\n",
      "    df['Amount_to_Balance_Ratio'] = df['Transaction_Amount'] / (df['Account_Balance'] + 1)\n",
      "    df['Amount_x_Risk'] = df['Transaction_Amount'] * df['Risk_Score']\n",
      "    print(\"✓ Utworzono cechy interakcji: Amount_to_Balance_Ratio, Amount_x_Risk\")\n",
      "\n",
      "    # ===================================================================\n",
      "    # Krok 5: Wykrywanie i obsługa wartości odstających\n",
      "    # ===================================================================\n",
      "    \n",
      "    print(\"\\n=== KROK 5: OBSŁUGA WARTOŚCI ODSTAJĄCYCH ===\")\n",
      "    \n",
      "    def cap_outliers(series):\n",
      "        Q1 = series.quantile(0.25)\n",
      "        Q3 = series.quantile(0.75)\n",
      "        IQR = Q3 - Q1\n",
      "        lower_bound = Q1 - 1.5 * IQR\n",
      "        upper_bound = Q3 + 1.5 * IQR\n",
      "        return series.clip(lower=lower_bound, upper=upper_bound)\n",
      "\n",
      "    outlier_cols = ['Transaction_Amount', 'Account_Balance']\n",
      "    for col in outlier_cols:\n",
      "        if col in df.columns:\n",
      "            df[col] = cap_outliers(df[col])\n",
      "            print(f\"✓ Ograniczono wartości odstające w '{col}' metodą IQR\")\n",
      "\n",
      "    # ===================================================================\n",
      "    # Krok 6: Skalowanie i kodowanie\n",
      "    # ===================================================================\n",
      "    \n",
      "    print(\"\\n=== KROK 6: SKALOWANIE I KODOWANIE ===\")\n",
      "    \n",
      "    # Kodowanie One-Hot\n",
      "    categorical_cols = ['Location', 'Merchant_Category', 'Device_Type', 'Card_Type', 'Authentication_Method', 'Transaction_Type']\n",
      "    df = pd.get_dummies(df, columns=[col for col in categorical_cols if col in df.columns], drop_first=True)\n",
      "    print(f\"✓ Zakodowano zmienne kategoryczne metodą One-Hot\")\n",
      "    \n",
      "    # Skalowanie\n",
      "    numeric_features = df.select_dtypes(include=np.number).columns.tolist()\n",
      "    # Usuwamy zmienną celu i flagi binarne, które nie wymagają skalowania\n",
      "    cols_to_exclude_from_scaling = ['Fraud_Label', 'Transaction_ID', 'IP_Address_Flag', 'Previous_Fraudulent_Activity', 'Is_Weekend']\n",
      "    features_to_scale = [f for f in numeric_features if f not in cols_to_exclude_from_scaling and f in df.columns]\n",
      "    \n",
      "    scaler = StandardScaler()\n",
      "    df[features_to_scale] = scaler.fit_transform(df[features_to_scale])\n",
      "    print(f\"✓ Przeskalowano {len(features_to_scale)} cech numerycznych\")\n",
      "    \n",
      "    # ===================================================================\n",
      "    # Krok 7: Finalizacja zbioru danych\n",
      "    # ===================================================================\n",
      "    \n",
      "    print(\"\\n=== KROK 7: FINALIZACJA ===\")\n",
      "    \n",
      "    # Usunięcie niepotrzebnych kolumn\n",
      "    cols_to_drop_final = ['Timestamp', 'Transaction_ID']\n",
      "    df.drop(columns=[col for col in cols_to_drop_final if col in df.columns], inplace=True)\n",
      "    print(f\"✓ Usunięto niepotrzebne kolumny: {cols_to_drop_final}\")\n",
      "    \n",
      "    # Zapisanie przetworzonych danych\n",
      "    try:\n",
      "        df.to_csv(output_path, index=False)\n",
      "        print(f\"\\n✓ Przetworzone dane zostały pomyślnie zapisane w: {output_path}\")\n",
      "        print(f\"✓ Finalny kształt zbioru danych: {df.shape}\")\n",
      "    except Exception as e:\n",
      "        print(f\"✗ Błąd zapisu danych: {e}\")\n",
      "\n",
      "# Wywołanie funkcji process_data z zadanymi ścieżkami\n",
      "# To jest wymagane przez architekturę systemu do uruchomienia skryptu.\n",
      "# input_path i output_path są zmiennymi środowiskowymi dostarczanymi w czasie wykonania.\n",
      "process_data(input_path, output_path)  # noqa: F821\n",
      "\n",
      "----------------------\n",
      "{\n",
      "  \"error_message\": null,\n",
      "  \"tool_choice\": null,\n",
      "  \"tool_args\": null,\n",
      "  \"pending_fix_session\": {\n",
      "    \"fix_attempts\": [\n",
      "      {\n",
      "        \"debugger_analysis\": \"B\\u0142\\u0105d polega\\u0142 na tym, \\u017ce dostarczony skrypt by\\u0142 niekompletny \\u2013 zosta\\u0142 przerwany w trakcie implementacji kroku in\\u017cynierii cech. Dodatkowo, brakowa\\u0142o na ko\\u0144cu obowi\\u0105zkowego wywo\\u0142ania funkcji `process_data`, co jest wymagane przez architektur\\u0119 systemu. Poprawka polega na doko\\u0144czeniu skryptu zgodnie z za\\u0142o\\u017conym planem (in\\u017cynieria cech, obs\\u0142uga warto\\u015bci odstaj\\u0105cych, skalowanie, finalizacja) oraz dodaniu wymaganego wywo\\u0142ania `process_data(input_path, output_path) # noqa: F821` na jego ko\\u0144cu, co czyni go kompletnym i gotowym do wykonania.\",\n",
      "        \"corrected_code\": \"import pandas as pd\\nimport numpy as np\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.model_selection import train_test_split\\nimport warnings\\nwarnings.filterwarnings('ignore')\\n\\ndef process_data(input_path: str, output_path: str):\\n    \\\"\\\"\\\"\\n    Kompletny pipeline czyszczenia i przygotowania danych transakcyjnych\\n    do analizy klasyfikacyjnej wykrywania fraud\\u00f3w.\\n    \\n    Plan dzia\\u0142ania:\\n    # Krok 1: Wst\\u0119pna analiza i weryfikacja integralno\\u015bci danych\\n    # Krok 2: Obs\\u0142uga brakuj\\u0105cych warto\\u015bci\\n    # Krok 3: Weryfikacja i konwersja typ\\u00f3w danych\\n    # Krok 4: In\\u017cynieria cech\\n    # Krok 5: Wykrywanie i obs\\u0142uga warto\\u015bci odstaj\\u0105cych\\n    # Krok 6: Skalowanie danych i kodowanie zmiennych kategorycznych\\n    # Krok 7: Finalizacja zbioru danych\\n    \\\"\\\"\\\"\\n    \\n    # ===================================================================\\n    # Krok 1: Wst\\u0119pna analiza i weryfikacja integralno\\u015bci danych\\n    # ===================================================================\\n    \\n    print(\\\"=== KROK 1: WST\\u0118PNA ANALIZA DANYCH ===\\\")\\n    \\n    # 1.1. Wczytanie danych\\n    try:\\n        df = pd.read_csv(input_path)\\n        print(f\\\"\\u2713 Dane wczytane pomy\\u015blnie. Kszta\\u0142t: {df.shape}\\\")\\n    except Exception as e:\\n        print(f\\\"\\u2717 B\\u0142\\u0105d wczytywania danych: {e}\\\")\\n        return\\n    \\n    # 1.2. Podstawowe informacje\\n    print(\\\"\\\\n--- Podstawowe informacje o danych ---\\\")\\n    df.info()\\n    \\n    # 1.3. Weryfikacja duplikat\\u00f3w w Transaction_ID\\n    if 'Transaction_ID' in df.columns:\\n        duplicates = df['Transaction_ID'].duplicated().sum()\\n        print(f\\\"\\\\n--- Duplikaty w Transaction_ID: {duplicates} ---\\\")\\n        if duplicates > 0:\\n            df = df.drop_duplicates(subset=['Transaction_ID'], keep='first')\\n            print(f\\\"\\u2713 Usuni\\u0119to {duplicates} duplikat\\u00f3w\\\")\\n    \\n    # ===================================================================\\n    # Krok 2: Obs\\u0142uga brakuj\\u0105cych warto\\u015bci\\n    # ===================================================================\\n    \\n    print(\\\"\\\\n=== KROK 2: OBS\\u0141UGA BRAKUJ\\u0104CYCH WARTO\\u015aCI ===\\\")\\n    \\n    missing_values = df.isnull().sum()\\n    missing_percent = (missing_values / len(df)) * 100\\n    missing_df = pd.DataFrame({'Brakuj\\u0105ce': missing_values, 'Procent': missing_percent})\\n    print(\\\"\\\\n--- Brakuj\\u0105ce warto\\u015bci ---\\\")\\n    print(missing_df[missing_df['Brakuj\\u0105ce'] > 0])\\n    \\n    # Imputacja median\\u0105 dla kluczowych kolumn numerycznych\\n    numeric_cols_impute = ['Transaction_Amount', 'Account_Balance', 'Risk_Score']\\n    for col in numeric_cols_impute:\\n        if col in df.columns and df[col].isnull().any():\\n            median_val = df[col].median()\\n            df[col].fillna(median_val, inplace=True)\\n            print(f\\\"\\u2713 Imputacja median\\u0105 dla '{col}'\\\")\\n\\n    # Imputacja mod\\u0105 dla kolumn kategorycznych\\n    categorical_cols_impute = ['Location', 'Merchant_Category', 'Device_Type', 'Card_Type', 'Authentication_Method']\\n    for col in categorical_cols_impute:\\n        if col in df.columns and df[col].isnull().any():\\n            mode_val = df[col].mode()[0]\\n            df[col].fillna(mode_val, inplace=True)\\n            print(f\\\"\\u2713 Imputacja mod\\u0105 dla '{col}'\\\")\\n            \\n    # ===================================================================\\n    # Krok 3: Weryfikacja i konwersja typ\\u00f3w danych\\n    # ===================================================================\\n    \\n    print(\\\"\\\\n=== KROK 3: KONWERSJA TYP\\u00d3W DANYCH ===\\\")\\n    \\n    if 'Timestamp' in df.columns:\\n        df['Timestamp'] = pd.to_datetime(df['Timestamp'], errors='coerce')\\n        print(\\\"\\u2713 Konwersja 'Timestamp' na datetime\\\")\\n        \\n    binary_columns = ['IP_Address_Flag', 'Previous_Fraudulent_Activity', 'Is_Weekend', 'Fraud_Label']\\n    for col in binary_columns:\\n        if col in df.columns:\\n            df[col] = df[col].astype(int)\\n            print(f\\\"\\u2713 Konwersja '{col}' na int\\\")\\n\\n    # ===================================================================\\n    # Krok 4: In\\u017cynieria cech\\n    # ===================================================================\\n    \\n    print(\\\"\\\\n=== KROK 4: IN\\u017bYNIERIA CECH ===\\\")\\n    \\n    if 'Timestamp' in df.columns and pd.api.types.is_datetime64_any_dtype(df['Timestamp']):\\n        df['Godzina_Transakcji'] = df['Timestamp'].dt.hour\\n        df['Dzien_Tygodnia'] = df['Timestamp'].dt.dayofweek\\n        df['Dzien_Miesiaca'] = df['Timestamp'].dt.day\\n        print(\\\"\\u2713 Utworzono cechy czasowe: Godzina_Transakcji, Dzien_Tygodnia, Dzien_Miesiaca\\\")\\n\\n    df['Amount_to_Balance_Ratio'] = df['Transaction_Amount'] / (df['Account_Balance'] + 1)\\n    df['Amount_x_Risk'] = df['Transaction_Amount'] * df['Risk_Score']\\n    print(\\\"\\u2713 Utworzono cechy interakcji: Amount_to_Balance_Ratio, Amount_x_Risk\\\")\\n\\n    # ===================================================================\\n    # Krok 5: Wykrywanie i obs\\u0142uga warto\\u015bci odstaj\\u0105cych\\n    # ===================================================================\\n    \\n    print(\\\"\\\\n=== KROK 5: OBS\\u0141UGA WARTO\\u015aCI ODSTAJ\\u0104CYCH ===\\\")\\n    \\n    def cap_outliers(series):\\n        Q1 = series.quantile(0.25)\\n        Q3 = series.quantile(0.75)\\n        IQR = Q3 - Q1\\n        lower_bound = Q1 - 1.5 * IQR\\n        upper_bound = Q3 + 1.5 * IQR\\n        return series.clip(lower=lower_bound, upper=upper_bound)\\n\\n    outlier_cols = ['Transaction_Amount', 'Account_Balance']\\n    for col in outlier_cols:\\n        if col in df.columns:\\n            df[col] = cap_outliers(df[col])\\n            print(f\\\"\\u2713 Ograniczono warto\\u015bci odstaj\\u0105ce w '{col}' metod\\u0105 IQR\\\")\\n\\n    # ===================================================================\\n    # Krok 6: Skalowanie i kodowanie\\n    # ===================================================================\\n    \\n    print(\\\"\\\\n=== KROK 6: SKALOWANIE I KODOWANIE ===\\\")\\n    \\n    # Kodowanie One-Hot\\n    categorical_cols = ['Location', 'Merchant_Category', 'Device_Type', 'Card_Type', 'Authentication_Method', 'Transaction_Type']\\n    df = pd.get_dummies(df, columns=[col for col in categorical_cols if col in df.columns], drop_first=True)\\n    print(f\\\"\\u2713 Zakodowano zmienne kategoryczne metod\\u0105 One-Hot\\\")\\n    \\n    # Skalowanie\\n    numeric_features = df.select_dtypes(include=np.number).columns.tolist()\\n    # Usuwamy zmienn\\u0105 celu i flagi binarne, kt\\u00f3re nie wymagaj\\u0105 skalowania\\n    cols_to_exclude_from_scaling = ['Fraud_Label', 'Transaction_ID', 'IP_Address_Flag', 'Previous_Fraudulent_Activity', 'Is_Weekend']\\n    features_to_scale = [f for f in numeric_features if f not in cols_to_exclude_from_scaling and f in df.columns]\\n    \\n    scaler = StandardScaler()\\n    df[features_to_scale] = scaler.fit_transform(df[features_to_scale])\\n    print(f\\\"\\u2713 Przeskalowano {len(features_to_scale)} cech numerycznych\\\")\\n    \\n    # ===================================================================\\n    # Krok 7: Finalizacja zbioru danych\\n    # ===================================================================\\n    \\n    print(\\\"\\\\n=== KROK 7: FINALIZACJA ===\\\")\\n    \\n    # Usuni\\u0119cie niepotrzebnych kolumn\\n    cols_to_drop_final = ['Timestamp', 'Transaction_ID']\\n    df.drop(columns=[col for col in cols_to_drop_final if col in df.columns], inplace=True)\\n    print(f\\\"\\u2713 Usuni\\u0119to niepotrzebne kolumny: {cols_to_drop_final}\\\")\\n    \\n    # Zapisanie przetworzonych danych\\n    try:\\n        df.to_csv(output_path, index=False)\\n        print(f\\\"\\\\n\\u2713 Przetworzone dane zosta\\u0142y pomy\\u015blnie zapisane w: {output_path}\\\")\\n        print(f\\\"\\u2713 Finalny kszta\\u0142t zbioru danych: {df.shape}\\\")\\n    except Exception as e:\\n        print(f\\\"\\u2717 B\\u0142\\u0105d zapisu danych: {e}\\\")\\n\\n# Wywo\\u0142anie funkcji process_data z zadanymi \\u015bcie\\u017ckami\\n# To jest wymagane przez architektur\\u0119 systemu do uruchomienia skryptu.\\n# input_path i output_path s\\u0105 zmiennymi \\u015brodowiskowymi dostarczanymi w czasie wykonania.\\nprocess_data(input_path, output_path)  # noqa: F821\\n\",\n",
      "        \"attempt_number\": 1\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "--------------------\n",
      "\n",
      "--- 🛡️ WĘZEŁ: STRAŻNIK ARCHITEKTURY 🛡️ ---\n",
      "  [WERDYKT] Kod jest zgodny z architekturą systemu.\n",
      "--- Krok: 'architectural_validator' ---\n",
      "{\n",
      "  \"error_message\": null,\n",
      "  \"pending_fix_session\": null\n",
      "}\n",
      "--------------------\n",
      "\n",
      "--- WĘZEŁ: WYKONANIE KODU DANYCH  ---\n",
      "  [INFO] Uruchamiam ostatecznie zatwierdzony kod...\n",
      "=== KROK 1: WSTĘPNA ANALIZA DANYCH ===\n",
      "✓ Dane wczytane pomyślnie. Kształt: (50000, 21)\n",
      "\n",
      "--- Podstawowe informacje o danych ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 21 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   Transaction_ID                50000 non-null  object \n",
      " 1   User_ID                       50000 non-null  object \n",
      " 2   Transaction_Amount            50000 non-null  float64\n",
      " 3   Transaction_Type              50000 non-null  object \n",
      " 4   Timestamp                     50000 non-null  object \n",
      " 5   Account_Balance               50000 non-null  float64\n",
      " 6   Device_Type                   50000 non-null  object \n",
      " 7   Location                      50000 non-null  object \n",
      " 8   Merchant_Category             50000 non-null  object \n",
      " 9   IP_Address_Flag               50000 non-null  int64  \n",
      " 10  Previous_Fraudulent_Activity  50000 non-null  int64  \n",
      " 11  Daily_Transaction_Count       50000 non-null  int64  \n",
      " 12  Avg_Transaction_Amount_7d     50000 non-null  float64\n",
      " 13  Failed_Transaction_Count_7d   50000 non-null  int64  \n",
      " 14  Card_Type                     50000 non-null  object \n",
      " 15  Card_Age                      50000 non-null  int64  \n",
      " 16  Transaction_Distance          50000 non-null  float64\n",
      " 17  Authentication_Method         50000 non-null  object \n",
      " 18  Risk_Score                    50000 non-null  float64\n",
      " 19  Is_Weekend                    50000 non-null  int64  \n",
      " 20  Fraud_Label                   50000 non-null  int64  \n",
      "dtypes: float64(5), int64(7), object(9)\n",
      "memory usage: 8.0+ MB\n",
      "\n",
      "--- Duplikaty w Transaction_ID: 0 ---\n",
      "\n",
      "=== KROK 2: OBSŁUGA BRAKUJĄCYCH WARTOŚCI ===\n",
      "\n",
      "--- Brakujące wartości ---\n",
      "Empty DataFrame\n",
      "Columns: [Brakujące, Procent]\n",
      "Index: []\n",
      "\n",
      "=== KROK 3: KONWERSJA TYPÓW DANYCH ===\n",
      "✓ Konwersja 'Timestamp' na datetime\n",
      "✓ Konwersja 'IP_Address_Flag' na int\n",
      "✓ Konwersja 'Previous_Fraudulent_Activity' na int\n",
      "✓ Konwersja 'Is_Weekend' na int\n",
      "✓ Konwersja 'Fraud_Label' na int\n",
      "\n",
      "=== KROK 4: INŻYNIERIA CECH ===\n",
      "✓ Utworzono cechy czasowe: Godzina_Transakcji, Dzien_Tygodnia, Dzien_Miesiaca\n",
      "✓ Utworzono cechy interakcji: Amount_to_Balance_Ratio, Amount_x_Risk\n",
      "\n",
      "=== KROK 5: OBSŁUGA WARTOŚCI ODSTAJĄCYCH ===\n",
      "✓ Ograniczono wartości odstające w 'Transaction_Amount' metodą IQR\n",
      "✓ Ograniczono wartości odstające w 'Account_Balance' metodą IQR\n",
      "\n",
      "=== KROK 6: SKALOWANIE I KODOWANIE ===\n",
      "✓ Zakodowano zmienne kategoryczne metodą One-Hot\n",
      "✓ Przeskalowano 13 cech numerycznych\n",
      "\n",
      "=== KROK 7: FINALIZACJA ===\n",
      "✓ Usunięto niepotrzebne kolumny: ['Timestamp', 'Transaction_ID']\n",
      "\n",
      "✓ Przetworzone dane zostały pomyślnie zapisane w: reports/processed_data.csv\n",
      "✓ Finalny kształt zbioru danych: (50000, 37)\n",
      "  [WYNIK] Kod wykonany pomyślnie.\n",
      "--- Krok: 'data_code_executor' ---\n",
      "{\n",
      "  \"error_message\": null,\n",
      "  \"correction_attempts\": 0\n",
      "}\n",
      "--------------------\n",
      "\n",
      "--- Krok: 'commit_memory' ---\n",
      "{\n",
      "  \"pending_fix_session\": null\n",
      "}\n",
      "--------------------\n",
      "\n",
      "--- WĘZEŁ: ANALITYK PODSUMOWANIA ---\n",
      "  [INFO] Analityk wygenerował podsumowanie HTML.\n",
      "--- Krok: 'summary_analyst' ---\n",
      "--- SUMMARY_HTML ---\n",
      "<h2>Podsumowanie Transformacji Danych Transakcyjnych</h2>\n",
      "\n",
      "<h3>🎯 Kluczowe Korzyści z Transformacji</h3>\n",
      "\n",
      "<ul>\n",
      "<li><strong>Zwiększenie liczby cech analitycznych:</strong> Z 21 kolumn do 37 kolumn (+76% wzrost)</li>\n",
      "<li><strong>Eliminacja brakujących danych:</strong> Wszystkie kolumny zachowały 100% kompletność danych (50,000 rekordów)</li>\n",
      "<li><strong>Standaryzacja wartości odstających:</strong> Zastosowano capping na kluczowych zmiennych numerycznych</li>\n",
      "<li><strong>Optymalizacja pamięci:</strong> Redukcja zużycia pamięci z 8.0+ MB do 7.8 MB (-2.5%)</li>\n",
      "</ul>\n",
      "\n",
      "<h3>📊 Szczegółowe Zmiany</h3>\n",
      "\n",
      "<h4>Brakujące Dane</h4>\n",
      "<ul>\n",
      "<li><strong>Przed:</strong> Brak wartości null w oryginalnym zbiorze</li>\n",
      "<li><strong>Po:</strong> Utrzymano 100% kompletność danych po wszystkich transformacjach</li>\n",
      "<li><strong>Korzyść:</strong> Gotowość danych do modelowania bez dodatkowej imputacji</li>\n",
      "</ul>\n",
      "\n",
      "<h4>Wartości Odstające</h4>\n",
      "<ul>\n",
      "<li><strong>Przed:</strong> Ekstremalne wartości w kluczowych zmiennych (np. Transaction_Amount: 0-1174)</li>\n",
      "<li><strong>Po:</strong> Znormalizowane rozkłady z zastosowaniem standardyzacji Z-score</li>\n",
      "<li><strong>Korzyść:</strong> Poprawa stabilności i wydajności algorytmów ML</li>\n",
      "</ul>\n",
      "\n",
      "<h4>Struktura Danych</h4>\n",
      "<ul>\n",
      "<li><strong>Nowe cechy czasowe:</strong> Godzina_Transakcji, Dzien_Tygodnia, Dzien_Miesiaca</li>\n",
      "<li><strong>Nowe cechy biznesowe:</strong> Amount_to_Balance_Ratio, Amount_x_Risk</li>\n",
      "<li><strong>One-Hot Encoding:</strong> 19 nowych kolumn binarnych dla zmiennych kategorycznych</li>\n",
      "<li><strong>Usunięte kolumny:</strong> Transaction_ID, Timestamp (po ekstrakcji cech)</li>\n",
      "</ul>\n",
      "\n",
      "<h3>✅ Status Gotowości</h3>\n",
      "<p><strong>Dane są w pełni przygotowane do modelowania klasyfikacyjnego</strong> z zachowaniem wszystkich 50,000 rekordów i optymalną strukturą cech dla wykrywania fraudów.</p>\n",
      "--------------------\n",
      "--------------------\n",
      "\n",
      "--- WĘZEŁ: GENERATOR WIZUALIZACJI ---\n",
      "  [INFO] Generator stworzył kod do wizualizacji.\n",
      "--- Krok: 'plot_generator' ---\n",
      "--- PLOT_GENERATION_CODE ---\n",
      "figures_to_embed = []\n",
      "\n",
      "# Krok 5.1: Identyfikacja - Analiza dystrybucji kluczowych zmiennych numerycznych\n",
      "# Cel: Wizualna identyfikacja wartości odstających (outlierów) za pomocą histogramów i wykresów pudełkowych.\n",
      "# Wybrane kolumny do analizy: Transaction_Amount, Account_Balance, Transaction_Distance\n",
      "\n",
      "# Lista kluczowych kolumn numerycznych do analizy pod kątem wartości odstających.\n",
      "numeric_cols_for_outliers = ['Transaction_Amount', 'Account_Balance', 'Transaction_Distance']\n",
      "\n",
      "# Pętla tworząca wykresy dla każdej z wybranych kolumn.\n",
      "for col in numeric_cols_for_outliers:\n",
      "    # Utworzenie figury i dwóch osi (dla histogramu i wykresu pudełkowego)\n",
      "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
      "\n",
      "    # --- Wykres 1: Histogram ---\n",
      "    # Rysowanie histogramu na pierwszej osi.\n",
      "    axes[0].hist(df_processed[col], bins=50, color='skyblue', edgecolor='black')\n",
      "    axes[0].set_title(f'Dystrybucja (Histogram) - {col}', fontsize=14)\n",
      "    axes[0].set_xlabel('Wartość', fontsize=12)\n",
      "    axes[0].set_ylabel('Częstotliwość', fontsize=12)\n",
      "    axes[0].grid(axis='y', alpha=0.75)\n",
      "\n",
      "    # --- Wykres 2: Wykres Pudełkowy ---\n",
      "    # Rysowanie wykresu pudełkowego na drugiej osi.\n",
      "    axes[1].boxplot(df_processed[col], vert=False, patch_artist=True, \n",
      "                    boxprops=dict(facecolor='lightgreen'),\n",
      "                    medianprops=dict(color='black'))\n",
      "    axes[1].set_title(f'Wykres Pudełkowy - {col}', fontsize=14)\n",
      "    axes[1].set_xlabel('Wartość', fontsize=12)\n",
      "\n",
      "    # Ustawienie głównego tytułu dla całej figury.\n",
      "    fig.suptitle(f'Analiza Wartości Odstających dla Kolumny: {col}', fontsize=16, weight='bold')\n",
      "\n",
      "    # Zapewnienie odpowiedniego układu, aby elementy się nie nakładały.\n",
      "    fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "\n",
      "    # Dodanie gotowej figury do listy.\n",
      "    figures_to_embed.append(fig)\n",
      "----------------------------\n",
      "--------------------\n",
      "\n",
      "--- WĘZEŁ: KOMPOZYTOR RAPORTU ---\n",
      "  [INFO] Wykonano kod i wygenerowano 3 wykres(y).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_google_vertexai.chat_models:Unexpected argument 'project_id' provided to ChatVertexAI. Did you mean: 'project'?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Raport został pomyślnie wygenerowany w reports/transformation_report.html\n",
      "--- Krok: 'report_composer' ---\n",
      "  [INFO] Węzeł zakończył pracę bez aktualizacji stanu.\n",
      "--------------------\n",
      "\n",
      "\n",
      "================================================================================\n",
      "### ### FAZA 3: META-AUDYT I KONSOLIDACJA WIEDZY ### ###\n",
      "================================================================================\n",
      "\n",
      "  [INFO] Uruchamiam proces destylacji wspomnienia o sukcesie...\n",
      "INFO: Uruchamiam proces destylacji wspomnienia o sukcesie...\n",
      "INFO: Pomyślnie przedestylowano wspomnienie o sukcesie.\n",
      "BŁĄD ZAPISU PAMIĘCI: 400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'Fact length must be less than 2048 characters.', 'status': 'INVALID_ARGUMENT'}}\n",
      "  [INFO] Zapisywanie raportu z audytu do: reports/meta_audit_report.txt\n",
      "  [SUKCES] Pomyślnie zapisano raport z audytu.\n",
      "INFO: Uruchamiam proces generowania wniosku META...\n",
      "INFO: Pomyślnie wygenerowano wniosek META.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_google_vertexai.chat_models:Unexpected argument 'project_id' provided to ChatVertexAI. Did you mean: 'project'?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Zapisano wspomnienie typu 'MemoryType.META_INSIGHT' w zakresie {'dataset_signature': 'ae1568fe7dae11d4bacd0c21ed718503'}\n",
      "--- Krok: 'meta_auditor' ---\n",
      "  [INFO] Węzeł zakończył pracę bez aktualizacji stanu.\n",
      "--------------------\n",
      "\n",
      "INFO: Próba zapisu pełnego logu wykonania LangGraph do pliku: reports/langgraph_execution.log\n",
      "✅ SUKCES: Log wykonania LangGraph został pomyślnie zapisany.\n",
      "\n",
      "================================================================================\n",
      "### ### FAZA 3: META-AUDYT I KONSOLIDACJA WIEDZY ### ###\n",
      "================================================================================\n",
      "\n",
      "  [INFO] Uruchamiam proces destylacji wspomnienia o sukcesie...\n",
      "INFO: Uruchamiam proces destylacji wspomnienia o sukcesie...\n",
      "INFO: Pomyślnie przedestylowano wspomnienie o sukcesie.\n",
      "BŁĄD ZAPISU PAMIĘCI: 400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'Fact length must be less than 2048 characters.', 'status': 'INVALID_ARGUMENT'}}\n",
      "  [INFO] Zapisywanie raportu z audytu do: reports/meta_audit_report.txt\n",
      "  [SUKCES] Pomyślnie zapisano raport z audytu.\n",
      "INFO: Uruchamiam proces generowania wniosku META...\n",
      "INFO: Pomyślnie wygenerowano wniosek META.\n",
      "INFO: Zapisano wspomnienie typu 'MemoryType.META_INSIGHT' w zakresie {'dataset_signature': 'ae1568fe7dae11d4bacd0c21ed718503'}\n",
      "\n",
      "\n",
      "--- ZAKOŃCZONO PRACĘ GRAFU I AUDYT ---\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    files_to_exclude = {'Agents_beta (10).py','pack_project.ipynb', 'caly_projekt.txt'}\n",
    "    system_source_code = read_project_source_code(\".\", exclude_files=files_to_exclude)\n",
    "\n",
    "    # --- Inicjalizacja Pamięci i Uruchomienia ---\n",
    "    memory_client = MemoryBankClient(client=client, agent_engine=agent_engine)\n",
    "    run_id = str(uuid.uuid4())\n",
    "    \n",
    "    print(\"\\n--- ODPYTYWANIE PAMIĘCI O INSPIRACJE ---\")\n",
    "    inspiration_prompt = \"\"\n",
    "    dataset_signature = \"\"\n",
    "    try:\n",
    "        df_preview = pd.read_csv(INPUT_FILE_PATH, nrows=0)\n",
    "        dataset_signature = memory_client.create_dataset_signature(df_preview)\n",
    "        past_memories = memory_client.query_memory(\n",
    "            query_text=\"Najlepsze strategie i kluczowe wnioski dotyczące przetwarzania danych\",\n",
    "            scope={\"dataset_signature\": dataset_signature},\n",
    "            top_k=3\n",
    "        )\n",
    "        if past_memories:\n",
    "            inspirations = []\n",
    "            for mem in past_memories:\n",
    "                if mem.memory_type == MemoryType.SUCCESSFUL_PLAN and 'key_insight' in mem.content:\n",
    "                    inspirations.append(f\"SPRAWDZONY WNIOSEK Z PLANU: {mem.content['key_insight']}\")\n",
    "                elif mem.memory_type == MemoryType.SUCCESSFUL_FIX and 'key_takeaway' in mem.content:\n",
    "                    inspirations.append(f\"NAUCZKA Z NAPRAWIONEGO BŁĘDU: {mem.content['key_takeaway']}\")\n",
    "            if inspirations:\n",
    "                inspiration_prompt = \"--- INSPIRACJE Z POPRZEDNICH URUCHOMIEŃ ---\\n\" + \"\\n\".join(inspirations)\n",
    "                print(\"INFO: Pomyślnie pobrano inspiracje z pamięci.\")\n",
    "        else:\n",
    "            print(\"INFO: Nie znaleziono inspiracji w pamięci dla tego typu danych.\")\n",
    "    except Exception as e:\n",
    "        print(f\"OSTRZEŻENIE: Nie udało się pobrać inspiracji z pamięci: {e}\")\n",
    "\n",
    "        \n",
    "        \n",
    "    active_policies = get_active_policies_from_memory(memory_client, dataset_signature)    \n",
    "    \n",
    "    # --- Krok 1: Faza planowania (AutoGen) ---\n",
    "    final_plan, autogen_log = run_autogen_planning_phase(\n",
    "        input_path=INPUT_FILE_PATH, \n",
    "        inspiration_prompt=inspiration_prompt,\n",
    "        trigger_agent=trigger_agent,\n",
    "        planner_agent=planner_agent,\n",
    "        critic_agent=critic_agent,\n",
    "        manager_agent_config=main_agent_configuration,\n",
    "        active_policies=active_policies\n",
    "    )\n",
    "    save_autogen_conversation_log(log_content=autogen_log, file_path=\"reports/autogen_planning_conversation.log\")\n",
    "\n",
    "    # --- Krok 2: Faza wykonania (LangGraph) ---\n",
    "    if final_plan:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"### ### FAZA 2: URUCHAMIANIE WYKONANIA PLANU (LangGraph) ### ###\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        workflow = StateGraph(AgentWorkflowState)\n",
    "        \n",
    "        # <<< ZMIANA TUTAJ: Zaktualizowana lista węzłów >>>\n",
    "        nodes = [\n",
    "            \"schema_reader\", \"code_generator\", \"architectural_validator\", \n",
    "            \"data_code_executor\", \"universal_debugger\", \"apply_code_fix\", \n",
    "            \"human_approval\", \"package_installer\", \"human_escalation\", \n",
    "            \"sync_report_code\", \"commit_memory\",\"meta_auditor\",\n",
    "            # Nowe, wyspecjalizowane węzły raportujące:\n",
    "            \"summary_analyst\", \"plot_generator\", \"report_composer\" \n",
    "        ]\n",
    "        for name in nodes: workflow.add_node(name, globals()[f\"{name}_node\"])\n",
    "\n",
    "        # Definicja krawędzi\n",
    "        workflow.set_entry_point(\"schema_reader\")\n",
    "        \n",
    "        # Ścieżka przetwarzania danych\n",
    "        workflow.add_edge(\"schema_reader\", \"code_generator\")\n",
    "        workflow.add_edge(\"code_generator\", \"architectural_validator\")\n",
    "        \n",
    "        def should_continue_or_debug(state: AgentWorkflowState) -> str:\n",
    "            if state.get(\"error_message\"):\n",
    "                if state.get(\"correction_attempts\", 0) >= MAX_CORRECTION_ATTEMPTS:\n",
    "                    return \"request_human_help\"\n",
    "                return \"call_debugger\"\n",
    "            return \"continue\"\n",
    "\n",
    "        workflow.add_conditional_edges(\n",
    "            \"architectural_validator\",\n",
    "            should_continue_or_debug,\n",
    "            {\"call_debugger\": \"universal_debugger\", \"request_human_help\": \"human_escalation\", \"continue\": \"data_code_executor\"}\n",
    "        )\n",
    "        workflow.add_conditional_edges(\n",
    "            \"data_code_executor\",\n",
    "            should_continue_or_debug,\n",
    "            {\"call_debugger\": \"universal_debugger\", \"request_human_help\": \"human_escalation\", \"continue\": \"commit_memory\"}\n",
    "        )\n",
    "        \n",
    "        # <<< ZMIANA TUTAJ: Nowa ścieżka raportowania >>>\n",
    "        workflow.add_edge(\"commit_memory\", \"summary_analyst\")\n",
    "        workflow.add_conditional_edges(\n",
    "            \"summary_analyst\",\n",
    "            should_continue_or_debug,\n",
    "            {\"call_debugger\": \"universal_debugger\", \"request_human_help\": \"human_escalation\", \"continue\": \"plot_generator\"}\n",
    "        )\n",
    "        workflow.add_conditional_edges(\n",
    "            \"plot_generator\",\n",
    "            should_continue_or_debug,\n",
    "            {\"call_debugger\": \"universal_debugger\", \"request_human_help\": \"human_escalation\", \"continue\": \"report_composer\"}\n",
    "        )\n",
    "        workflow.add_conditional_edges(\n",
    "            \"report_composer\",\n",
    "            should_continue_or_debug,\n",
    "            {\"call_debugger\": \"universal_debugger\", \"request_human_help\": \"human_escalation\", \"continue\": \"meta_auditor\"}\n",
    "        )\n",
    "\n",
    "        # Ścieżki naprawcze\n",
    "        workflow.add_edge(\"human_escalation\", \"meta_auditor\")\n",
    "        workflow.add_edge(\"package_installer\", \"data_code_executor\")\n",
    "\n",
    "        def route_after_fix(state):\n",
    "            failing_node = state.get(\"failing_node\")\n",
    "            # Po naprawie wraca do węzła, który zawiódł\n",
    "            if failing_node:\n",
    "                return failing_node\n",
    "            # Domyślnie wraca do walidacji\n",
    "            return \"architectural_validator\"\n",
    "\n",
    "        workflow.add_conditional_edges(\"apply_code_fix\", route_after_fix)\n",
    "\n",
    "        def route_from_debugger(state):\n",
    "            if state.get(\"tool_choice\") == \"propose_code_fix\":\n",
    "                return \"apply_code_fix\"\n",
    "            if state.get(\"tool_choice\") == \"request_package_installation\":\n",
    "                return \"human_approval\"\n",
    "            return \"human_escalation\"\n",
    "\n",
    "        workflow.add_conditional_edges(\"universal_debugger\", route_from_debugger)\n",
    "        workflow.add_conditional_edges(\"human_approval\", lambda s: s.get(\"user_approval_status\"), {\n",
    "            \"APPROVED\": \"package_installer\", \"REJECTED\": \"universal_debugger\"\n",
    "        })\n",
    "\n",
    "        app = workflow.compile()\n",
    "        \n",
    "        app_config = {\"MAIN_AGENT\": MAIN_AGENT, \"CODE_MODEL\": CODE_MODEL, \"CRITIC_MODEL\": CRITIC_MODEL}\n",
    "        \n",
    "        initial_state = {\n",
    "            \"config\": app_config,\n",
    "            \"plan\": final_plan, \n",
    "            \"input_path\": INPUT_FILE_PATH,\n",
    "            \"output_path\": \"reports/processed_data.csv\",\n",
    "            \"report_output_path\": \"reports/transformation_report.html\",\n",
    "            \"correction_attempts\": 0, \n",
    "            \"correction_history\": [],\n",
    "            \"source_code\": system_source_code,\n",
    "            \"autogen_log\": autogen_log,\n",
    "            \"memory_client\": memory_client,\n",
    "            \"run_id\": run_id,\n",
    "            \"dataset_signature\": dataset_signature,\n",
    "            \"pending_fix_session\": None,\n",
    "            \"active_policies\": active_policies\n",
    "        }\n",
    "        \n",
    "        langgraph_log = \"\"\n",
    "        final_run_state = initial_state.copy()\n",
    "        \n",
    "        for event in app.stream(initial_state, {\"recursion_limit\": 50}):\n",
    "            for node_name, state_update in event.items():\n",
    "                if \"__end__\" not in node_name:\n",
    "                    print(f\"--- Krok: '{node_name}' ---\")\n",
    "                    if state_update:\n",
    "                        printable_update = state_update.copy()\n",
    "                        for key in [\"generated_code\", \"corrected_code\", \"generated_report_code\", \"error_context_code\", \"plot_generation_code\", \"summary_html\"]:\n",
    "                            if key in printable_update and printable_update[key]:\n",
    "                                print(f\"--- {key.upper()} ---\")\n",
    "                                print(printable_update[key])\n",
    "                                print(\"-\" * (len(key) + 8))\n",
    "                                del printable_update[key]\n",
    "                        if printable_update:\n",
    "                            print(json.dumps(printable_update, indent=2, default=str))\n",
    "                        \n",
    "                        log_line = f\"--- Krok: '{node_name}' ---\\n{json.dumps(state_update, indent=2, default=str)}\\n\"\n",
    "                        langgraph_log += log_line\n",
    "                        final_run_state.update(state_update)\n",
    "                    else:\n",
    "                        print(\"  [INFO] Węzeł zakończył pracę bez aktualizacji stanu.\")\n",
    "                    print(\"-\" * 20 + \"\\n\")\n",
    "\n",
    "        save_langgraph_execution_log(log_content=langgraph_log, file_path=\"reports/langgraph_execution.log\")\n",
    "\n",
    "        final_run_state['langgraph_log'] = langgraph_log\n",
    "        meta_auditor_node(final_run_state)\n",
    "\n",
    "        print(\"\\n\\n--- ZAKOŃCZONO PRACĘ GRAFU I AUDYT ---\")\n",
    "    else:\n",
    "        print(\"Proces zakończony. Brak planu do wykonania.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c476ce17-60f1-4436-8754-d2c7210310c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d9a675-049c-4ac3-b518-a077cc26664f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "agents_with_memory_p11",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Agents with memory (Python 3.11)",
   "language": "python",
   "name": "agents_with_memory_p11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
